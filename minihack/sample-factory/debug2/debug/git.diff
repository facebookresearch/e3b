diff --git a/minihack/README.md b/minihack/README.md
deleted file mode 100644
index 4ab8b32..0000000
--- a/minihack/README.md
+++ /dev/null
@@ -1,35 +0,0 @@
-# Exploration via Elliptical Episodic Bonuses
-
-To install the dependencies, run:
-
-```
-conda create -n e3b python=3.8
-conda activate e3b
-pip install -r requirements.txt
-```
-
-Make sure to install MiniHack following the instructions [here](https://github.com/facebookresearch/minihack). 
-
-To train an agent with E3B using the hyperparameters from the paper, run:
-
-```
-OMP_NUM_THREADS=1 python main.py  --learning_rate 0.0001 --model e3b --episodic_bonus_type elliptical-icm --savedir ./results/elliptical/ --env MiniHack-MultiRoom-N6-v0 --ridge 0.1 --reward_norm int --intrinsic_reward_coef 1.0 --seed 1
-```
-
-The file `sweep_slurm.py` will submit SLURM jobs for the experiments in the paper (may require some editing based on your computing infrastructure). It can also be run with the argument `--dry`, which will print out a list of commands instead. For example:
-
-```
-python sweep_slurm_neurips_2022.py --task train-elliptical 
-```
-
-trains E3B on all MiniHack tasks.
-
-```
-python sweep_slurm_neurips_2022.py --task train-noveld --dry
-```
-
-will print out all the commands to train NovelD (standard NovelD and the 3 variants described in the paper) on MiniHack.
-
-The `sweep_slurm_icml_2023.py` file contains the commands to run the MiniHack experiments from the paper [A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs](https://arxiv.org/abs/2306.03236). This includes both the count-based global and episodic bonus experiments in Section 3 and the E3BxRND algorithm in Section 4. 
-
-
diff --git a/minihack/main.py b/minihack/main.py
deleted file mode 100644
index 5b33bdb..0000000
--- a/minihack/main.py
+++ /dev/null
@@ -1,57 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-
-from src.arguments import parser 
-
-from src.algos.torchbeast import train as train_vanilla 
-from src.algos.count import train as train_count
-from src.algos.curiosity import train as train_curiosity 
-from src.algos.rnd import train as train_rnd
-from src.algos.ride import train as train_ride
-from src.algos.bebold import train as train_bebold
-from src.algos.e3b import train as train_e3b
-from src.algos.e3b_noveld import train as train_e3b_noveld
-
-# Necessary for multithreading.
-import os
-import pdb
-import numpy as np
-os.environ["OMP_NUM_THREADS"] = "1"
-
-
-def main(flags):
-    print(flags)
-    flags.use_lstm = flags.use_lstm==1
-
-
-    if flags.num_contexts != -1:
-        # if we are limiting number of contexts, sample random seeds used to generate envs
-        flags.fix_seed = True
-        flags.env_seed = list(np.random.choice(range(100000), flags.num_contexts, replace=False))
-        flags.env_seed = [int(i) for i in flags.env_seed]
-        print(flags.env_seed)
-    else:
-        flags.fix_seed = False
-    
-    
-    if flags.model == 'vanilla':
-        train_vanilla(flags)
-    elif flags.model == 'count':
-        train_count(flags)
-    elif flags.model == 'curiosity':
-        train_curiosity(flags)
-    elif flags.model == 'rnd':
-        train_rnd(flags)
-    elif flags.model == 'ride':
-        train_ride(flags)
-    elif flags.model == 'bebold':
-        train_bebold(flags)
-    elif flags.model == 'e3b':
-        train_e3b(flags)
-    elif flags.model == 'e3b-noveld':
-        train_e3b_noveld(flags)
-    else:
-        raise NotImplementedError("This model has not been implemented.")
-
-if __name__ == '__main__':
-    flags = parser.parse_args()
-    main(flags)
diff --git a/minihack/requirements.txt b/minihack/requirements.txt
deleted file mode 100644
index 1bfaa13..0000000
--- a/minihack/requirements.txt
+++ /dev/null
@@ -1,139 +0,0 @@
-absl-py==1.0.0
-appdirs==1.4.4
-arch==5.1.0
-argcomplete==1.12.3
-argon2-cffi==21.1.0
-attrs==21.2.0
-backcall==0.2.0
-bleach==4.1.0
-cached-property==1.5.2
-certifi==2021.10.8
-cffi==1.13.2
-cfgv==3.1.0
-charset-normalizer==2.0.8
-click==8.0.4
-cloudpickle==1.2.2
-cycler==0.11.0
-debugpy==1.5.1
-decorator==5.1.0
-defusedxml==0.7.1
-distlib==0.3.0
-dm-control==0.0.403778684
-dm-env==1.5
-dm-tree==0.1.6
-docker-pycreds==0.4.0
-entrypoints==0.3
-filelock==3.0.12
-fonttools==4.28.2
-future==0.18.2
-gitdb2==2.0.6
-GitPython==3.1.27
-glcontext==2.3.4
-glfw==1.12.0
-gym==0.15.4
--e git+https://github.com/maximecb/gym-minigrid.git@b84d99a2fbb481977172a85661eee812f022f130#egg=gym_minigrid
-gym-super-mario-bros==7.3.0
-gym3==0.3.3
-h5py==3.6.0
-identify==1.4.19
-idna==3.3
-imageio==2.15.0
-imageio-ffmpeg==0.3.0
-importlib-metadata==1.6.1
-importlib-resources==2.0.0
-ipykernel==6.6.0
-ipython==7.30.1
-ipython-genutils==0.2.0
-jedi==0.18.1
-Jinja2==3.0.3
-jsonpatch==1.32
-jsonpointer==2.2
-jsonschema==4.2.1
-jupyter-client==7.1.0
-jupyter-core==4.9.1
-jupyterlab-pygments==0.1.2
-kiwisolver==1.3.2
-labmaze==1.0.5
-lxml==4.6.4
-MarkupSafe==2.0.1
-matplotlib==3.5.0
-matplotlib-inline==0.1.3
--e git+https://github.com/facebookresearch/minihack@a971025d067c90e2b6cbc37bb53f47eaaad27761#egg=minihack
-mistune==0.8.4
-moderngl==5.6.4
-nbclient==0.5.9
-nbconvert==6.3.0
-nbformat==5.1.3
-nes-py==8.1.1
-nest-asyncio==1.5.4
-nle==0.7.3
-nodeenv==1.4.0
-notebook==6.4.6
-numpy==1.17.4
-olefile==0.46
-opencv-python==4.1.2.30
-packaging==21.3
-pandas==1.3.4
-pandocfilters==1.5.0
-parso==0.8.3
-pathtools==0.1.2
-patsy==0.5.2
-pexpect==4.8.0
-pickleshare==0.7.5
-Pillow==9.0.1
-pre-commit==2.5.1
-procgen==0.10.7
-prometheus-client==0.12.0
-promise==2.3
-prompt-toolkit==3.0.23
-property-cached==1.6.4
-protobuf==3.19.1
-psutil==5.9.0
-ptyprocess==0.7.0
-pybind11==2.8.1
-pycparser==2.19
-pyglet==1.3.2
-Pygments==2.10.0
-PyOpenGL==3.1.5
-pyparsing==3.0.6
-PyQt5==5.13.2
-PyQt5-sip==12.7.0
-pyrsistent==0.18.0
-python-dateutil==2.8.2
-pytz==2021.3
-PyYAML==5.4
-pyzmq==22.3.0
-requests==2.26.0
-rliable==1.0.6
-scipy==1.7.3
-seaborn==0.11.2
-Send2Trash==1.8.0
-sentry-sdk==1.5.7
-setproctitle==1.2.2
-setuptools-scm==6.3.2
-shortuuid==1.0.8
-six==1.13.0
-smmap2==2.0.5
-statsmodels==0.13.2
-termcolor==1.1.0
-terminado==0.12.1
-testpath==0.5.0
-toml==0.10.1
-tomli==1.2.2
-torch==1.10.0
-torchaudio==0.10.0
-torchfile==0.1.0
-torchvision
-tornado==6.1
-tqdm==4.40.2
-traitlets==5.1.1
-typing_extensions==4.0.0
-urllib3==1.26.7
-virtualenv==20.0.21
-visdom==0.1.8.9
-wandb==0.12.11
-wcwidth==0.2.5
-webencodings==0.5.1
-websocket-client==1.2.1
-yaspin==2.1.0
-zipp==3.1.0
diff --git a/minihack/src/__init__.py b/minihack/src/__init__.py
deleted file mode 100644
index 47d9858..0000000
--- a/minihack/src/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
diff --git a/minihack/src/__pycache__/__init__.cpython-37.pyc b/minihack/src/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index cfc16e0..0000000
Binary files a/minihack/src/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/__init__.cpython-38.pyc b/minihack/src/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index b2a0bfa..0000000
Binary files a/minihack/src/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/arguments.cpython-37.pyc b/minihack/src/__pycache__/arguments.cpython-37.pyc
deleted file mode 100644
index e8d6975..0000000
Binary files a/minihack/src/__pycache__/arguments.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/arguments.cpython-38.pyc b/minihack/src/__pycache__/arguments.cpython-38.pyc
deleted file mode 100644
index 1aa3b16..0000000
Binary files a/minihack/src/__pycache__/arguments.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/atari_wrappers.cpython-37.pyc b/minihack/src/__pycache__/atari_wrappers.cpython-37.pyc
deleted file mode 100644
index 85f4af1..0000000
Binary files a/minihack/src/__pycache__/atari_wrappers.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/atari_wrappers.cpython-38.pyc b/minihack/src/__pycache__/atari_wrappers.cpython-38.pyc
deleted file mode 100644
index 459c1a6..0000000
Binary files a/minihack/src/__pycache__/atari_wrappers.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/env_utils.cpython-37.pyc b/minihack/src/__pycache__/env_utils.cpython-37.pyc
deleted file mode 100644
index ba9d779..0000000
Binary files a/minihack/src/__pycache__/env_utils.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/env_utils.cpython-38.pyc b/minihack/src/__pycache__/env_utils.cpython-38.pyc
deleted file mode 100644
index b2e8a51..0000000
Binary files a/minihack/src/__pycache__/env_utils.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/losses.cpython-37.pyc b/minihack/src/__pycache__/losses.cpython-37.pyc
deleted file mode 100644
index 6aecbb6..0000000
Binary files a/minihack/src/__pycache__/losses.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/losses.cpython-38.pyc b/minihack/src/__pycache__/losses.cpython-38.pyc
deleted file mode 100644
index 743a5c9..0000000
Binary files a/minihack/src/__pycache__/losses.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/models.cpython-37.pyc b/minihack/src/__pycache__/models.cpython-37.pyc
deleted file mode 100644
index 37c0933..0000000
Binary files a/minihack/src/__pycache__/models.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/models.cpython-38.pyc b/minihack/src/__pycache__/models.cpython-38.pyc
deleted file mode 100644
index a15b675..0000000
Binary files a/minihack/src/__pycache__/models.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/utils.cpython-37.pyc b/minihack/src/__pycache__/utils.cpython-37.pyc
deleted file mode 100644
index a9f1a0f..0000000
Binary files a/minihack/src/__pycache__/utils.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/utils.cpython-38.pyc b/minihack/src/__pycache__/utils.cpython-38.pyc
deleted file mode 100644
index 748c3ae..0000000
Binary files a/minihack/src/__pycache__/utils.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/__pycache__/utils_orig.cpython-37.pyc b/minihack/src/__pycache__/utils_orig.cpython-37.pyc
deleted file mode 100644
index edbfd14..0000000
Binary files a/minihack/src/__pycache__/utils_orig.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__init__.py b/minihack/src/algos/__init__.py
deleted file mode 100644
index 47d9858..0000000
--- a/minihack/src/algos/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
diff --git a/minihack/src/algos/__pycache__/__init__.cpython-37.pyc b/minihack/src/algos/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index e93f2cf..0000000
Binary files a/minihack/src/algos/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/__init__.cpython-38.pyc b/minihack/src/algos/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index d882a0c..0000000
Binary files a/minihack/src/algos/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/bebold.cpython-37.pyc b/minihack/src/algos/__pycache__/bebold.cpython-37.pyc
deleted file mode 100644
index c3c104b..0000000
Binary files a/minihack/src/algos/__pycache__/bebold.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/bebold.cpython-38.pyc b/minihack/src/algos/__pycache__/bebold.cpython-38.pyc
deleted file mode 100644
index 2655fcd..0000000
Binary files a/minihack/src/algos/__pycache__/bebold.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/bebold_icm.cpython-37.pyc b/minihack/src/algos/__pycache__/bebold_icm.cpython-37.pyc
deleted file mode 100644
index f87cbfe..0000000
Binary files a/minihack/src/algos/__pycache__/bebold_icm.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/bebold_icm_dropout.cpython-37.pyc b/minihack/src/algos/__pycache__/bebold_icm_dropout.cpython-37.pyc
deleted file mode 100644
index 0b16d48..0000000
Binary files a/minihack/src/algos/__pycache__/bebold_icm_dropout.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/count.cpython-37.pyc b/minihack/src/algos/__pycache__/count.cpython-37.pyc
deleted file mode 100644
index 52a2668..0000000
Binary files a/minihack/src/algos/__pycache__/count.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/count.cpython-38.pyc b/minihack/src/algos/__pycache__/count.cpython-38.pyc
deleted file mode 100644
index 3be4a03..0000000
Binary files a/minihack/src/algos/__pycache__/count.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/curiosity.cpython-37.pyc b/minihack/src/algos/__pycache__/curiosity.cpython-37.pyc
deleted file mode 100644
index 8f4118f..0000000
Binary files a/minihack/src/algos/__pycache__/curiosity.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/curiosity.cpython-38.pyc b/minihack/src/algos/__pycache__/curiosity.cpython-38.pyc
deleted file mode 100644
index c00c89e..0000000
Binary files a/minihack/src/algos/__pycache__/curiosity.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/e3b.cpython-37.pyc b/minihack/src/algos/__pycache__/e3b.cpython-37.pyc
deleted file mode 100644
index 8091a39..0000000
Binary files a/minihack/src/algos/__pycache__/e3b.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/e3b.cpython-38.pyc b/minihack/src/algos/__pycache__/e3b.cpython-38.pyc
deleted file mode 100644
index 17a9f08..0000000
Binary files a/minihack/src/algos/__pycache__/e3b.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/icm_elliptical.cpython-37.pyc b/minihack/src/algos/__pycache__/icm_elliptical.cpython-37.pyc
deleted file mode 100644
index c7c241a..0000000
Binary files a/minihack/src/algos/__pycache__/icm_elliptical.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/icm_elliptical_noveld.cpython-37.pyc b/minihack/src/algos/__pycache__/icm_elliptical_noveld.cpython-37.pyc
deleted file mode 100644
index 1feec44..0000000
Binary files a/minihack/src/algos/__pycache__/icm_elliptical_noveld.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/icm_elliptical_skip.cpython-37.pyc b/minihack/src/algos/__pycache__/icm_elliptical_skip.cpython-37.pyc
deleted file mode 100644
index e3c3c4d..0000000
Binary files a/minihack/src/algos/__pycache__/icm_elliptical_skip.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/ride.cpython-37.pyc b/minihack/src/algos/__pycache__/ride.cpython-37.pyc
deleted file mode 100644
index 4f15159..0000000
Binary files a/minihack/src/algos/__pycache__/ride.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/ride.cpython-38.pyc b/minihack/src/algos/__pycache__/ride.cpython-38.pyc
deleted file mode 100644
index 7793399..0000000
Binary files a/minihack/src/algos/__pycache__/ride.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/rnd.cpython-37.pyc b/minihack/src/algos/__pycache__/rnd.cpython-37.pyc
deleted file mode 100644
index f499f5b..0000000
Binary files a/minihack/src/algos/__pycache__/rnd.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/rnd.cpython-38.pyc b/minihack/src/algos/__pycache__/rnd.cpython-38.pyc
deleted file mode 100644
index b550846..0000000
Binary files a/minihack/src/algos/__pycache__/rnd.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/torchbeast.cpython-37.pyc b/minihack/src/algos/__pycache__/torchbeast.cpython-37.pyc
deleted file mode 100644
index be3101a..0000000
Binary files a/minihack/src/algos/__pycache__/torchbeast.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/algos/__pycache__/torchbeast.cpython-38.pyc b/minihack/src/algos/__pycache__/torchbeast.cpython-38.pyc
deleted file mode 100644
index 9c101bb..0000000
Binary files a/minihack/src/algos/__pycache__/torchbeast.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/algos/bebold.py b/minihack/src/algos/bebold.py
deleted file mode 100644
index e875eb2..0000000
--- a/minihack/src/algos/bebold.py
+++ /dev/null
@@ -1,412 +0,0 @@
-
-
-# Copyright (c) Facebook, Inc. and its affiliates.
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-import os
-import sys
-import threading
-import time
-import timeit
-import pprint
-import json
-import pdb
-import copy
-
-import numpy as np
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack, Environment
-from src.utils import get_batch, log, create_env, create_buffers, act, create_heatmap_buffers
-
-MinigridMLPEmbeddingNet = models.MinigridMLPEmbeddingNet
-MinigridMLPTargetEmbeddingNet = models.MinigridMLPTargetEmbeddingNet
-
-
-NetHackStateEmbeddingNet = models.NetHackStateEmbeddingNet
-
-
-
-
-
-def learn(actor_model,
-          model,
-          random_target_network,
-          predictor_network,
-          actor_encoder,
-          encoder,
-          batch,
-          initial_agent_state, 
-          optimizer,
-          predictor_optimizer,
-          scheduler,
-          flags,
-          frames=None,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-        timings = prof.Timings()
-        
-        count_rewards = torch.ones((flags.unroll_length, flags.batch_size), 
-            dtype=torch.float32).to(device=flags.device)
-        # Use the scale of square root N
-        count_rewards = batch['episode_state_count'][1:].float().to(device=flags.device)
-
-        timings.reset()
-        encoded_states, unused_state = encoder(batch, tuple())
-        random_embedding_next, unused_state = random_target_network(encoded_states[1:].detach(), initial_agent_state)
-        predicted_embedding_next, unused_state = predictor_network(encoded_states[1:].detach(), initial_agent_state)
-        random_embedding, unused_state = random_target_network(encoded_states[:-1].detach(), initial_agent_state)
-        predicted_embedding, unused_state = predictor_network(encoded_states[:-1].detach(), initial_agent_state)
-
-        intrinsic_rewards_next = torch.norm(predicted_embedding_next.detach() - random_embedding_next.detach(), dim=2, p=2)
-        intrinsic_rewards = torch.norm(predicted_embedding.detach() - random_embedding.detach(), dim=2, p=2)
-        intrinsic_rewards = torch.clamp(intrinsic_rewards_next - flags.scale_fac * intrinsic_rewards, min=0)            
-
-        if flags.episodic_bonus_type in ['counts-obs', 'counts-glyphs', 'counts-pos', 'counts-msg', 'counts-img']:
-            if flags.count_reward_type == 'ind':
-                mask = (count_rewards.to(flags.device) == 1).float()
-                intrinsic_rewards *= mask
-            elif flags.count_reward_type == 'isqrt':
-                intrinsic_rewards *= count_rewards 
-            elif flags.count_reward_type == 'const':
-                pass
-        else:
-            raise NotImplementedError("invalid episodic_bonus_type")
-            
-
-        timings.time('int. reward comp')
-
-        timings.reset()
-        
-        num_samples = flags.unroll_length * flags.batch_size
-        actions_flat = batch['action'][1:].reshape(num_samples).cpu().detach().numpy()
-        rnd_loss = flags.rnd_loss_coef * \
-                losses.compute_rnd_loss(predicted_embedding_next, random_embedding_next.detach())
-
-            
-        learner_outputs, unused_state = model(batch, initial_agent_state)#, decode=False)        
-
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-        
-        rewards = batch['reward']
-
-
-        if flags.reward_norm == 'int':
-            model.update_running_moments(intrinsic_rewards)
-            std = model.get_running_std()
-            if std > 0:
-                intrinsic_rewards /= std
-        elif flags.reward_norm == 'ext':
-            model.update_running_moments(rewards)
-            std = model.get_running_std()
-            if std > 0:
-                rewards /= std
-
-
-        
-        if flags.no_reward:
-            total_rewards = intrinsic_rewards
-        else:            
-            total_rewards = rewards + intrinsic_rewards * flags.intrinsic_reward_coef
-            
-        if flags.clip_rewards == 1:
-            clipped_rewards = torch.clamp(total_rewards, -1, 1)
-        else:
-            clipped_rewards = total_rewards
-
-            
-            
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss + rnd_loss #+ decoder_loss
-
-
-        episode_returns = batch['episode_return'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-            'rnd_loss': rnd_loss.item(),
-            'mean_rewards': torch.mean(rewards).item(),
-            'mean_intrinsic_rewards': torch.mean(intrinsic_rewards).item(),
-            'mean_total_rewards': torch.mean(total_rewards).item(),
-        }
-        
-        optimizer.zero_grad()
-        predictor_optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(predictor_network.parameters(), flags.max_grad_norm)
-        optimizer.step()
-        predictor_optimizer.step()
-
-        timings.time('policy update')
-        timings.reset()
-        
-        timings.time('encoder update')
-
-        actor_model.load_state_dict(model.state_dict())
-        actor_encoder.load_state_dict(encoder.state_dict())
-        return stats, None#decoder_logits.detach().cpu()
-
-
-
-    
-    
-
-def train(flags):
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'-model_{flags.model}'
-    xpid += f'-bt_{flags.episodic_bonus_type}'
-    xpid += f'-cr_{flags.count_reward_type}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-cl_{flags.clip_rewards}'
-    xpid += f'-rn_{flags.reward_norm}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-intweight_{flags.intrinsic_reward_coef}'
-    xpid += f'-scalefac_{flags.scale_fac}'
-    xpid += f'-seed_{flags.seed}'
-
-    flags.xpid = xpid
-        
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-
-    checkpointpath = os.path.expandvars(
-        os.path.expanduser('%s/%s/%s' % (flags.savedir, flags.xpid,
-                                         'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device(f'cuda:{flags.device}')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-
-    env = create_env(flags)
-
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)  
-        
-    if 'MiniHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm)
-        random_target_network = MinigridMLPTargetEmbeddingNet(hidden_dim=flags.hidden_dim).to(device=flags.device) 
-        predictor_network = MinigridMLPEmbeddingNet(hidden_dim=flags.hidden_dim).to(device=flags.device) 
-        encoder = NetHackStateEmbeddingNet(env.observation_space, False) #do not use LSTM
-    elif 'Vizdoom' in flags.env:
-        model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-        random_target_network = MinigridMLPTargetEmbeddingNet(hidden_dim=flags.hidden_dim).to(device=flags.device) 
-        predictor_network = MinigridMLPEmbeddingNet(hidden_dim=flags.hidden_dim).to(device=flags.device) 
-        encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape).to(flags.device)
-    else:
-        raise Exception('Only MiniHack and Vizdoom are suppported Now!')
-
-
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-    model.share_memory()
-    encoder.share_memory()
-    
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.Queue()
-    full_queue = ctx.Queue()
-
-    episode_state_count_dict = dict()
-    for i in range(flags.num_actors):
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, None, buffers, 
-                  episode_state_count_dict, initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-
-    if 'MiniHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm).to(flags.device)
-        learner_encoder = NetHackStateEmbeddingNet(env.observation_space, False).to(device=flags.device)
-    elif 'Vizdoom' in flags.env:
-        learner_model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n).to(flags.device)
-        learner_encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape).to(flags.device)
-    else:
-        raise Exception('Only MiniHack and Vizdoom are suppported Now!')
-    learner_encoder.load_state_dict(encoder.state_dict())
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-
-    predictor_optimizer = torch.optim.Adam(
-        predictor_network.parameters(), 
-        lr=flags.predictor_learning_rate)
-
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'total_loss',
-        'mean_episode_return',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-        'rnd_loss',
-        'mean_rewards',
-        'mean_intrinsic_rewards',
-        'mean_total_rewards',
-    ]
-
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-
-    frames, stats = 0, {}
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                initial_agent_state_buffers, flags, timings)
-            stats, decoder_logits = learn(model, learner_model, random_target_network, predictor_network,
-                                          encoder, learner_encoder, batch, agent_state, optimizer, 
-                                          predictor_optimizer, scheduler, flags, frames=frames)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-                
-
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []    
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-
-
-    def checkpoint(frames):
-        if flags.disable_checkpoint:
-            return
-        checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'frames': frames,
-            'model_state_dict': model.state_dict(),
-            'encoder': encoder.state_dict(),
-            'random_target_network_state_dict': random_target_network.state_dict(),
-            'predictor_network_state_dict': predictor_network.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'predictor_optimizer_state_dict': predictor_optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60:  
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-
-            total_loss = stats.get('total_loss', float('inf'))
-            if stats:
-                log.info('After %i frames: loss %f @ %.1f fps. Mean Return %.1f. \n Stats \n %s', \
-                        frames, total_loss, fps, stats['mean_episode_return'], pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return  
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-
-    checkpoint(frames)
-    plogger.close()
diff --git a/minihack/src/algos/count.py b/minihack/src/algos/count.py
deleted file mode 100644
index c8ec369..0000000
--- a/minihack/src/algos/count.py
+++ /dev/null
@@ -1,325 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-import os
-import threading
-import time
-import timeit
-import pprint
-
-import numpy as np
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack
-from src.utils import get_batch, log, create_env, create_buffers, act
-
-
-def learn(actor_model,
-          model,
-          batch,
-          initial_agent_state, 
-          optimizer,
-          scheduler,
-          flags,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-
-        if flags.episodic_bonus_type != 'none':
-            # this is 1/sqrt(N)
-            intrinsic_rewards_episodic = batch['episode_state_count'][1:].float().to(device=flags.device)
-            intrinsic_rewards_episodic = (intrinsic_rewards_episodic == 1).float()
-
-        if flags.global_bonus_type != 'none':
-            # this is 1/sqrt(N)
-            intrinsic_rewards_global = batch['global_state_count'][1:].float().to(device=flags.device)
-
-        
-        if flags.episodic_bonus_type != 'none' and flags.global_bonus_type == 'none':
-            intrinsic_rewards = intrinsic_rewards_episodic
-        elif flags.episodic_bonus_type == 'none' and flags.global_bonus_type != 'none':
-            intrinsic_rewards = intrinsic_rewards_global
-        elif flags.episodic_bonus_type != 'none' and flags.global_bonus_type != 'none':
-            intrinsic_rewards = intrinsic_rewards_episodic * intrinsic_rewards_global
-        
-                
-        print(intrinsic_rewards)
-            
-        
-        learner_outputs, unused_state = model(batch, initial_agent_state)    
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-
-        rewards = batch['reward']
-
-
-
-        if flags.reward_norm == 'int':
-            model.update_running_moments(intrinsic_rewards)
-            std = model.get_running_std()
-            if std > 0:
-                intrinsic_rewards /= std
-        elif flags.reward_norm == 'ext':
-            model.update_running_moments(rewards)
-            std = model.get_running_std()
-            if std > 0:
-                rewards /= std
-
-        
-        if flags.no_reward:
-            total_rewards = intrinsic_rewards
-        else:
-            total_rewards = rewards + flags.intrinsic_reward_coef*intrinsic_rewards
-            
-        clipped_rewards = torch.clamp(total_rewards, -1, 1)
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss
-
-        episode_returns = batch['episode_return'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-            'mean_rewards': torch.mean(rewards).item(),
-            'mean_intrinsic_rewards': torch.mean(intrinsic_rewards).item(),
-            'mean_total_rewards': torch.mean(total_rewards).item(),
-        }
-        
-        optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        optimizer.step()
-
-        actor_model.load_state_dict(model.state_dict())
-        return stats
-
-
-def train(flags):
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'-model_{flags.model}'
-    xpid += f'-btype_{flags.episodic_bonus_type}'
-    xpid += f'-gtype_{flags.global_bonus_type}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-intrew_{flags.intrinsic_reward_coef}'
-    xpid += f'-rn_{flags.reward_norm}'
-    xpid += f'-nc_{flags.num_contexts}'
-    xpid += f'-seed_{flags.seed}'
-
-    flags.xpid = xpid
-
-
-    
-    if flags.xpid is None:
-        flags.xpid = 'count-%s' % time.strftime('%Y%m%d-%H%M%S')
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-    checkpointpath = os.path.expandvars(
-        os.path.expanduser('%s/%s/%s' % (flags.savedir, flags.xpid,
-                                         'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-    flags.device = None
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device('cuda')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-
-    env = create_env(flags)
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)  
-
-    if 'MiniHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim)
-
-
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-    
-    model.share_memory()
-
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-    
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.SimpleQueue()
-    full_queue = ctx.SimpleQueue()
-
-    episode_state_count_dict = dict()
-    global_state_count_dict = dict()
-    for i in range(flags.num_actors):
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, None, buffers, 
-                  episode_state_count_dict, global_state_count_dict, initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-
-
-    if 'MiniHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim).to(flags.device)
-        
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-
-
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'total_loss',
-        'mean_episode_return',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-        'mean_rewards',
-        'mean_intrinsic_rewards',
-        'mean_total_rewards',
-    ]
-    
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-    frames, stats = 0, {}
-
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                                           initial_agent_state_buffers, flags, timings)
-            stats = learn(model, learner_model, batch, agent_state, 
-                optimizer, scheduler, flags)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-    
-    def checkpoint(frames):
-        if flags.disable_checkpoint:
-            return
-        checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'model_state_dict': model.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60:  
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-            total_loss = stats.get('total_loss', float('inf'))
-            log.info('After %i frames: loss %f @ %.1f fps. %sStats:\n%s',
-                         frames, total_loss, fps, mean_return,
-                         pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return  
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-        
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-    checkpoint(frames)
-    plogger.close()
diff --git a/minihack/src/algos/curiosity.py b/minihack/src/algos/curiosity.py
deleted file mode 100644
index 1e14b2b..0000000
--- a/minihack/src/algos/curiosity.py
+++ /dev/null
@@ -1,407 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-import os
-import sys
-import threading
-import time
-import timeit
-import pprint
-
-import numpy as np
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack
-from src.utils import get_batch, log, create_env, create_buffers, act
-
-NetHackStateEmbeddingNet = models.NetHackStateEmbeddingNet
-MinigridForwardDynamicsNet = models.MinigridForwardDynamicsNet
-MinigridInverseDynamicsNet = models.MinigridInverseDynamicsNet
-
-def learn(actor_model,
-          model,
-          state_embedding_model,
-          forward_dynamics_model,
-          inverse_dynamics_model,
-          batch,
-          initial_agent_state, 
-          optimizer,
-          state_embedding_optimizer, 
-          forward_dynamics_optimizer, 
-          inverse_dynamics_optimizer, 
-          scheduler,
-          flags,
-          frames=None,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-        '''
-        if flags.use_fullobs_intrinsic:
-            state_emb = state_embedding_model(batch, next_state=False)\
-                    .reshape(flags.unroll_length, flags.batch_size, 128)
-            next_state_emb = state_embedding_model(batch, next_state=True)\
-                    .reshape(flags.unroll_length, flags.batch_size, 128)
-        else:
-            state_emb = state_embedding_model(batch['partial_obs'][:-1].to(device=flags.device))
-            next_state_emb = state_embedding_model(batch['partial_obs'][1:].to(device=flags.device))
-        '''
-
-        state_emb_all, _ = state_embedding_model(batch, tuple())
-        state_emb = state_emb_all[:-1]
-        next_state_emb = state_emb_all[1:]
-            
-
-        pred_next_state_emb = forward_dynamics_model(\
-            state_emb, batch['action'][1:].to(device=flags.device))
-        pred_actions = inverse_dynamics_model(state_emb, next_state_emb) 
-        entropy_emb_actions = losses.compute_entropy_loss(pred_actions)
-
-        intrinsic_rewards = torch.norm(pred_next_state_emb - next_state_emb, dim=2, p=2)
-        
-        intrinsic_reward_coef = flags.intrinsic_reward_coef
-        intrinsic_rewards *= intrinsic_reward_coef 
-        
-        forward_dynamics_loss = flags.forward_loss_coef * \
-            losses.compute_forward_dynamics_loss(pred_next_state_emb, next_state_emb)
-
-        inverse_dynamics_loss = flags.inverse_loss_coef * \
-            losses.compute_inverse_dynamics_loss(pred_actions, batch['action'][1:])
-
-        num_samples = flags.unroll_length * flags.batch_size
-        actions_flat = batch['action'][1:].reshape(num_samples).cpu().detach().numpy()
-        intrinsic_rewards_flat = intrinsic_rewards.reshape(num_samples).cpu().detach().numpy()
-
-            
-        learner_outputs, unused_state = model(batch, initial_agent_state)
-
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-        
-        actions = batch['action'].reshape(flags.unroll_length * flags.batch_size).cpu().numpy()
-        action_percentage = [0 for _ in range(model.num_actions)]
-        for i in range(model.num_actions):
-            action_percentage[i] = np.sum([a == i for a in actions]) / len(actions)
-        
-        rewards = batch['reward']
-            
-        if flags.no_reward:
-            total_rewards = intrinsic_rewards
-        else:            
-            total_rewards = rewards + intrinsic_rewards
-        clipped_rewards = torch.clamp(total_rewards, -1, 1)
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss \
-                + forward_dynamics_loss  + inverse_dynamics_loss
-        
-        episode_returns = batch['episode_return'][batch['done']]
-        episode_lengths = batch['episode_step'][batch['done']]
-#        episode_wins = batch['episode_win'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-            'forward_dynamics_loss': forward_dynamics_loss.item(),
-            'inverse_dynamics_loss': inverse_dynamics_loss.item(),
-            'mean_rewards': torch.mean(rewards).item(),
-            'mean_intrinsic_rewards': torch.mean(intrinsic_rewards).item(),
-            'mean_total_rewards': torch.mean(total_rewards).item(),
-        }
-        
-#        scheduler.step()
-        optimizer.zero_grad()
-        state_embedding_optimizer.zero_grad()
-        forward_dynamics_optimizer.zero_grad()
-        inverse_dynamics_optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(state_embedding_model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(forward_dynamics_model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(inverse_dynamics_model.parameters(), flags.max_grad_norm)
-        optimizer.step()
-        state_embedding_optimizer.step()
-        forward_dynamics_optimizer.step()
-        inverse_dynamics_optimizer.step()
-
-        actor_model.load_state_dict(model.state_dict())
-        return stats
-
-
-def train(flags): 
-#    if flags.xpid is None:
-#        flags.xpid = 'curiosity-%s' % time.strftime('%Y%m%d-%H%M%S')
-
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'model_{flags.model}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-fc_{flags.forward_loss_coef}'
-    xpid += f'-ic_{flags.inverse_loss_coef}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-intweight_{flags.intrinsic_reward_coef}'
-    xpid += f'-seed_{flags.seed}'
-
-    flags.xpid = xpid
-    
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-
-    checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-    flags.device = None
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device('cuda')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-
-    env = create_env(flags)
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)  
-
-    if 'MiniHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm)
-        state_embedding_model = NetHackStateEmbeddingNet(env.observation_space, False).to(flags.device)
-        inverse_dynamics_model = MinigridInverseDynamicsNet(env.action_space.n, emb_size=1024)\
-            .to(device=flags.device) 
-        forward_dynamics_model = MinigridForwardDynamicsNet(env.action_space.n)\
-            .to(device=flags.device)
-    
-    elif 'Vizdoom' in flags.env:
-        model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-        state_embedding_model = models.MarioDoomStateEmbeddingNet(env.observation_space.shape)\
-            .to(device=flags.device) 
-        forward_dynamics_model = models.MarioDoomForwardDynamicsNet(env.action_space.n)\
-            .to(device=flags.device) 
-        inverse_dynamics_model = models.MarioDoomInverseDynamicsNet(env.action_space.n)\
-            .to(device=flags.device) 
-
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-#    print(buffers.keys())
-#    print(env.observation_space)
-    model.share_memory()
-    
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.SimpleQueue()
-    full_queue = ctx.SimpleQueue()
-    
-    episode_state_count_dict = dict()
-    train_state_count_dict = dict()
-    for i in range(flags.num_actors):
-        print(buffers.keys())
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, None, buffers, 
-                episode_state_count_dict, 
-                initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-  
-    if 'MiniHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim).to(flags.device)
-    elif 'Vizdoom' in flags.env:
-        learner_model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)\
-            .to(device=flags.device)
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-    state_embedding_optimizer = torch.optim.RMSprop(
-        state_embedding_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-    inverse_dynamics_optimizer = torch.optim.RMSprop(
-        inverse_dynamics_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-    forward_dynamics_optimizer = torch.optim.RMSprop(
-        forward_dynamics_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-    
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'total_loss',
-        'mean_episode_return',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-        'forward_dynamics_loss',
-        'inverse_dynamics_loss',
-        'mean_rewards',
-        'mean_intrinsic_rewards',
-        'mean_total_rewards',
-    ]
-
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-
-    frames, stats = 0, {}
-
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                initial_agent_state_buffers, flags, timings)
-            stats = learn(model, learner_model, state_embedding_model, forward_dynamics_model, 
-                          inverse_dynamics_model, batch, agent_state, optimizer, 
-                          state_embedding_optimizer, forward_dynamics_optimizer, 
-                          inverse_dynamics_optimizer, scheduler, flags, frames=frames)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []    
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-
-
-    def checkpoint(frames):
-        if flags.disable_checkpoint:
-            return
-        checkpointpath = os.path.expandvars(
-            os.path.expanduser('%s/%s/%s' % (flags.savedir, flags.xpid,
-            'model.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'model_state_dict': model.state_dict(),
-            'state_embedding_model_state_dict': state_embedding_model.state_dict(),
-            'forward_dynamics_model_state_dict': forward_dynamics_model.state_dict(),
-            'inverse_dynamics_model_state_dict': inverse_dynamics_model.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'state_embedding_optimizer_state_dict': state_embedding_optimizer.state_dict(),
-            'forward_dynamics_optimizer_state_dict': forward_dynamics_optimizer.state_dict(),
-            'inverse_dynamics_optimizer_state_dict': inverse_dynamics_optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60: 
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-
-            total_loss = stats.get('total_loss', float('inf'))
-            if stats:
-                log.info('After %i frames: loss %f @ %.1f fps. Mean Return %.1f. \n Stats \n %s', \
-                        frames, total_loss, fps, stats['mean_episode_return'], pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return 
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-
-    checkpoint(frames)
-    plogger.close()
diff --git a/minihack/src/algos/e3b.py b/minihack/src/algos/e3b.py
deleted file mode 100644
index 27b8c12..0000000
--- a/minihack/src/algos/e3b.py
+++ /dev/null
@@ -1,388 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-
-import logging
-import os
-import sys
-import threading
-import time
-import timeit
-import pprint
-import json
-import pdb
-import contextlib
-
-import numpy as np
-import random
-import copy
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack, Environment
-from src.utils import get_batch, log, create_env, create_buffers, act, create_heatmap_buffers
-
-NetHackStateEmbeddingNet = models.NetHackStateEmbeddingNet
-MinigridInverseDynamicsNet = models.MinigridInverseDynamicsNet
-
-MinigridMLPEmbeddingNet = models.MinigridMLPEmbeddingNet
-MinigridMLPTargetEmbeddingNet = models.MinigridMLPTargetEmbeddingNet
-
-
-def learn(actor_model,
-          model,
-          inverse_dynamics_model,
-          actor_elliptical_encoder, 
-          elliptical_encoder, 
-          batch,
-          initial_agent_state, 
-          optimizer,
-          elliptical_encoder_optimizer,
-          inverse_dynamics_optimizer,
-          scheduler,
-          flags,
-          frames=None,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-        timings = prof.Timings()
-#        timings.reset()
-
-        intrinsic_rewards = batch['bonus_reward'][1:]
-
-        # ICM loss
-        elliptical_encoder.train()
-        inverse_dynamics_model.train()
-        icm_state_emb_all, _ = elliptical_encoder(batch, tuple())
-        icm_state_emb = icm_state_emb_all[:-1]
-        icm_next_state_emb = icm_state_emb_all[1:]
-        pred_actions = inverse_dynamics_model(icm_state_emb, icm_next_state_emb)
-        inverse_dynamics_loss = losses.compute_inverse_dynamics_loss(pred_actions, batch['action'][1:])
-            
-        learner_outputs, unused_state = model(batch, initial_agent_state)
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-        
-        rewards = batch['reward']
-
-        # apply running normalization to the intrinsic or extrinsic reward
-        if flags.reward_norm == 'int':
-            model.update_running_moments(intrinsic_rewards)
-            std = model.get_running_std()
-            if std > 0:
-                intrinsic_rewards /= std
-        elif flags.reward_norm == 'ext':
-            model.update_running_moments(rewards)
-            std = model.get_running_std()
-            if std > 0:
-                rewards /= std
-                    
-        if flags.no_reward:
-            total_rewards = intrinsic_rewards * flags.intrinsic_reward_coef
-        else:            
-            total_rewards = rewards + intrinsic_rewards * flags.intrinsic_reward_coef
-
-        if flags.reward_norm == 'all':
-            model.update_running_moments(total_rewards)
-            std = model.get_running_std()
-            if std > 0:
-                total_rewards /= std
-            
-
-        if flags.clip_rewards == 1:
-            clipped_rewards = torch.clamp(total_rewards, -1, 1)
-        else:
-            clipped_rewards = total_rewards
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        # compute policy losses
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss + inverse_dynamics_loss
-
-        
-        episode_returns = batch['episode_return'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-            'inverse_dynamics_loss': inverse_dynamics_loss.item(),
-            'mean_rewards': torch.mean(rewards).item(),
-            'mean_intrinsic_rewards': torch.mean(intrinsic_rewards).item(),
-            'mean_total_rewards': torch.mean(total_rewards).item(),
-        }
-
-
-        # update networks
-        optimizer.zero_grad()
-        elliptical_encoder_optimizer.zero_grad()
-        inverse_dynamics_optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(inverse_dynamics_model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(elliptical_encoder.parameters(), flags.max_grad_norm)
-
-        optimizer.step()
-        elliptical_encoder_optimizer.step()
-        inverse_dynamics_optimizer.step()        
-
-        actor_model.load_state_dict(model.state_dict())
-        actor_elliptical_encoder.load_state_dict(elliptical_encoder.state_dict())
-        return stats, None
-
-
-
-    
-    
-
-def train(flags):
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'-eb_{flags.episodic_bonus_type}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-plr_{flags.predictor_learning_rate}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-intweight_{flags.intrinsic_reward_coef}'
-    xpid += f'-ridge_{flags.ridge}'
-    xpid += f'-cr_{flags.clip_rewards}'
-    xpid += f'-rn_{flags.reward_norm}'
-    xpid += f'-seed_{flags.seed}'
-
-    flags.xpid = xpid
-        
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-
-    checkpointpath = os.path.expandvars(
-        os.path.expanduser('%s/%s/%s' % (flags.savedir, flags.xpid,
-                                         'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device(f'cuda:{flags.device}')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-
-    env = create_env(flags)
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)  
-        
-    if 'MiniHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm)
-        elliptical_encoder = NetHackStateEmbeddingNet(env.observation_space, False) # do not use LSTM for encoder
-        inverse_dynamics_model = MinigridInverseDynamicsNet(env.action_space.n, emb_size=1024)\
-            .to(device=flags.device) 
-
-    elif 'Vizdoom' in flags.env:
-        model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-        elliptical_encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape)
-        inverse_dynamics_model = models.MarioDoomInverseDynamicsNet(env.action_space.n)\
-            .to(device=flags.device) 
-        heatmap_buffers = create_heatmap_buffers(env.observation_space.shape)
-        
-    else:
-        raise Exception('Only MiniHack and Vizdoom are suppported Now!')
-
-
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-    model.share_memory()
-    elliptical_encoder.share_memory()
-    
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.Queue()
-    full_queue = ctx.Queue()
-
-    episode_state_count_dict = dict()
-    
-    for i in range(flags.num_actors):
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, elliptical_encoder, buffers, 
-                  episode_state_count_dict, initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-
-    if 'MiniHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim).to(flags.device)
-        elliptical_learner_encoder = NetHackStateEmbeddingNet(env.observation_space, False, hidden_dim=flags.hidden_dim, p_dropout=flags.dropout).to(flags.device)
-        elliptical_learner_encoder.load_state_dict(elliptical_encoder.state_dict())
-        
-    elif 'Vizdoom' in flags.env:
-        learner_model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n).to(flags.device)
-        elliptical_learner_encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape).to(flags.device)
-        elliptical_learner_encoder.load_state_dict(elliptical_encoder.state_dict())
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-
-    elliptical_encoder_optimizer = torch.optim.Adam(
-        elliptical_learner_encoder.parameters(), 
-        lr=flags.predictor_learning_rate)
-    
-    inverse_dynamics_optimizer = torch.optim.Adam(
-        inverse_dynamics_model.parameters(), 
-        lr=flags.predictor_learning_rate)
-    
-
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'total_loss',
-        'mean_episode_return',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-        'inverse_dynamics_loss',
-        'mean_rewards',
-        'mean_intrinsic_rewards',
-        'mean_total_rewards',
-    ]
-
-        
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-
-    frames, stats = 0, {}
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                initial_agent_state_buffers, flags, timings)
-            stats, decoder_logits = learn(model, learner_model, inverse_dynamics_model,
-                                          elliptical_encoder,
-                                          elliptical_learner_encoder, batch, agent_state, optimizer, 
-                                          elliptical_encoder_optimizer, inverse_dynamics_optimizer, scheduler,
-                                          flags, frames=frames)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-                
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []    
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-
-
-    def checkpoint(frames):
-        checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'frames': frames,
-            'model_state_dict': model.state_dict(),
-            'elliptical_encoder_state_dict': elliptical_encoder.state_dict(),
-            'inverse_dynamics_model_state_dict': inverse_dynamics_model.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60:  
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-
-            total_loss = stats.get('total_loss', float('inf'))
-            if stats:
-                log.info('After %i frames: loss %f @ %.1f fps. Mean Return %.1f. \n Stats \n %s', \
-                        frames, total_loss, fps, stats['mean_episode_return'], pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return  
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-
-    checkpoint(frames)
-    plogger.close()
diff --git a/minihack/src/algos/e3b_noveld.py b/minihack/src/algos/e3b_noveld.py
deleted file mode 100644
index e0314a2..0000000
--- a/minihack/src/algos/e3b_noveld.py
+++ /dev/null
@@ -1,483 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-import os
-import sys
-import threading
-import time
-import timeit
-import pprint
-import json
-import pdb
-import contextlib
-
-import numpy as np
-import random
-import copy
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack, Environment
-from src.utils import get_batch, log, create_env, create_buffers, act, create_heatmap_buffers
-
-
-
-NetHackStateEmbeddingNet = models.NetHackStateEmbeddingNet
-MinigridInverseDynamicsNet = models.MinigridInverseDynamicsNet
-
-MinigridMLPEmbeddingNet = models.MinigridMLPEmbeddingNet
-MinigridMLPTargetEmbeddingNet = models.MinigridMLPTargetEmbeddingNet
-
-
-def learn(actor_model,
-          model,
-          inverse_dynamics_model,
-          random_target_network,
-          predictor_network,
-          random_encoder,
-          actor_elliptical_encoder, 
-          elliptical_encoder, 
-          batch,
-          initial_agent_state, 
-          optimizer,
-          elliptical_encoder_optimizer,
-          inverse_dynamics_optimizer,
-          predictor_optimizer, 
-          scheduler,
-          flags,
-          frames=None,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-        timings = prof.Timings()
-#        timings.reset()
-
-        episodic_bonus = batch['bonus_reward'][1:]
-        intrinsic_rewards = episodic_bonus
-        
-
-        # NovelD/RND bonus
-        with torch.no_grad():
-            encoded_states, unused_state = random_encoder(batch, tuple())
-            
-        random_embedding_next, unused_state = random_target_network(encoded_states[1:].detach(), initial_agent_state)
-        predicted_embedding_next, unused_state = predictor_network(encoded_states[1:].detach(), initial_agent_state)
-        random_embedding, unused_state = random_target_network(encoded_states[:-1].detach(), initial_agent_state)
-        predicted_embedding, unused_state = predictor_network(encoded_states[:-1].detach(), initial_agent_state)
-
-        intrinsic_rewards_noveld_next = torch.norm(predicted_embedding_next.detach() - random_embedding_next.detach(), dim=2, p=2)
-        intrinsic_rewards_noveld = torch.norm(predicted_embedding.detach() - random_embedding.detach(), dim=2, p=2)
-        intrinsic_rewards_noveld = torch.clamp(intrinsic_rewards_noveld_next - flags.scale_fac * intrinsic_rewards_noveld, min=0)
-
-        if flags.bonus_combine == 'mult':
-            intrinsic_rewards *= intrinsic_rewards_noveld
-        elif flags.bonus_combine == 'add':
-            intrinsic_rewards += flags.global_bonus_coeff*intrinsic_rewards_noveld
-
-        rnd_loss = flags.rnd_loss_coef * \
-                losses.compute_rnd_loss(predicted_embedding_next, random_embedding_next.detach())
-        
-
-
-        # ICM loss
-        elliptical_encoder.train()
-        inverse_dynamics_model.train()
-        icm_state_emb_all, _ = elliptical_encoder(batch, tuple())
-        icm_state_emb = icm_state_emb_all[:-1]
-        icm_next_state_emb = icm_state_emb_all[1:]
-        pred_actions = inverse_dynamics_model(icm_state_emb, icm_next_state_emb)
-        inverse_dynamics_loss = losses.compute_inverse_dynamics_loss(pred_actions, batch['action'][1:])
-        
-            
-        learner_outputs, unused_state = model(batch, initial_agent_state)
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-        
-        rewards = batch['reward']
-
-        if flags.reward_norm == 'int':
-            model.update_running_moments(intrinsic_rewards)
-            std = model.get_running_std()
-            if std > 0:
-                intrinsic_rewards /= std
-        elif flags.reward_norm == 'ext':
-            model.update_running_moments(rewards)
-            std = model.get_running_std()
-            if std > 0:
-                rewards /= std
-        
-                    
-        if flags.no_reward:
-            total_rewards = intrinsic_rewards * flags.intrinsic_reward_coef
-        else:            
-            total_rewards = rewards + intrinsic_rewards * flags.intrinsic_reward_coef
-
-        if flags.reward_norm == 'all':
-            model.update_running_moments(total_rewards)
-            std = model.get_running_std()
-            if std > 0:
-                total_rewards /= std
-            
-
-        if flags.clip_rewards == 1:
-            clipped_rewards = torch.clamp(total_rewards, -1, 1)
-        else:
-            clipped_rewards = total_rewards
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss + inverse_dynamics_loss + rnd_loss
-
-
-        
-        episode_returns = batch['episode_return'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-            'inverse_dynamics_loss': inverse_dynamics_loss.item(),
-            'rnd_loss': rnd_loss.item(),
-            'mean_rewards': torch.mean(rewards).item(),
-            'episodic_rewards': torch.mean(episodic_bonus).item(),
-            'noveld_rewards': torch.mean(intrinsic_rewards_noveld).item(),
-            'mean_intrinsic_rewards': torch.mean(intrinsic_rewards).item(),
-            'mean_total_rewards': torch.mean(total_rewards).item(),
-        }
-
-        
-        optimizer.zero_grad()
-        predictor_optimizer.zero_grad()
-        elliptical_encoder_optimizer.zero_grad()
-        inverse_dynamics_optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(inverse_dynamics_model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(elliptical_encoder.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(predictor_network.parameters(), flags.max_grad_norm)
-
-        optimizer.step()
-        predictor_optimizer.step()
-        elliptical_encoder_optimizer.step()
-        inverse_dynamics_optimizer.step()
-        
-
-        actor_model.load_state_dict(model.state_dict())
-        actor_elliptical_encoder.load_state_dict(elliptical_encoder.state_dict())
-        return stats, None
-
-
-
-    
-    
-
-def train(flags):
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'-eb_{flags.episodic_bonus_type}'
-    xpid += f'-cb_{flags.bonus_combine}'
-    xpid += f'-gbc_{flags.global_bonus_coeff}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-plr_{flags.predictor_learning_rate}'
-    xpid += f'-sf_{flags.scale_fac}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-intweight_{flags.intrinsic_reward_coef}'
-    xpid += f'-ridge_{flags.ridge}'
-    xpid += f'-cr_{flags.clip_rewards}'
-    xpid += f'-rn_{flags.reward_norm}'
-    xpid += f'-seed_{flags.seed}'
-
-        
-    flags.xpid = xpid
-        
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-
-    checkpointpath = os.path.expandvars(
-        os.path.expanduser('%s/%s/%s' % (flags.savedir, flags.xpid,
-                                         'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device(f'cuda:{flags.device}')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-
-    env = create_env(flags)
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)  
-        
-    if 'MiniHack' in flags.env or 'NetHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm)
-        elliptical_encoder = NetHackStateEmbeddingNet(env.observation_space, False) # do not use LSTM for encoder
-        inverse_dynamics_model = MinigridInverseDynamicsNet(env.action_space.n, emb_size=1024)\
-            .to(device=flags.device)
-        random_target_network = MinigridMLPTargetEmbeddingNet(hidden_dim=flags.hidden_dim).to(device=flags.device) 
-        predictor_network = MinigridMLPEmbeddingNet(hidden_dim=flags.hidden_dim).to(device=flags.device) 
-        
-    elif 'Vizdoom' in flags.env:
-        model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-        encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape)
-        elliptical_encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape)
-        inverse_dynamics_model = models.MarioDoomInverseDynamicsNet(env.action_space.n)\
-            .to(device=flags.device) 
-        heatmap_buffers = create_heatmap_buffers(env.observation_space.shape)
-        
-    else:
-        raise Exception('Only MiniHack and Vizdoom are suppported Now!')
-
-
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-    model.share_memory()
-    elliptical_encoder.share_memory()
-    
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.Queue()
-    full_queue = ctx.Queue()
-
-    episode_state_count_dict = dict()
-    
-    for i in range(flags.num_actors):
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, elliptical_encoder, buffers, 
-                  episode_state_count_dict, None, initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-
-    if 'MiniHack' in flags.env or 'NetHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim).to(flags.device)
-        random_encoder = NetHackStateEmbeddingNet(env.observation_space, False, hidden_dim=flags.hidden_dim).to(flags.device)
-        elliptical_learner_encoder = NetHackStateEmbeddingNet(env.observation_space, False, hidden_dim=flags.hidden_dim).to(flags.device)
-        elliptical_learner_encoder.load_state_dict(elliptical_encoder.state_dict())
-        
-
-    elif 'Vizdoom' in flags.env:
-        learner_model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n).to(flags.device)
-        learner_encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape).to(flags.device)
-        elliptical_learner_encoder = models.MarioDoomStateEmbeddingNet(env.observation_space.shape).to(flags.device)
-        elliptical_learner_encoder.load_state_dict(elliptical_encoder.state_dict())
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-
-    elliptical_encoder_optimizer = torch.optim.Adam(
-        elliptical_learner_encoder.parameters(), 
-        lr=flags.predictor_learning_rate)
-    
-    inverse_dynamics_optimizer = torch.optim.Adam(
-        inverse_dynamics_model.parameters(), 
-        lr=flags.predictor_learning_rate)
-
-    predictor_optimizer = torch.optim.Adam(
-        predictor_network.parameters(), 
-        lr=flags.predictor_learning_rate)
-    
-    
-
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'total_loss',
-        'mean_episode_return',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-        'inverse_dynamics_loss',
-        'mean_rewards',
-        'episodic_rewards',
-        'noveld_rewards',
-#        'global_rewards',
-        'mean_intrinsic_rewards',
-        'mean_total_rewards',
-    ]
-
-    if flags.episodic_bonus_type == 'elliptical-icm-global':
-        stat_keys += ['global_elliptical_rewards']
-        
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-
-    frames, stats = 0, {}
-
-
-    if os.path.exists(checkpointpath):
-        print(f'[loading checkpoint: {checkpointpath}]')
-        checkpoint = torch.load(checkpointpath)
-        model.load_state_dict(checkpoint['model_state_dict'])
-        learner_model.load_state_dict(checkpoint['model_state_dict'])
-        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
-        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
-        encoder.load_state_dict(checkpoint['encoder'])
-        learner_encoder.load_state_dict(checkpoint['encoder'])
-        elliptical_encoder.load_state_dict(checkpoint['elliptical_encoder_state_dict'])
-        elliptical_learner_encoder.load_state_dict(checkpoint['elliptical_encoder_state_dict'])
-        inverse_dynamics_model.load_state_dict(checkpoint['inverse_dynamics_model_state_dict'])
-        predictor_network.load_state_dict(checkpoint['predictor_network_state_dict'])
-        predictor_optimizer.load_state_dict(checkpoint['predictor_optimizer_state_dict'])
-        random_target_network.load_state_dict(checkpoint['random_target_network_state_dict'])
-        elliptical_encoder_optimizer.load_state_dict(checkpoint['elliptical_encoder_optimizer_state_dict'])
-        inverse_dynamics_optimizer.load_state_dict(checkpoint['inverse_dynamics_optimizer_state_dict'])
-        
-        frames = checkpoint['frames']
-    
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        batches = []
-        
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                initial_agent_state_buffers, flags, timings)
-            stats, decoder_logits = learn(model, learner_model,
-                                          inverse_dynamics_model,
-                                          random_target_network, predictor_network,
-                                          random_encoder,
-                                          elliptical_encoder,
-                                          elliptical_learner_encoder, batch, agent_state, optimizer, 
-                                          elliptical_encoder_optimizer, inverse_dynamics_optimizer,
-                                          predictor_optimizer, scheduler,
-                                          flags, frames=frames)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-                
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []    
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-
-
-    def checkpoint(frames):
-        checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'frames': frames,
-            'model_state_dict': model.state_dict(),
-            'random_target_network_state_dict': random_target_network.state_dict(),
-            'predictor_network_state_dict': predictor_network.state_dict(),
-            'elliptical_encoder_state_dict': elliptical_encoder.state_dict(),
-            'inverse_dynamics_model_state_dict': inverse_dynamics_model.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'elliptical_encoder_optimizer_state_dict': elliptical_encoder_optimizer.state_dict(),
-            'inverse_dynamics_optimizer_state_dict': inverse_dynamics_optimizer.state_dict(),
-            'predictor_optimizer_state_dict': predictor_optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60:  
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-
-            total_loss = stats.get('total_loss', float('inf'))
-            if stats:
-                log.info('After %i frames: loss %f @ %.1f fps. Mean Return %.1f. \n Stats \n %s', \
-                        frames, total_loss, fps, stats['mean_episode_return'], pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return  
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-
-    checkpoint(frames)
-    plogger.close()
-
diff --git a/minihack/src/algos/ride.py b/minihack/src/algos/ride.py
deleted file mode 100644
index a2dfe4c..0000000
--- a/minihack/src/algos/ride.py
+++ /dev/null
@@ -1,403 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-import os
-import threading
-import time
-import timeit
-import pprint
-import json
-
-import numpy as np
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack
-from src.utils import get_batch, log, create_env, create_buffers, act, create_heatmap_buffers
-
-NetHackStateEmbeddingNet = models.NetHackStateEmbeddingNet
-
-MinigridForwardDynamicsNet = models.MinigridForwardDynamicsNet
-MinigridInverseDynamicsNet = models.MinigridInverseDynamicsNet
-
-
-def learn(actor_model,
-          model,
-          state_embedding_model,
-          forward_dynamics_model,
-          inverse_dynamics_model,
-          batch,
-          initial_agent_state, 
-          optimizer,
-          state_embedding_optimizer, 
-          forward_dynamics_optimizer, 
-          inverse_dynamics_optimizer, 
-          scheduler,
-          flags,
-          frames=None,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-        count_rewards = torch.ones((flags.unroll_length, flags.batch_size), 
-            dtype=torch.float32).to(device=flags.device)
-        count_rewards = batch['episode_state_count'][1:].float().to(device=flags.device)
-        
-        state_emb_all, _ = state_embedding_model(batch, tuple())
-        state_emb = state_emb_all[:-1]
-        next_state_emb = state_emb_all[1:]
-        
-        pred_next_state_emb = forward_dynamics_model(
-            state_emb, batch['action'][1:].to(device=flags.device))
-        pred_actions = inverse_dynamics_model(state_emb, next_state_emb) 
-
-        control_rewards = torch.norm(next_state_emb - state_emb, dim=2, p=2)
-
-        intrinsic_rewards = count_rewards * control_rewards
-        
-        intrinsic_reward_coef = flags.intrinsic_reward_coef
-        intrinsic_rewards *= intrinsic_reward_coef 
-        
-        forward_dynamics_loss = flags.forward_loss_coef * \
-            losses.compute_forward_dynamics_loss(pred_next_state_emb, next_state_emb)
-
-        inverse_dynamics_loss = flags.inverse_loss_coef * \
-            losses.compute_inverse_dynamics_loss(pred_actions, batch['action'][1:])
-
-        learner_outputs, unused_state = model(batch, initial_agent_state)
-    
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-
-        
-        rewards = batch['reward'] 
-        if flags.no_reward:
-            total_rewards = intrinsic_rewards
-        else:            
-            total_rewards = rewards + intrinsic_rewards
-        clipped_rewards = torch.clamp(total_rewards, -1, 1)
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = flags.pg_loss_coef * losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss + \
-                    forward_dynamics_loss + inverse_dynamics_loss
-
-        episode_returns = batch['episode_return'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-            'mean_rewards': torch.mean(rewards).item(),
-            'mean_intrinsic_rewards': torch.mean(intrinsic_rewards).item(),
-            'mean_total_rewards': torch.mean(total_rewards).item(),
-            'mean_control_rewards': torch.mean(control_rewards).item(),
-            'mean_count_rewards': torch.mean(count_rewards).item(),
-            'forward_dynamics_loss': forward_dynamics_loss.item(),
-            'inverse_dynamics_loss': inverse_dynamics_loss.item(),
-        }
-        
-        optimizer.zero_grad()
-        state_embedding_optimizer.zero_grad()
-        forward_dynamics_optimizer.zero_grad()
-        inverse_dynamics_optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(state_embedding_model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(forward_dynamics_model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(inverse_dynamics_model.parameters(), flags.max_grad_norm)
-        optimizer.step()
-        state_embedding_optimizer.step()
-        forward_dynamics_optimizer.step()
-        inverse_dynamics_optimizer.step()
-#        scheduler.step()
-
-        actor_model.load_state_dict(model.state_dict())
-        return stats
-
-
-def train(flags):
-
-
-    '''
-    if flags.xpid is None:
-        flags.xpid = 'ride-%s' % time.strftime('%Y%m%d-%H%M%S')
-    '''
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'model_{flags.model}'
-    xpid += f'-bt_{flags.episodic_bonus_type}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-fc_{flags.forward_loss_coef}'
-    xpid += f'-ic_{flags.inverse_loss_coef}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-pc_{flags.pg_loss_coef}'
-    xpid += f'-intweight_{flags.intrinsic_reward_coef}'
-    xpid += f'-scalefac_{flags.scale_fac}'
-    xpid += f'-seed_{flags.seed}'
-
-    flags.xpid = xpid
-    
-
-
-
-        
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-    checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-    flags.device = None
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device('cuda')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-
-    env = create_env(flags)
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)  
-
-
-
-    if 'MiniHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm)
-        state_embedding_model = NetHackStateEmbeddingNet(env.observation_space, False).to(flags.device)
-        inverse_dynamics_model = MinigridInverseDynamicsNet(env.action_space.n, emb_size=1024)\
-            .to(device=flags.device) 
-        forward_dynamics_model = MinigridForwardDynamicsNet(env.action_space.n)\
-            .to(device=flags.device)
-
-    elif 'Vizdoom' in flags.env:
-        model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-        state_embedding_model = models.MarioDoomStateEmbeddingNet(env.observation_space.shape).to(flags.device)
-        inverse_dynamics_model = models.MarioDoomInverseDynamicsNet(env.action_space.n)\
-            .to(device=flags.device) 
-        forward_dynamics_model = models.MarioDoomForwardDynamicsNet(env.action_space.n)\
-            .to(device=flags.device)
-        
-        
-
-        
-
-
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-    
-    model.share_memory()
-
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-    
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.Queue()
-    full_queue = ctx.Queue()
-    
-    episode_state_count_dict = dict()
-    for i in range(flags.num_actors):
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, None, buffers, 
-                episode_state_count_dict, initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-
-
-    if 'MiniHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim).to(flags.device)
-    elif 'Vizdoom' in flags.env:
-        learner_model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n).to(flags.device)
-
-
-        
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-
-    state_embedding_optimizer = torch.optim.RMSprop(
-        state_embedding_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-    inverse_dynamics_optimizer = torch.optim.RMSprop(
-        inverse_dynamics_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-    forward_dynamics_optimizer = torch.optim.RMSprop(
-        forward_dynamics_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-        
-
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'total_loss',
-        'mean_episode_return',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-        'mean_rewards',
-        'mean_intrinsic_rewards',
-        'mean_total_rewards',
-        'mean_control_rewards',
-        'mean_count_rewards',
-        'forward_dynamics_loss',
-        'inverse_dynamics_loss',
-    ]
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-    frames, stats = 0, {}
-
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                initial_agent_state_buffers, flags, timings)
-            stats = learn(model, learner_model, state_embedding_model, forward_dynamics_model, 
-                          inverse_dynamics_model, batch, agent_state, optimizer, 
-                          state_embedding_optimizer, forward_dynamics_optimizer, 
-                          inverse_dynamics_optimizer, scheduler, flags, frames=frames)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-    
-    
-    def checkpoint(frames):
-        if flags.disable_checkpoint:
-            return
-        checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model_'+str(frames)+'.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'model_state_dict': model.state_dict(),
-            'state_embedding_model_state_dict': state_embedding_model.state_dict(),
-            'forward_dynamics_model_state_dict': forward_dynamics_model.state_dict(),
-            'inverse_dynamics_model_state_dict': inverse_dynamics_model.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'state_embedding_optimizer_state_dict': state_embedding_optimizer.state_dict(),
-            'forward_dynamics_optimizer_state_dict': forward_dynamics_optimizer.state_dict(),
-            'inverse_dynamics_optimizer_state_dict': inverse_dynamics_optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60:  
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-            total_loss = stats.get('total_loss', float('inf'))
-            log.info('After %i frames: loss %f @ %.1f fps. %sStats:\n%s',
-                         frames, total_loss, fps, mean_return,
-                         pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return  
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-        
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-    checkpoint(frames)
-    plogger.close()
diff --git a/minihack/src/algos/rnd.py b/minihack/src/algos/rnd.py
deleted file mode 100644
index db4ee67..0000000
--- a/minihack/src/algos/rnd.py
+++ /dev/null
@@ -1,382 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-import os
-import sys
-import threading
-import time
-import timeit
-import pprint
-
-import numpy as np
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack
-from src.utils import get_batch, log, create_env, create_buffers, act
-
-NetHackStateEmbeddingNet = models.NetHackStateEmbeddingNet
-
-def learn(actor_model,
-          model,
-          random_target_network,
-          predictor_network,
-          batch,
-          initial_agent_state, 
-          optimizer,
-          predictor_optimizer,
-          scheduler,
-          flags,
-          frames=None,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-        '''
-        if flags.use_fullobs_intrinsic:
-            random_embedding = random_target_network(batch, next_state=True)\
-                    .reshape(flags.unroll_length, flags.batch_size, 128)        
-            predicted_embedding = predictor_network(batch, next_state=True)\
-                    .reshape(flags.unroll_length, flags.batch_size, 128)
-        else:
-            random_embedding = random_target_network(batch['partial_obs'][1:].to(device=flags.device))
-            predicted_embedding = predictor_network(batch['partial_obs'][1:].to(device=flags.device))
-        '''
-
-        random_embedding, _ = random_target_network(batch, tuple())
-        predicted_embedding, _ = predictor_network(batch, tuple())
-        random_embedding = random_embedding[1:]
-        predicted_embedding = predicted_embedding[1:]
-
-        intrinsic_rewards = torch.norm(predicted_embedding.detach() - random_embedding.detach(), dim=2, p=2)
-
-        intrinsic_reward_coef = flags.intrinsic_reward_coef
-        intrinsic_rewards *= intrinsic_reward_coef 
-        
-        num_samples = flags.unroll_length * flags.batch_size
-        actions_flat = batch['action'][1:].reshape(num_samples).cpu().detach().numpy()
-        intrinsic_rewards_flat = intrinsic_rewards.reshape(num_samples).cpu().detach().numpy()
-
-        rnd_loss = flags.rnd_loss_coef * \
-                losses.compute_forward_dynamics_loss(predicted_embedding, random_embedding.detach()) 
-            
-        learner_outputs, unused_state = model(batch, initial_agent_state)
-
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-        
-        rewards = batch['reward']
-            
-        if flags.no_reward:
-            total_rewards = intrinsic_rewards
-        else:            
-            total_rewards = rewards + intrinsic_rewards
-        clipped_rewards = torch.clamp(total_rewards, -1, 1)
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss + rnd_loss
-
-        episode_returns = batch['episode_return'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-            'rnd_loss': rnd_loss.item(),
-            'mean_rewards': torch.mean(rewards).item(),
-            'mean_intrinsic_rewards': torch.mean(intrinsic_rewards).item(),
-            'mean_total_rewards': torch.mean(total_rewards).item(),
-        }
-        
-#        scheduler.step()
-        optimizer.zero_grad()
-        predictor_optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        nn.utils.clip_grad_norm_(predictor_network.parameters(), flags.max_grad_norm)
-        optimizer.step()
-        predictor_optimizer.step()
-
-        actor_model.load_state_dict(model.state_dict())
-        return stats
-
-
-def train(flags):  
-#    if flags.xpid is None:
-#        flags.xpid = 'rnd-%s' % time.strftime('%Y%m%d-%H%M%S')
-
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'model_{flags.model}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-intweight_{flags.intrinsic_reward_coef}'
-    xpid += f'-seed_{flags.seed}'
-
-    flags.xpid = xpid
-    
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-
-    checkpointpath = os.path.expandvars(
-        os.path.expanduser('%s/%s/%s' % (flags.savedir, flags.xpid,
-                                         'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-    flags.device = None
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device('cuda')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-
-    env = create_env(flags)
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)  
-
-
-    if 'MiniHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm)
-        random_target_network = NetHackStateEmbeddingNet(env.observation_space, False).to(flags.device)
-        predictor_network = NetHackStateEmbeddingNet(env.observation_space, False).to(flags.device)
-
-    elif 'Vizdoom' in flags.env:
-        model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-        state_embedding_model = models.MarioDoomStateEmbeddingNet(env.observation_space.shape).to(flags.device)
-
-
-        
-    '''
-    if 'MiniGrid' in flags.env: 
-        if flags.use_fullobs_policy:
-            model = FullObsMinigridPolicyNet(env.observation_space.shape, env.action_space.n)                        
-        else:
-            model = MinigridPolicyNet(env.observation_space.shape, env.action_space.n)    
-        if flags.use_fullobs_intrinsic:                        
-            random_target_network = FullObsMinigridStateEmbeddingNet(env.observation_space.shape).to(device=flags.device) 
-            predictor_network = FullObsMinigridStateEmbeddingNet(env.observation_space.shape).to(device=flags.device)             
-        else:
-            random_target_network = MinigridStateEmbeddingNet(env.observation_space.shape).to(device=flags.device) 
-            predictor_network = MinigridStateEmbeddingNet(env.observation_space.shape).to(device=flags.device) 
-    else:
-        model = MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-        random_target_network = MarioDoomStateEmbeddingNet(env.observation_space.shape).to(device=flags.device) 
-        predictor_network = MarioDoomStateEmbeddingNet(env.observation_space.shape).to(device=flags.device) 
-    '''
-    
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-    
-    model.share_memory()
-    
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.SimpleQueue()
-    full_queue = ctx.SimpleQueue()
-
-    episode_state_count_dict = dict()
-    train_state_count_dict = dict()
-    for i in range(flags.num_actors):
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, None, buffers, 
-                episode_state_count_dict, 
-                initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-
-
-    if 'MiniHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim).to(flags.device)
-    elif 'Vizdoom' in flags.env:
-        learner_model = models.MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n).to(flags.device)
-
-        
-    '''
-    if 'MiniGrid' in flags.env: 
-        if flags.use_fullobs_policy:
-            learner_model = FullObsMinigridPolicyNet(env.observation_space.shape, env.action_space.n)\
-                .to(device=flags.device)
-        else:
-            learner_model = MinigridPolicyNet(env.observation_space.shape, env.action_space.n)\
-                .to(device=flags.device)
-    else:
-        learner_model = MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)\
-            .to(device=flags.device)
-    '''
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-    predictor_optimizer = torch.optim.RMSprop(
-        predictor_network.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-    
-
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'total_loss',
-        'mean_episode_return',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-        'rnd_loss',
-        'mean_rewards',
-        'mean_intrinsic_rewards',
-        'mean_total_rewards',
-    ]
-
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-
-    frames, stats = 0, {}
-
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                initial_agent_state_buffers, flags, timings)
-            stats = learn(model, learner_model, random_target_network, predictor_network,
-                          batch, agent_state, optimizer, predictor_optimizer, scheduler, 
-                          flags, frames=frames)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []    
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-
-
-    def checkpoint(frames):
-        if flags.disable_checkpoint:
-            return
-        checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'model_state_dict': model.state_dict(),
-            'random_target_network_state_dict': random_target_network.state_dict(),
-            'predictor_network_state_dict': predictor_network.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'predictor_optimizer_state_dict': predictor_optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60:  
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-
-            total_loss = stats.get('total_loss', float('inf'))
-            if stats:
-                log.info('After %i frames: loss %f @ %.1f fps. Mean Return %.1f. \n Stats \n %s', \
-                        frames, total_loss, fps, stats['mean_episode_return'], pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return  
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-
-    checkpoint(frames)
-    plogger.close()
diff --git a/minihack/src/algos/torchbeast.py b/minihack/src/algos/torchbeast.py
deleted file mode 100644
index b05f306..0000000
--- a/minihack/src/algos/torchbeast.py
+++ /dev/null
@@ -1,304 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import logging
-import os
-import threading
-import time
-import timeit
-import pprint
-
-import numpy as np
-
-import torch
-from torch import multiprocessing as mp
-from torch import nn
-from torch.nn import functional as F
-
-from src.core import file_writer
-from src.core import prof
-from src.core import vtrace
-
-import src.models as models
-import src.losses as losses
-
-from src.env_utils import FrameStack
-from src.utils import get_batch, log, create_env, create_buffers, act
-
-
-NetHackStateEmbeddingNet = models.NetHackStateEmbeddingNet
-
-
-def learn(actor_model,
-          model,
-          batch,
-          initial_agent_state, 
-          optimizer,
-          scheduler,
-          flags,
-          lock=threading.Lock()):
-    """Performs a learning (optimization) step."""
-    with lock:
-        learner_outputs, unused_state = model(batch, initial_agent_state)
-    
-        bootstrap_value = learner_outputs['baseline'][-1]
-
-        batch = {key: tensor[1:] for key, tensor in batch.items()}
-        learner_outputs = {
-            key: tensor[:-1]
-            for key, tensor in learner_outputs.items()
-        }
-
-        rewards = batch['reward']
-        clipped_rewards = torch.clamp(rewards, -1, 1)
-        
-        discounts = (~batch['done']).float() * flags.discounting
-
-        vtrace_returns = vtrace.from_logits(
-            behavior_policy_logits=batch['policy_logits'],
-            target_policy_logits=learner_outputs['policy_logits'],
-            actions=batch['action'],
-            discounts=discounts,
-            rewards=clipped_rewards,
-            values=learner_outputs['baseline'],
-            bootstrap_value=bootstrap_value)
-
-        pg_loss = losses.compute_policy_gradient_loss(learner_outputs['policy_logits'],
-                                               batch['action'],
-                                               vtrace_returns.pg_advantages)
-        baseline_loss = flags.baseline_cost * losses.compute_baseline_loss(
-            vtrace_returns.vs - learner_outputs['baseline'])
-        entropy_loss = flags.entropy_cost * losses.compute_entropy_loss(
-            learner_outputs['policy_logits'])
-
-        total_loss = pg_loss + baseline_loss + entropy_loss
-
-        episode_returns = batch['episode_return'][batch['done']]
-        stats = {
-            'mean_episode_return': torch.mean(episode_returns).item(),
-            'total_loss': total_loss.item(),
-            'pg_loss': pg_loss.item(),
-            'baseline_loss': baseline_loss.item(),
-            'entropy_loss': entropy_loss.item(),
-        }
-        
-        scheduler.step()
-        optimizer.zero_grad()
-        total_loss.backward()
-        nn.utils.clip_grad_norm_(model.parameters(), flags.max_grad_norm)
-        optimizer.step()
-
-        actor_model.load_state_dict(model.state_dict())
-        return stats
-
-
-def train(flags):
-
-    '''
-    if flags.xpid is None:
-        flags.xpid = 'torchbeast-%s' % time.strftime('%Y%m%d-%H%M%S')
-    '''
-
-    xpid = ''
-    xpid += f'env_{flags.env}'
-    xpid += f'model_{flags.model}'
-    xpid += f'-lr_{flags.learning_rate}'
-    xpid += f'-entropy_{flags.entropy_cost}'
-    xpid += f'-seed_{flags.seed}'
-
-    flags.xpid = xpid
-
-
-
-    
-    
-    plogger = file_writer.FileWriter(
-        xpid=flags.xpid,
-        xp_args=flags.__dict__,
-        rootdir=flags.savedir,
-    )
-    checkpointpath = os.path.expandvars(
-        os.path.expanduser('%s/%s/%s' % (flags.savedir, flags.xpid,
-                                         'model.tar')))
-
-    T = flags.unroll_length
-    B = flags.batch_size
-
-#    flags.device = None
-    if not flags.disable_cuda and torch.cuda.is_available():
-        log.info('Using CUDA.')
-        flags.device = torch.device(f'cuda:{flags.device}')
-    else:
-        log.info('Not using CUDA.')
-        flags.device = torch.device('cpu')
-    env = create_env(flags)
-    if flags.num_input_frames > 1:
-        env = FrameStack(env, flags.num_input_frames)
-
-    '''
-    if 'MiniGrid' in flags.env: 
-        model = MinigridPolicyNet(env.observation_space.shape, env.action_space.n)
-    else:
-        model = MarioDoomPolicyNet(env.observation_space.shape, env.action_space.n)
-    '''
-
-
-    if 'MiniGrid' in flags.env: 
-        if flags.use_fullobs_policy:
-            raise Exception('We have not implemented full ob policy!')
-        else:
-            model = MinigridPolicyNet(env.observation_space.shape, env.action_space.n)        
-    elif 'MiniHack' in flags.env:
-        model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim)
-    else:
-        raise Exception('Only MiniGrid is suppported Now!')
-    
-
-    buffers = create_buffers(env.observation_space, model.num_actions, flags)
-    
-    model.share_memory()
-
-    initial_agent_state_buffers = []
-    for _ in range(flags.num_buffers):
-        state = model.initial_state(batch_size=1)
-        for t in state:
-            t.share_memory_()
-        initial_agent_state_buffers.append(state)
-
-    
-    
-    actor_processes = []
-    ctx = mp.get_context('fork')
-    free_queue = ctx.SimpleQueue()
-    full_queue = ctx.SimpleQueue()
-
-    episode_state_count_dict = dict()
-    train_state_count_dict = dict()
-    partial_state_count_dict = dict()
-    encoded_state_count_dict = dict()
-    heatmap_dict = dict()    
-    for i in range(flags.num_actors):
-        actor = ctx.Process(
-            target=act,
-            args=(i, free_queue, full_queue, model, None, buffers, 
-                  episode_state_count_dict,
-                  initial_agent_state_buffers, flags))
-        actor.start()
-        actor_processes.append(actor)
-
-
-    if 'MiniHack' in flags.env:
-        learner_model = models.NetHackPolicyNet(env.observation_space, env.action_space.n, flags.use_lstm, hidden_dim=flags.hidden_dim, sphere_norm=flags.sphere_norm).to(flags.device)
-    else:
-        raise Exception('Only MiniGrid is suppported Now!')
-
-    
-
-    optimizer = torch.optim.RMSprop(
-        learner_model.parameters(),
-        lr=flags.learning_rate,
-        momentum=flags.momentum,
-        eps=flags.epsilon,
-        alpha=flags.alpha)
-
-
-    def lr_lambda(epoch):
-        return 1 - min(epoch * T * B, flags.total_frames) / flags.total_frames
-
-    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
-
-    logger = logging.getLogger('logfile')
-    stat_keys = [
-        'mean_episode_return',
-        'total_loss',
-        'pg_loss',
-        'baseline_loss',
-        'entropy_loss',
-    ]
-    logger.info('# Step\t%s', '\t'.join(stat_keys))
-    frames, stats = 0, {}
-
-
-    def batch_and_learn(i, lock=threading.Lock()):
-        """Thread target for the learning process."""
-        nonlocal frames, stats
-        timings = prof.Timings()
-        while frames < flags.total_frames:
-            timings.reset()
-            batch, agent_state = get_batch(free_queue, full_queue, buffers, 
-                                           initial_agent_state_buffers, flags, timings)
-            stats = learn(model, learner_model, batch, agent_state, 
-                optimizer, scheduler, flags)
-            timings.time('learn')
-            with lock:
-                to_log = dict(frames=frames)
-                to_log.update({k: stats[k] for k in stat_keys})
-                plogger.log(to_log)
-                frames += T * B
-
-        if i == 0:
-            log.info('Batch and learn: %s', timings.summary())
-
-    for m in range(flags.num_buffers):
-        free_queue.put(m)
-
-    threads = []
-    for i in range(flags.num_threads):
-        thread = threading.Thread(
-            target=batch_and_learn, name='batch-and-learn-%d' % i, args=(i,))
-        thread.start()
-        threads.append(thread)
-    
-    def checkpoint(frames):
-        if flags.disable_checkpoint:
-            return
-        checkpointpath = os.path.expandvars(os.path.expanduser(
-            '%s/%s/%s' % (flags.savedir, flags.xpid,'model.tar')))
-        log.info('Saving checkpoint to %s', checkpointpath)
-        torch.save({
-            'model_state_dict': model.state_dict(),
-            'optimizer_state_dict': optimizer.state_dict(),
-            'scheduler_state_dict': scheduler.state_dict(),
-            'flags': vars(flags),
-        }, checkpointpath)
-
-    timer = timeit.default_timer
-    try:
-        last_checkpoint_time = timer()
-        while frames < flags.total_frames:
-            start_frames = frames
-            start_time = timer()
-            time.sleep(5)
-
-            if timer() - last_checkpoint_time > flags.save_interval * 60:  
-                checkpoint(frames)
-                last_checkpoint_time = timer()
-
-            fps = (frames - start_frames) / (timer() - start_time)
-            if stats.get('episode_returns', None):
-                mean_return = 'Return per episode: %.1f. ' % stats[
-                    'mean_episode_return']
-            else:
-                mean_return = ''
-            total_loss = stats.get('total_loss', float('inf'))
-            log.info('After %i frames: loss %f @ %.1f fps. %sStats:\n%s',
-                         frames, total_loss, fps, mean_return,
-                         pprint.pformat(stats))
-
-    except KeyboardInterrupt:
-        return 
-    else:
-        for thread in threads:
-            thread.join()
-        log.info('Learning finished after %d frames.', frames)
-        
-    finally:
-        for _ in range(flags.num_actors):
-            free_queue.put(None)
-        for actor in actor_processes:
-            actor.join(timeout=1)
-    checkpoint(frames)
-    plogger.close()
diff --git a/minihack/src/arguments.py b/minihack/src/arguments.py
deleted file mode 100644
index b469855..0000000
--- a/minihack/src/arguments.py
+++ /dev/null
@@ -1,144 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-
-
-import argparse
-
-parser = argparse.ArgumentParser(description='PyTorch Scalable Agent')
-
-# General Settings.
-parser.add_argument('--env', type=str, default='MiniHack-KeyRoom-S5-v0',
-                    help='Gym environment. Can include other MiniHack envs or Vizdoom ones')
-parser.add_argument('--xpid', default=None,
-                    help='Experiment id (default: None).')
-parser.add_argument('--num_input_frames', default=1, type=int,
-                    help='Number of input frames to the model and state embedding including the current frame \
-                    When num_input_frames > 1, it will also take the previous num_input_frames - 1 frames as input.')
-parser.add_argument('--run_id', default=0, type=int,
-                    help='Run id used for running multiple instances of the same HP set \
-                    (instead of a different random seed since torchbeast does not accept this).')
-parser.add_argument('--seed', default=0, type=int,
-                    help='Environment seed.')
-parser.add_argument('--save_interval', default=30, type=int, metavar='N',
-                    help='Time interval (in minutes) at which to save the model.')    
-parser.add_argument('--checkpoint_num_frames', default=10000000, type=int,
-                    help='Number of frames for checkpoint to load.')
-parser.add_argument('--device', default=0, type=int, help='GPU device')
-parser.add_argument('--checkpointpath', default='', type=str)
-parser.add_argument('--clip_rewards', default=1, type=int, help='Clip rewards to [-1, 1]. Set 1 for true, 0 for false.')
-parser.add_argument('--reward_norm', default='int', type=str,
-                    help='Running normalization for reward. Can be int (normalize intrinsic reward), \
-                    ext (normalize extrinsic reward) or all (normalize the sum of the two)')
-parser.add_argument('--decay_lr', default=0, type=int, help='decay learning rate (1 for true, 0 for false)')
-parser.add_argument('--num_contexts', default=-1, type=int,
-                    help='Number of distinct contexts. Set to -1 for infinite.')
-
-
-
-# Training settings.
-parser.add_argument('--disable_checkpoint', action='store_true',
-                    help='Disable saving checkpoint.')
-parser.add_argument('--savedir', default='./results/',
-                    help='Root dir where experiment data will be saved.')
-parser.add_argument('--num_actors', default=256, type=int, metavar='N',
-                    help='Number of actors.')
-parser.add_argument('--total_frames', default=5e7, type=int, metavar='T',
-                    help='Total environment frames to train for.')
-parser.add_argument('--batch_size', default=8, type=int, metavar='B',
-                    help='Learner batch size.')
-parser.add_argument('--unroll_length', default=80, type=int, metavar='T',
-                    help='The unroll length (time dimension).')
-parser.add_argument('--queue_timeout', default=1, type=int,
-                    metavar='S', help='Error timeout for queue.')
-parser.add_argument('--num_buffers', default=80, type=int,
-                    metavar='N', help='Number of shared-memory buffers.')
-parser.add_argument('--num_threads', default=4, type=int,
-                    metavar='N', help='Number learner threads.')
-parser.add_argument('--disable_cuda', action='store_true',
-                    help='Disable CUDA.')
-parser.add_argument('--max_grad_norm', default=40., type=float, metavar='MGN',
-                    help='Max norm of gradients.')
-parser.add_argument('--hidden_dim', default=1024, type=int)
-parser.add_argument('--msg_model', default="lt_cnn", type=str,
-                    help='Architecture for reading messages')
-parser.add_argument('--use_lstm', default=1, type=int,
-                    help='Use a lstm version of policy network.')
-parser.add_argument('--use_lstm_intrinsic', action='store_true',
-                    help='Use a lstm version of intrinsic embedding network.')
-parser.add_argument('--state_embedding_dim', default=256, type=int,
-                    help='Embedding dimension of last layer of network')
-parser.add_argument('--scale_fac', default=0.1, type=float,
-                    help='coefficient for scaling visitation count difference')
-
-
-# Loss settings.
-parser.add_argument('--entropy_cost', default=0.005, type=float,
-                    help='Entropy cost/multiplier.')
-parser.add_argument('--baseline_cost', default=0.5, type=float,
-                    help='Baseline cost/multiplier.')
-parser.add_argument('--discounting', default=0.99, type=float,
-                    help='Discounting factor.')
-parser.add_argument('--forward_loss_coef', default=10.0, type=float,
-                    help='Coefficient for the forward dynamics loss.')
-parser.add_argument('--inverse_loss_coef', default=1.0, type=float,
-                    help='Coefficient for the inverse dynamics loss.')
-parser.add_argument('--pg_loss_coef', default=1.0, type=float)
-
-
-# Optimizer settings.
-parser.add_argument('--learning_rate', default=0.0001, type=float,
-                    metavar='LR', help='Learning rate.')
-parser.add_argument('--predictor_learning_rate', default=0.0001, type=float,
-                    metavar='LR', help='Learning rate for RND predictor.')
-parser.add_argument('--alpha', default=0.99, type=float,
-                    help='RMSProp smoothing constant.')
-parser.add_argument('--momentum', default=0, type=float,
-                    help='RMSProp momentum.')
-parser.add_argument('--epsilon', default=1e-5, type=float,
-                    help='RMSProp epsilon.')
-
-# Exploration Settings.
-parser.add_argument('--intrinsic_reward_coef', default=1.0, type=float,
-                    help='Coefficient for the intrinsic reward. \
-                    This weighs the intrinsic reaward against the extrinsic one. \
-                    Should be larger than 0.')
-parser.add_argument('--rnd_loss_coef', default=1.0, type=float,
-                    help='Coefficient for the RND loss coefficient relative to the IMPALA one.')
-parser.add_argument('--count_reward_type', default='ind', type=str)
-parser.add_argument('--encoder_momentum_update', default=1.0, type=float)
-parser.add_argument('--episodic_bonus_type', default='counts-pos', type=str)
-parser.add_argument('--global_bonus_type', default='none', type=str)
-parser.add_argument('--bonus_combine', default='mult', type=str)
-parser.add_argument('--global_bonus_coeff', default=1.0, type=float)
-parser.add_argument('--ridge', default=0.1, type=float, help='covariance matrix regularizer for E3B')
-
-# Singleton Environments.
-parser.add_argument('--fix_seed', action='store_true',
-                    help='Fix the environment seed so that it is \
-                    no longer procedurally generated but rather the same layout every episode.')
-parser.add_argument('--env_seed', default=1, type=int,
-                    help='The seed used to generate the environment if we are using a \
-                    singleton (i.e. not procedurally generated) environment.')
-parser.add_argument('--no_reward', action='store_true',
-                    help='No extrinsic reward. The agent uses only intrinsic reward to learn.')
-
-# Training Models.
-parser.add_argument('--model', default='vanilla', type=str,
-                    help='Model used for training the agent.')
-
-parser.add_argument('--dropout', default=0.0, type=float)
-
-# Baselines for AMIGo paper.
-parser.add_argument('--use_fullobs_policy', action='store_true',
-                    help='Use a full view of the environment as input to the policy network.')
-parser.add_argument('--use_fullobs_intrinsic', action='store_true',
-                    help='Use a full view of the environment for computing the intrinsic reward.')
-parser.add_argument('--target_update_freq', default=2, type=int,
-                    help='Number of time steps for updating target')
-parser.add_argument('--init_num_frames', default=1e6, type=int,
-                    help='Number of frames for updating teacher network')
-parser.add_argument('--planning_intrinsic_reward_coef', default=0.5, type=float,
-                    help='Coefficient for the planning intrinsic reward. \
-                    This weighs the intrinsic reaward against the extrinsic one. \
-                    Should be larger than 0.')
-parser.add_argument('--ema_momentum', default=1.0, type=float,
-                    help='Coefficient for the EMA update of the RND network')
diff --git a/minihack/src/atari_wrappers.py b/minihack/src/atari_wrappers.py
deleted file mode 100644
index cff43c0..0000000
--- a/minihack/src/atari_wrappers.py
+++ /dev/null
@@ -1,353 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-# This code was taken from 
-# https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py
-# and modified.
-
-from collections import deque, defaultdict
-
-import numpy as np
-
-import gym
-from gym import spaces
-import cv2
-cv2.ocl.setUseOpenCL(False)
-
-
-class NoNegativeRewardEnv(gym.RewardWrapper):
-    """Clip reward in negative direction."""
-    def __init__(self, env=None, neg_clip=0.0):
-        super(NoNegativeRewardEnv, self).__init__(env)
-        self.neg_clip = neg_clip
-
-    def _reward(self, reward):
-        new_reward = self.neg_clip if reward < self.neg_clip else reward
-        return new_reward
-
-
-class NoopResetEnv(gym.Wrapper):
-
-    def __init__(self, env, noop_max=30):
-        """Sample initial states by taking random number of no-ops on reset.
-        No-op is assumed to be action 0.
-        """
-        gym.Wrapper.__init__(self, env)
-        self.noop_max = noop_max
-        self.override_num_noops = None
-        self.noop_action = 0
-
-    def reset(self, **kwargs):
-        """ Do no-op action for a number of steps in [1, noop_max]."""
-        self.env.reset(**kwargs)
-        if self.override_num_noops is not None:
-            noops = self.override_num_noops
-        else:
-            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  
-        assert noops > 0
-        obs = None
-        for _ in range(noops):
-            obs, _, done, _ = self.env.step(self.noop_action)
-            if done:
-                obs = self.env.reset(**kwargs)
-        return obs
-
-    def step(self, ac):
-        return self.env.step(ac)
-
-
-class FireResetEnv(gym.Wrapper):
-
-    def __init__(self, env):
-        """Take action on reset for environments that are fixed until firing."""
-        gym.Wrapper.__init__(self, env)
-
-    def reset(self, **kwargs):
-        self.env.reset(**kwargs)
-        obs, _, done, _ = self.env.step(1)
-        if done:
-            self.env.reset(**kwargs)
-        obs, _, done, _ = self.env.step(2)
-        if done:
-            self.env.reset(**kwargs)
-        return obs
-
-    def step(self, ac):
-        return self.env.step(ac)
-
-
-class EpisodicLifeEnv(gym.Wrapper):
-
-    def __init__(self, env):
-        """Make end-of-life == end-of-episode, but only reset on true game over.
-        Done by DeepMind for the DQN and co. since it helps value estimation.
-        """
-        gym.Wrapper.__init__(self, env)
-        self.lives = 0
-        self.was_real_done = True
-
-    def step(self, action):
-        obs, reward, done, info = self.env.step(action)
-        self.was_real_done = done
-        # check current lives, make loss of life terminal,
-        # then update lives to handle bonus lives
-        lives = self.env.unwrapped.ale.lives()
-        if lives < self.lives and lives > 0:
-            # for Qbert sometimes we stay in lives == 0 condition for a few frames
-            # so it's important to keep lives > 0, so that we only reset once
-            # the environment advertises done.
-            done = True
-        self.lives = lives
-        return obs, reward, done, info
-
-    def reset(self, **kwargs):
-        """Reset only when lives are exhausted.
-        This way all states are still reachable even though lives are episodic,
-        and the learner need not know about any of this behind-the-scenes.
-        """
-        if self.was_real_done:
-            obs = self.env.reset(**kwargs)
-        else:
-            # no-op step to advance from terminal/lost life state
-            obs, _, _, _ = self.env.step(0)
-        self.lives = self.env.unwrapped.ale.lives()
-        return obs
-
-
-class MaxAndSkipEnv(gym.Wrapper):
-
-    def __init__(self, env, skip=4):
-        """Return only every `skip`-th frame"""
-        gym.Wrapper.__init__(self, env)
-        # most recent raw observations (for max pooling across time steps)
-        self._obs_buffer = np.zeros(
-            (2,) + env.observation_space.shape, dtype=np.uint8)
-        self._skip = skip
-
-    def step(self, action):
-        """Repeat action, sum reward, and max over last observations."""
-        total_reward = 0.0
-        done = None
-        for i in range(self._skip):
-            obs, reward, done, info = self.env.step(action)
-            if i == self._skip - 2: self._obs_buffer[0] = obs
-            if i == self._skip - 1: self._obs_buffer[1] = obs
-            total_reward += reward
-            if done:
-                break
-        # Note that the observation on the done=True frame
-        # doesn't matter
-        max_frame = self._obs_buffer.max(axis=0)
-
-        return max_frame, total_reward, done, info
-
-    def reset(self, **kwargs):
-        return self.env.reset(**kwargs)
-
-
-class ClipRewardEnv(gym.RewardWrapper):
-
-    def __init__(self, env):
-        gym.RewardWrapper.__init__(self, env)
-
-    def reward(self, reward):
-        """Bin reward to {+1, 0, -1} by its sign."""
-        return np.sign(reward)
-
-
-class WarpFrame(gym.ObservationWrapper):
-
-    def __init__(self, env, width=84, height=84, grayscale=True):
-        """Warp frames to 84x84 as done in the Nature paper and later work."""
-        gym.ObservationWrapper.__init__(self, env)
-        self.width = width
-        self.height = height
-        self.grayscale = grayscale
-        if self.grayscale:
-            self.observation_space = spaces.Box(
-                low=0,
-                high=255,
-                shape=(self.height, self.width, 1),
-                dtype=np.uint8)
-        else:
-            self.observation_space = spaces.Box(
-                low=0,
-                high=255,
-                shape=(self.height, self.width, 3),
-                dtype=np.uint8)
-
-    def observation(self, frame):
-        if self.grayscale:
-            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
-        frame = cv2.resize(
-            frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
-        if self.grayscale:
-            frame = np.expand_dims(frame, -1)
-        return frame
-
-
-class FrameStack(gym.Wrapper):
-
-    def __init__(self, env, k):
-        """Stack k last frames.
-
-        Returns lazy array, which is much more memory efficient.
-
-        See Also
-        --------
-        baselines.common.atari_wrappers.LazyFrames
-        """
-        gym.Wrapper.__init__(self, env)
-        self.k = k
-        self.frames = deque([], maxlen=k)
-        shp = env.observation_space.shape
-        self.observation_space = spaces.Box(
-            low=0,
-            high=255,
-            shape=(shp[:-1] + (shp[-1] * k,)),
-            dtype=env.observation_space.dtype)
-
-    def reset(self):
-        ob = self.env.reset()
-        for _ in range(self.k):
-            self.frames.append(ob)
-        return self._get_ob()
-
-    def step(self, action):
-        ob, reward, done, info = self.env.step(action)
-        self.frames.append(ob)
-        return self._get_ob(), reward, done, info
-
-    def _get_ob(self):
-        assert len(self.frames) == self.k
-        return LazyFrames(list(self.frames))
-
-
-class ScaledFloatFrame(gym.ObservationWrapper):
-
-    def __init__(self, env):
-        gym.ObservationWrapper.__init__(self, env)
-        self.observation_space = gym.spaces.Box(
-            low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)
-
-    def observation(self, observation):
-        # careful! This undoes the memory optimization, use
-        # with smaller replay buffers only.
-        return np.array(observation).astype(np.float32) / 255.0
-
-
-class LazyFrames(object):
-
-    def __init__(self, frames):
-        """This object ensures that common frames between the observations are only stored once.
-        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay
-        buffers.
-
-        This object should only be converted to numpy array before being passed to the model.
-
-        You'd not believe how complex the previous solution was."""
-        self._frames = frames
-        self._out = None
-
-    def _force(self):
-        if self._out is None:
-            self._out = np.concatenate(self._frames, axis=-1)
-            self._frames = None
-        return self._out
-
-    def __array__(self, dtype=None):
-        out = self._force()
-        if dtype is not None:
-            out = out.astype(dtype)
-        return out
-
-    def __len__(self):
-        return len(self._force())
-
-    def __getitem__(self, i):
-        return self._force()[i]
-
-
-def make_atari(env_id, timelimit=True, noop=False):
-    # XXX(john): remove timelimit argument after gym is upgraded to allow double wrapping
-    env = gym.make(env_id)
-    if not timelimit:
-        env = env.env
-
-    # NOTE: this is changed from original atari implementation
-    # assert 'NoFrameskip' in env.spec.id
-    if noop:
-        env = NoopResetEnv(env, noop_max=30)
-    env = MaxAndSkipEnv(env, skip=6)
-    return env
-
-
-# NOTE: this was changed so that episode_life is False by default
-def wrap_deepmind(env,
-                  episode_life=False,
-                  clip_rewards=True,
-                  frame_stack=False,
-                  scale=True, 
-                  fire=False):  # FYI scale=False in openai/baselines
-    """Configure environment for DeepMind-style Atari.
-    """
-    if episode_life:
-        env = EpisodicLifeEnv(env)
-    if fire:
-        if 'FIRE' in env.unwrapped.get_action_meanings():
-            env = FireResetEnv(env)
-    env = WarpFrame(env, width=42, height=42)
-    if scale:
-        env = ScaledFloatFrame(env)
-    if clip_rewards:
-        env = ClipRewardEnv(env)
-    if frame_stack:
-        env = FrameStack(env, 4)
-    return env
-
-
-# Taken from https://github.com/openai/baselines/blob/master/baselines/run.py
-def get_env_type(env_id):
-    # Re-parse the gym registry, since we could have new envs since last time.
-    for env in gym.envs.registry.all():
-        env_type = env._entry_point.split(':')[0].split('.')[-1]
-        _game_envs[env_type].add(env.id)  # This is a set so add is idempotent
-
-    if env_id in _game_envs.keys():
-        env_type = env_id
-        env_id = [g for g in _game_envs[env_type]][0]
-    else:
-        env_type = None
-        for g, e in _game_envs.items():
-            if env_id in e:
-                env_type = g
-                break
-        assert env_type is not None, 'env_id {} is not recognized in env types'.format(
-            env_id, _game_envs.keys())
-
-    return env_type, env_id
-
-
-class ImageToPyTorch(gym.ObservationWrapper):
-    """
-    Image shape to channels x weight x height
-    """
-
-    def __init__(self, env):
-        super(ImageToPyTorch, self).__init__(env)
-        old_shape = self.observation_space.shape
-        self.observation_space = gym.spaces.Box(
-            low=0.0,
-            high=1.0,
-            shape=(old_shape[-1], old_shape[0], old_shape[1]),
-            dtype=np.uint8)
-
-    def observation(self, observation):
-        return np.swapaxes(observation, 2, 0)
-
-
-def wrap_pytorch(env):
-    return ImageToPyTorch(env)
diff --git a/minihack/src/core/__init__.py b/minihack/src/core/__init__.py
deleted file mode 100644
index 47d9858..0000000
--- a/minihack/src/core/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
diff --git a/minihack/src/core/__pycache__/__init__.cpython-37.pyc b/minihack/src/core/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index b0c7b2e..0000000
Binary files a/minihack/src/core/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/core/__pycache__/__init__.cpython-38.pyc b/minihack/src/core/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 204617c..0000000
Binary files a/minihack/src/core/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/core/__pycache__/file_writer.cpython-37.pyc b/minihack/src/core/__pycache__/file_writer.cpython-37.pyc
deleted file mode 100644
index f5ef97b..0000000
Binary files a/minihack/src/core/__pycache__/file_writer.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/core/__pycache__/file_writer.cpython-38.pyc b/minihack/src/core/__pycache__/file_writer.cpython-38.pyc
deleted file mode 100644
index 6021c33..0000000
Binary files a/minihack/src/core/__pycache__/file_writer.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/core/__pycache__/prof.cpython-37.pyc b/minihack/src/core/__pycache__/prof.cpython-37.pyc
deleted file mode 100644
index 5b50914..0000000
Binary files a/minihack/src/core/__pycache__/prof.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/core/__pycache__/prof.cpython-38.pyc b/minihack/src/core/__pycache__/prof.cpython-38.pyc
deleted file mode 100644
index 8c3aebf..0000000
Binary files a/minihack/src/core/__pycache__/prof.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/core/__pycache__/vtrace.cpython-37.pyc b/minihack/src/core/__pycache__/vtrace.cpython-37.pyc
deleted file mode 100644
index 3cae271..0000000
Binary files a/minihack/src/core/__pycache__/vtrace.cpython-37.pyc and /dev/null differ
diff --git a/minihack/src/core/__pycache__/vtrace.cpython-38.pyc b/minihack/src/core/__pycache__/vtrace.cpython-38.pyc
deleted file mode 100644
index 4575018..0000000
Binary files a/minihack/src/core/__pycache__/vtrace.cpython-38.pyc and /dev/null differ
diff --git a/minihack/src/core/environment.py b/minihack/src/core/environment.py
deleted file mode 100644
index 0691cdf..0000000
--- a/minihack/src/core/environment.py
+++ /dev/null
@@ -1,69 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import torch
-
-
-def _format_frame(frame):
-    frame = torch.from_numpy(frame)
-    return frame.view((1, 1) + frame.shape)  # (...) -> (T,B,...).
-
-
-class Environment:
-
-    def __init__(self, gym_env, fix_seed=False):
-        self.gym_env = gym_env
-        self.episode_return = None
-        self.episode_step = None
-        self.fix_seed = fix_seed
-
-    def initial(self):
-        initial_reward = torch.zeros(1, 1)
-        # This supports only single-tensor actions ATM.
-        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)
-        self.episode_return = torch.zeros(1, 1)
-        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)
-        initial_done = torch.zeros(1, 1, dtype=torch.uint8)
-        if self.fix_seed:
-            self.gym_env.seed(seed=1)
-        initial_frame = _format_frame(self.gym_env.reset())
-        return dict(
-            frame=initial_frame,
-            reward=initial_reward,
-            done=initial_done,
-            episode_return=self.episode_return,
-            episode_step=self.episode_step,
-            last_action=initial_last_action)
-
-
-    def step(self, action):
-        frame, reward, done, unused_info = self.gym_env.step(action.item())
-        self.episode_step += 1
-        self.episode_return += reward
-        episode_step = self.episode_step
-        episode_return = self.episode_return
-        if done:
-            if self.fix_seed:
-                self.gym_env.seed(seed=1)
-            frame = self.gym_env.reset()
-            self.episode_return = torch.zeros(1, 1)
-            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)
-
-        frame = _format_frame(frame)
-        reward = torch.tensor(reward).view(1, 1)
-        done = torch.tensor(done).view(1, 1)
-
-        return dict(
-            frame=frame,
-            reward=reward,
-            done=done,
-            episode_return=episode_return,
-            episode_step=episode_step,
-            last_action=action)
-
-
-    def close(self):
-        self.gym_env.close()
diff --git a/minihack/src/core/file_writer.py b/minihack/src/core/file_writer.py
deleted file mode 100644
index 919e88e..0000000
--- a/minihack/src/core/file_writer.py
+++ /dev/null
@@ -1,179 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import copy
-import datetime
-import csv
-import json
-import logging
-import os
-import time
-from typing import Dict
-
-import git
-
-
-def gather_metadata() -> Dict:
-    date_start = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
-    # gathering git metadata
-    try:
-        repo = git.Repo(search_parent_directories=True)
-        git_sha = repo.commit().hexsha
-        git_data = dict(
-            commit=git_sha,
-            branch=repo.active_branch.name,
-            is_dirty=repo.is_dirty(),
-            path=repo.git_dir,
-        )
-    except git.InvalidGitRepositoryError:
-        git_data = None
-    # gathering slurm metadata
-    if 'SLURM_JOB_ID' in os.environ:
-        slurm_env_keys = [k for k in os.environ if k.startswith('SLURM')]
-        slurm_data = {}
-        for k in slurm_env_keys:
-            d_key = k.replace('SLURM_', '').replace('SLURMD_', '').lower()
-            slurm_data[d_key] = os.environ[k]
-    else:
-        slurm_data = None
-    return dict(
-        date_start=date_start,
-        date_end=None,
-        successful=False,
-        git=git_data,
-        slurm=slurm_data,
-        env=os.environ.copy(),
-    )
-
-
-class FileWriter:
-    def __init__(self,
-                 xpid: str = None,
-                 xp_args: dict = None,
-                 rootdir: str = '~/palaas'):
-        if not xpid:
-            # make unique id
-            xpid = '{proc}_{unixtime}'.format(
-                proc=os.getpid(), unixtime=int(time.time()))
-        self.xpid = xpid
-        self._tick = 0
-
-        # metadata gathering
-        if xp_args is None:
-            xp_args = {}
-        self.metadata = gather_metadata()
-        # we need to copy the args, otherwise when we close the file writer
-        # (and rewrite the args) we might have non-serializable objects (or
-        # other nasty stuff).
-        self.metadata['args'] = copy.deepcopy(xp_args)
-        self.metadata['xpid'] = self.xpid
-
-        formatter = logging.Formatter('%(message)s')
-        self._logger = logging.getLogger('palaas/out')
-
-        # to stdout handler
-        shandle = logging.StreamHandler()
-        shandle.setFormatter(formatter)
-        self._logger.addHandler(shandle)
-        self._logger.setLevel(logging.INFO)
-
-        rootdir = os.path.expandvars(os.path.expanduser(rootdir))
-        # to file handler
-        self.basepath = os.path.join(rootdir, self.xpid)
-
-        if not os.path.exists(self.basepath):
-            self._logger.info('Creating log directory: %s', self.basepath)
-            os.makedirs(self.basepath, exist_ok=True)
-        else:
-            self._logger.info('Found log directory: %s', self.basepath)
-
-        # NOTE: remove latest because it creates errors when running on slurm 
-        # multiple jobs trying to write to latest but cannot find it 
-        # Add 'latest' as symlink unless it exists and is no symlink.
-        # symlink = os.path.join(rootdir, 'latest')
-        # if os.path.islink(symlink):
-        #     os.remove(symlink)
-        # if not os.path.exists(symlink):
-        #     os.symlink(self.basepath, symlink)
-        #     self._logger.info('Symlinked log directory: %s', symlink)
-
-        self.paths = dict(
-            msg='{base}/out.log'.format(base=self.basepath),
-            logs='{base}/logs.csv'.format(base=self.basepath),
-            fields='{base}/fields.csv'.format(base=self.basepath),
-            meta='{base}/meta.json'.format(base=self.basepath),
-        )
-
-        self._logger.info('Saving arguments to %s', self.paths['meta'])
-        if os.path.exists(self.paths['meta']):
-            self._logger.warning('Path to meta file already exists. '
-                                 'Not overriding meta.')
-        else:
-            self._save_metadata()
-
-        self._logger.info('Saving messages to %s', self.paths['msg'])
-        if os.path.exists(self.paths['msg']):
-            self._logger.warning('Path to message file already exists. '
-                                 'New data will be appended.')
-
-        fhandle = logging.FileHandler(self.paths['msg'])
-        fhandle.setFormatter(formatter)
-        self._logger.addHandler(fhandle)
-
-        self._logger.info('Saving logs data to %s', self.paths['logs'])
-        self._logger.info('Saving logs\' fields to %s', self.paths['fields'])
-        if os.path.exists(self.paths['logs']):
-            self._logger.warning('Path to log file already exists. '
-                                 'New data will be appended.')
-            with open(self.paths['fields'], 'r') as csvfile:
-                reader = csv.reader(csvfile)
-                self.fieldnames = list(reader)[0]
-        else:
-            self.fieldnames = ['_tick', '_time', '_datetime']
-
-    def log(self, to_log: Dict, tick: int = None,
-            verbose: bool = False) -> None:
-        if tick is not None:
-            raise NotImplementedError
-        else:
-            to_log['_tick'] = self._tick
-            self._tick += 1
-        to_log['_time'] = time.time()
-        to_log['_datetime'] = str(datetime.datetime.now())
-
-        old_len = len(self.fieldnames)
-        for k in to_log:
-            if k not in self.fieldnames:
-                self.fieldnames.append(k)
-        if old_len != len(self.fieldnames):
-            with open(self.paths['fields'], 'w') as csvfile:
-                writer = csv.writer(csvfile)
-                writer.writerow(self.fieldnames)
-            self._logger.info('Updated log fields: %s', self.fieldnames)
-
-        if to_log['_tick'] == 0:
-            # print("\ncreating logs file ")
-            with open(self.paths['logs'], 'a') as f:
-                f.write('# %s\n' % ','.join(self.fieldnames))
-
-        if verbose:
-            self._logger.info('LOG | %s', ', '.join(
-                ['{}: {}'.format(k, to_log[k]) for k in sorted(to_log)]))
-
-        with open(self.paths['logs'], 'a') as f:
-            writer = csv.DictWriter(f, fieldnames=self.fieldnames)
-            writer.writerow(to_log)
-            # print("\nadded to log file")
-
-    def close(self, successful: bool = True) -> None:
-        self.metadata['date_end'] = datetime.datetime.now().strftime(
-            '%Y-%m-%d %H:%M:%S.%f')
-        self.metadata['successful'] = successful
-        self._save_metadata()
-
-    def _save_metadata(self) -> None:
-        with open(self.paths['meta'], 'w') as jsonfile:
-            json.dump(self.metadata, jsonfile, indent=4, sort_keys=True)
diff --git a/minihack/src/core/prof.py b/minihack/src/core/prof.py
deleted file mode 100644
index 10ce89e..0000000
--- a/minihack/src/core/prof.py
+++ /dev/null
@@ -1,69 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-"""Naive profiling using timeit."""
-
-import collections
-import timeit
-
-
-class Timings:
-    """Not thread-safe."""
-
-    def __init__(self):
-        self._means = collections.defaultdict(int)
-        self._vars = collections.defaultdict(int)
-        self._counts = collections.defaultdict(int)
-        self.reset()
-
-    def reset(self):
-        self.last_time = timeit.default_timer()
-
-    def time(self, name):
-        """Save an update for event `name`.
-
-        Nerd alarm: We could just store a
-            collections.defaultdict(list)
-        and compute means and standard deviations at the end. But thanks to the
-        clever math in Sutton-Barto
-        (http://www.incompleteideas.net/book/first/ebook/node19.html) and
-        https://math.stackexchange.com/a/103025/5051 we can update both the
-        means and the stds online. O(1) FTW!
-        """
-        now = timeit.default_timer()
-        x = now - self.last_time
-        self.last_time = now
-
-        n = self._counts[name]
-
-        mean = self._means[name] + (x - self._means[name]) / (n + 1)
-        var = (n * self._vars[name] + n * (self._means[name] - mean)**2 +
-               (x - mean)**2) / (n + 1)
-
-        self._means[name] = mean
-        self._vars[name] = var
-        self._counts[name] += 1
-
-    def means(self):
-        return self._means
-
-    def vars(self):
-        return self._vars
-
-    def stds(self):
-        return {k: v**0.5 for k, v in self._vars.items()}
-
-    def summary(self, prefix=''):
-        means = self.means()
-        stds = self.stds()
-        total = sum(means.values())
-
-        result = prefix
-        for k in sorted(means, key=means.get, reverse=True):
-            result += f'\n    %s: %.6fms +- %.6fms (%.2f%%) ' % (
-                k, 1000 * means[k], 1000 * stds[k], 100 * means[k] / total)
-        result += '\nTotal: %.6fms' % (1000 * total)
-        return result
diff --git a/minihack/src/core/vtrace.py b/minihack/src/core/vtrace.py
deleted file mode 100644
index a587d8a..0000000
--- a/minihack/src/core/vtrace.py
+++ /dev/null
@@ -1,127 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# This file taken from
-#     https://github.com/deepmind/scalable_agent/blob/
-#         cd66d00914d56c8ba2f0615d9cdeefcb169a8d70/vtrace.py
-# and modified.
-
-# Copyright 2018 Google LLC
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Functions to compute V-trace off-policy actor critic targets.
-
-For details and theory see:
-
-"IMPALA: Scalable Distributed Deep-RL with
-Importance Weighted Actor-Learner Architectures"
-by Espeholt, Soyer, Munos et al.
-
-See https://arxiv.org/abs/1802.01561 for the full paper.
-"""
-
-import collections
-
-import torch
-import torch.nn.functional as F
-
-VTraceFromLogitsReturns = collections.namedtuple('VTraceFromLogitsReturns', [
-    'vs', 'pg_advantages', 'log_rhos', 'behavior_action_log_probs',
-    'target_action_log_probs'
-])
-
-VTraceReturns = collections.namedtuple('VTraceReturns', 'vs pg_advantages')
-
-
-def action_log_probs(policy_logits, actions):
-    return -F.nll_loss(
-        F.log_softmax(torch.flatten(policy_logits, 0, 1), dim=-1),
-        torch.flatten(actions, 0, 1),
-        reduction='none').view_as(actions)
-
-
-def from_logits(behavior_policy_logits,
-                target_policy_logits,
-                actions,
-                discounts,
-                rewards,
-                values,
-                bootstrap_value,
-                clip_rho_threshold=1.0,
-                clip_pg_rho_threshold=1.0):
-    """V-trace for softmax policies."""
-
-    target_action_log_probs = action_log_probs(target_policy_logits, actions)
-    behavior_action_log_probs = action_log_probs(behavior_policy_logits,
-                                                 actions)
-    log_rhos = target_action_log_probs - behavior_action_log_probs
-    vtrace_returns = from_importance_weights(
-        log_rhos=log_rhos,
-        discounts=discounts,
-        rewards=rewards,
-        values=values,
-        bootstrap_value=bootstrap_value,
-        clip_rho_threshold=clip_rho_threshold,
-        clip_pg_rho_threshold=clip_pg_rho_threshold)
-    return VTraceFromLogitsReturns(
-        log_rhos=log_rhos,
-        behavior_action_log_probs=behavior_action_log_probs,
-        target_action_log_probs=target_action_log_probs,
-        **vtrace_returns._asdict())
-
-
-@torch.no_grad()
-def from_importance_weights(log_rhos,
-                            discounts,
-                            rewards,
-                            values,
-                            bootstrap_value,
-                            clip_rho_threshold=1.0,
-                            clip_pg_rho_threshold=1.0):
-    """V-trace from log importance weights."""
-    with torch.no_grad():
-        rhos = torch.exp(log_rhos)
-        if clip_rho_threshold is not None:
-            clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)
-        else:
-            clipped_rhos = rhos
-
-        cs = torch.clamp(rhos, max=1.0)
-        # Append bootstrapped value to get [v1, ..., v_t+1]
-        values_t_plus_1 = torch.cat(
-            [values[1:], torch.unsqueeze(bootstrap_value, 0)], dim=0)
-        deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)
-
-        acc = torch.zeros_like(bootstrap_value)
-        result = []
-
-        for t in range(discounts.shape[0] - 1, -1, -1):
-            acc = deltas[t] + discounts[t] * cs[t] * acc
-            result.append(acc)
-        result.reverse()
-        vs_minus_v_xs = torch.stack(result)
-        
-        # Add V(x_s) to get v_s.
-        vs = torch.add(vs_minus_v_xs, values)
-
-        # Advantage for policy gradient.
-        vs_t_plus_1 = torch.cat(
-            [vs[1:], torch.unsqueeze(bootstrap_value, 0)], dim=0)
-        if clip_pg_rho_threshold is not None:
-            clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)
-        else:
-            clipped_pg_rhos = rhos
-        pg_advantages = (clipped_pg_rhos *
-                         (rewards + discounts * vs_t_plus_1 - values))
-
-        # Make sure no gradients backpropagated through the returned values.
-        return VTraceReturns(vs=vs, pg_advantages=pg_advantages)
diff --git a/minihack/src/core/vtrace_test.py b/minihack/src/core/vtrace_test.py
deleted file mode 100644
index 07ed2a5..0000000
--- a/minihack/src/core/vtrace_test.py
+++ /dev/null
@@ -1,264 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# This file taken from
-#     https://github.com/deepmind/scalable_agent/blob/
-#         d24bd74bd53d454b7222b7f0bea57a358e4ca33e/vtrace_test.py
-# and modified.
-
-# Copyright 2018 Google LLC
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for V-trace.
-
-For details and theory see:
-
-"IMPALA: Scalable Distributed Deep-RL with
-Importance Weighted Actor-Learner Architectures"
-by Espeholt, Soyer, Munos et al.
-"""
-
-import unittest
-
-import numpy as np
-import torch
-
-import vtrace
-
-
-def _shaped_arange(*shape):
-    """Runs np.arange, converts to float and reshapes."""
-    return np.arange(np.prod(shape), dtype=np.float32).reshape(*shape)
-
-
-def _softmax(logits):
-    """Applies softmax non-linearity on inputs."""
-    return np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)
-
-
-def _ground_truth_calculation(discounts, log_rhos, rewards, values,
-                              bootstrap_value, clip_rho_threshold,
-                              clip_pg_rho_threshold):
-    """Calculates the ground truth for V-trace in Python/Numpy."""
-    vs = []
-    seq_len = len(discounts)
-    rhos = np.exp(log_rhos)
-    cs = np.minimum(rhos, 1.0)
-    clipped_rhos = rhos
-    if clip_rho_threshold:
-        clipped_rhos = np.minimum(rhos, clip_rho_threshold)
-    clipped_pg_rhos = rhos
-    if clip_pg_rho_threshold:
-        clipped_pg_rhos = np.minimum(rhos, clip_pg_rho_threshold)
-
-    # This is a very inefficient way to calculate the V-trace ground truth.
-    # We calculate it this way because it is close to the mathematical notation
-    # of V-trace.
-    # v_s = V(x_s)
-    #             + \sum^{T-1}_{t=s} \gamma^{t-s}
-    #                 * \prod_{i=s}^{t-1} c_i
-    #                 * \rho_t (r_t + \gamma V(x_{t+1}) - V(x_t))
-    # Note that when we take the product over c_i, we write `s:t` as the
-    # notation of the paper is inclusive of the `t-1`, but Python is exclusive.
-    # Also note that np.prod([]) == 1.
-    values_t_plus_1 = np.concatenate([values, bootstrap_value[None, :]], axis=0)
-    for s in range(seq_len):
-        v_s = np.copy(values[s])  # Very important copy.
-        for t in range(s, seq_len):
-            v_s += (
-                np.prod(discounts[s:t], axis=0) * np.prod(
-                    cs[s:t], axis=0) * clipped_rhos[t] *
-                (rewards[t] + discounts[t] * values_t_plus_1[t + 1] - values[t]
-                ))
-        vs.append(v_s)
-    vs = np.stack(vs, axis=0)
-    pg_advantages = (clipped_pg_rhos * (rewards + discounts * np.concatenate(
-        [vs[1:], bootstrap_value[None, :]], axis=0) - values))
-
-    return vtrace.VTraceReturns(vs=vs, pg_advantages=pg_advantages)
-
-
-def assert_allclose(actual, desired):
-    return np.testing.assert_allclose(actual, desired, rtol=1e-06, atol=1e-05)
-
-
-class ActionLogProbsTest(unittest.TestCase):
-
-    def test_action_log_probs(self, batch_size=2):
-        seq_len = 7
-        num_actions = 3
-
-        policy_logits = _shaped_arange(seq_len, batch_size, num_actions) + 10
-        actions = np.random.randint(
-            0, num_actions, size=(seq_len, batch_size), dtype=np.int64)
-
-        action_log_probs_tensor = vtrace.action_log_probs(
-            torch.from_numpy(policy_logits), torch.from_numpy(actions))
-
-        # Ground Truth
-        # Using broadcasting to create a mask that indexes action logits
-        action_index_mask = actions[..., None] == np.arange(num_actions)
-
-        def index_with_mask(array, mask):
-            return array[mask].reshape(*array.shape[:-1])
-
-        # Note: Normally log(softmax) is not a good idea because it's not
-        # numerically stable. However, in this test we have well-behaved values.
-        ground_truth_v = index_with_mask(
-            np.log(_softmax(policy_logits)), action_index_mask)
-
-        assert_allclose(ground_truth_v, action_log_probs_tensor)
-
-    def test_action_log_probs_batch_1(self):
-        self.test_action_log_probs(1)
-
-
-class VtraceTest(unittest.TestCase):
-
-    def test_vtrace(self, batch_size=5):
-        """Tests V-trace against ground truth data calculated in python."""
-        seq_len = 5
-
-        # Create log_rhos such that rho will span from near-zero to above the
-        # clipping thresholds. In particular, calculate log_rhos in [-2.5, 2.5),
-        # so that rho is in approx [0.08, 12.2).
-        log_rhos = _shaped_arange(seq_len, batch_size) / (batch_size * seq_len)
-        log_rhos = 5 * (log_rhos - 0.5)  # [0.0, 1.0) -> [-2.5, 2.5).
-        values = {
-            'log_rhos':
-            log_rhos,
-            # T, B where B_i: [0.9 / (i+1)] * T
-            'discounts':
-            np.array(
-                [[0.9 / (b + 1)
-                  for b in range(batch_size)]
-                 for _ in range(seq_len)],
-                dtype=np.float32),
-            'rewards':
-            _shaped_arange(seq_len, batch_size),
-            'values':
-            _shaped_arange(seq_len, batch_size) / batch_size,
-            'bootstrap_value':
-            _shaped_arange(batch_size) + 1.0,
-            'clip_rho_threshold':
-            3.7,
-            'clip_pg_rho_threshold':
-            2.2,
-        }
-
-        ground_truth = _ground_truth_calculation(**values)
-
-        values = {key: torch.tensor(value) for key, value in values.items()}
-        output = vtrace.from_importance_weights(**values)
-
-        for a, b in zip(ground_truth, output):
-            assert_allclose(a, b)
-
-    def test_vtrace_batch_1(self):
-        self.test_vtrace(1)
-
-    def test_vtrace_from_logits(self, batch_size=2):
-        """Tests V-trace calculated from logits."""
-        seq_len = 5
-        num_actions = 3
-        clip_rho_threshold = None  # No clipping.
-        clip_pg_rho_threshold = None  # No clipping.
-
-        values = {
-            'behavior_policy_logits':
-            _shaped_arange(seq_len, batch_size, num_actions),
-            'target_policy_logits':
-            _shaped_arange(seq_len, batch_size, num_actions),
-            'actions':
-            np.random.randint(0, num_actions - 1, size=(seq_len, batch_size)),
-            'discounts':
-            np.array(  # T, B where B_i: [0.9 / (i+1)] * T
-                [[0.9 / (b + 1)
-                  for b in range(batch_size)]
-                 for _ in range(seq_len)],
-                dtype=np.float32),
-            'rewards':
-            _shaped_arange(seq_len, batch_size),
-            'values':
-            _shaped_arange(seq_len, batch_size) / batch_size,
-            'bootstrap_value':
-            _shaped_arange(batch_size) + 1.0,  # B
-        }
-        values = {k: torch.from_numpy(v) for k, v in values.items()}
-
-        from_logits_output = vtrace.from_logits(
-            clip_rho_threshold=clip_rho_threshold,
-            clip_pg_rho_threshold=clip_pg_rho_threshold,
-            **values)
-
-        target_log_probs = vtrace.action_log_probs(
-            values['target_policy_logits'], values['actions'])
-        behavior_log_probs = vtrace.action_log_probs(
-            values['behavior_policy_logits'], values['actions'])
-        log_rhos = target_log_probs - behavior_log_probs
-
-        # Calculate V-trace using the ground truth logits.
-        from_iw = vtrace.from_importance_weights(
-            log_rhos=log_rhos,
-            discounts=values['discounts'],
-            rewards=values['rewards'],
-            values=values['values'],
-            bootstrap_value=values['bootstrap_value'],
-            clip_rho_threshold=clip_rho_threshold,
-            clip_pg_rho_threshold=clip_pg_rho_threshold)
-
-        assert_allclose(from_iw.vs, from_logits_output.vs)
-        assert_allclose(from_iw.pg_advantages, from_logits_output.pg_advantages)
-        assert_allclose(behavior_log_probs,
-                        from_logits_output.behavior_action_log_probs)
-        assert_allclose(target_log_probs,
-                        from_logits_output.target_action_log_probs)
-        assert_allclose(log_rhos, from_logits_output.log_rhos)
-
-    def test_vtrace_from_logits_batch_1(self):
-        self.test_vtrace_from_logits(1)
-
-    def test_higher_rank_inputs_for_importance_weights(self):
-        """Checks support for additional dimensions in inputs."""
-        T = 3  # pylint: disable=invalid-name
-        B = 2  # pylint: disable=invalid-name
-        values = {
-            'log_rhos': torch.zeros(T, B, 1),
-            'discounts': torch.zeros(T, B, 1),
-            'rewards': torch.zeros(T, B, 42),
-            'values': torch.zeros(T, B, 42),
-            'bootstrap_value': torch.zeros(B, 42),
-        }
-        output = vtrace.from_importance_weights(**values)
-        self.assertSequenceEqual(output.vs.shape, (T, B, 42))
-
-    def test_inconsistent_rank_inputs_for_importance_weights(self):
-        """Test one of many possible errors in shape of inputs."""
-        T = 3  # pylint: disable=invalid-name
-        B = 2  # pylint: disable=invalid-name
-
-        values = {
-            'log_rhos': torch.zeros(T, B, 1),
-            'discounts': torch.zeros(T, B, 1),
-            'rewards': torch.zeros(T, B, 42),
-            'values': torch.zeros(T, B, 42),
-            # Should be [B, 42].
-            'bootstrap_value': torch.zeros(B),
-        }
-
-        with self.assertRaisesRegex(RuntimeError,
-                                    'same number of dimensions: got 3 and 2'):
-            vtrace.from_importance_weights(**values)
-
-
-if __name__ == '__main__':
-    unittest.main()
diff --git a/minihack/src/env_utils.py b/minihack/src/env_utils.py
deleted file mode 100644
index 67277aa..0000000
--- a/minihack/src/env_utils.py
+++ /dev/null
@@ -1,202 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import gym 
-import torch 
-from collections import deque, defaultdict
-from gym import spaces
-import numpy as np
-import random
-#from gym_minigrid.minigrid import OBJECT_TO_IDX, COLOR_TO_IDX
-import pdb
-
-
-def _format_observation_vizdoom(obs):
-    obs = torch.tensor(obs)
-    obs = obs.view((1, 1) + obs.shape)
-    return obs
-
-
-def _format_observations_nethack(observation, keys=("glyphs", "blstats", "message")):
-    observations = {}
-    if 'state_visits' in observation.keys():
-        keys += ('state_visits',)
-
-    for key in keys:
-        entry = observation[key]
-        entry = torch.from_numpy(entry)
-        entry = entry.view((1, 1) + entry.shape)  # (...) -> (T,B,...).
-        observations[key] = entry
-    return observations
-
-
-
-class Environment:
-    def __init__(self, gym_env, fix_seed=False, env_seed=1):
-        self.gym_env = gym_env
-        self.episode_return = None
-        self.episode_step = None
-        self.episode_win = None
-        self.fix_seed = fix_seed
-        self.env_seed = env_seed
-
-    def get_partial_obs(self):
-        return self.gym_env.env.env.gen_obs()['image']
-
-    def initial(self):
-        initial_reward = torch.zeros(1, 1)
-        self.episode_return = torch.zeros(1, 1)
-        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)
-        self.episode_win = torch.zeros(1, 1, dtype=torch.int32)
-        initial_done = torch.ones(1, 1, dtype=torch.uint8)
-        if self.fix_seed:
-            seed = random.choice(self.env_seed)
-            self.gym_env.seed(seed)
-        obs = self.gym_env.reset()
-        if type(obs) is dict:
-            initial_frame = _format_observations_nethack(obs)
-            partial_obs = None
-            carried_col, carried_obj = torch.LongTensor([[5]]), torch.LongTensor([[1]])
-            initial_frame.update(
-                reward=initial_reward,
-                done=initial_done,
-                episode_return=self.episode_return,
-                episode_step=self.episode_step,
-            )
-            return initial_frame
-        else:
-            initial_frame = _format_observation_vizdoom(self.gym_env.reset())
-
-            return dict(
-                frame=initial_frame,
-                reward=initial_reward,
-                done=initial_done.bool(),
-                episode_return=self.episode_return,
-                episode_step=self.episode_step,
-            )
-        
-    def step(self, action):
-        if not isinstance(action, int):
-            action = action.item()
-            
-        frame, reward, done, _ = self.gym_env.step(action)
-
-        self.episode_step += 1
-        episode_step = self.episode_step
-
-        self.episode_return += reward
-        episode_return = self.episode_return 
-
-        if done and reward > 0:
-            self.episode_win[0][0] = 1 
-        else:
-            self.episode_win[0][0] = 0 
-        episode_win = self.episode_win 
-        
-        if done:
-            if self.fix_seed:
-                seed = random.choice(self.env_seed)
-                self.gym_env.seed(seed)
-            frame = self.gym_env.reset()
-            self.episode_return = torch.zeros(1, 1)
-            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)
-            self.episode_win = torch.zeros(1, 1, dtype=torch.int32)
-
-        if type(frame) is dict:
-            frame = _format_observations_nethack(frame)
-            reward = torch.tensor(reward).view(1, 1)
-            done = torch.tensor(done).view(1, 1)        
-            carried_col, carried_obj = torch.LongTensor([[5]]), torch.LongTensor([[1]])
-
-            
-            frame.update(
-                reward=reward,
-                done=done,
-                episode_return=episode_return,
-                episode_step = episode_step,
-            )
-            return frame
-                
-        else:
-            frame = _format_observation_vizdoom(frame)
-            reward = torch.tensor(reward).view(1, 1)
-            done = torch.tensor(done).view(1, 1).bool()
-            
-            return dict(
-                frame=frame,
-                reward=reward,
-                done=done,
-                episode_return=episode_return,
-                episode_step = episode_step,
-                episode_win = episode_win,
-            )
-
-            
-    def close(self):
-        self.gym_env.close()
-
-
-class FrameStack(gym.Wrapper):
-    def __init__(self, env, k):
-        """Stack k last frames.
-        Returns lazy array, which is much more memory efficient.
-        See Also
-        --------
-        baselines.common.atari_wrappers.LazyFrames
-        """
-        gym.Wrapper.__init__(self, env)
-        self.k = k
-        self.frames = deque([], maxlen=k)
-        shp = env.observation_space.shape
-        self.observation_space = spaces.Box(
-            low=0,
-            high=255,
-            shape=(shp[:-1] + (shp[-1] * k,)),
-            dtype=env.observation_space.dtype)
-
-    def reset(self):
-        ob = self.env.reset()
-        for _ in range(self.k):
-            self.frames.append(ob)
-        return self._get_ob()
-
-    def step(self, action):
-        ob, reward, done, info = self.env.step(action)
-        self.frames.append(ob)
-        return self._get_ob(), reward, done, info
-
-    def _get_ob(self):
-        assert len(self.frames) == self.k
-        return LazyFrames(list(self.frames))
-
-
-class LazyFrames(object):
-    def __init__(self, frames):
-        """This object ensures that common frames between the observations are only stored once.
-        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay
-        buffers.
-        This object should only be converted to numpy array before being passed to the model.
-        You'd not believe how complex the previous solution was."""
-        self._frames = frames
-        self._out = None
-
-    def _force(self):
-        if self._out is None:
-            self._out = np.concatenate(self._frames, axis=-1)
-            self._frames = None
-        return self._out
-
-    def __array__(self, dtype=None):
-        out = self._force()
-        if dtype is not None:
-            out = out.astype(dtype)
-        return out
-
-    def __len__(self):
-        return len(self._force())
-
-    def __getitem__(self, i):
-        return self._force()[i]
diff --git a/minihack/src/losses.py b/minihack/src/losses.py
deleted file mode 100644
index b0fdad1..0000000
--- a/minihack/src/losses.py
+++ /dev/null
@@ -1,48 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import torch
-from torch import nn 
-from torch.nn import functional as F
-import numpy as np 
-
-
-def compute_baseline_loss(advantages):
-    return 0.5 * torch.sum(torch.mean(advantages**2, dim=1))
-
-def compute_entropy_loss(logits):
-    policy = F.softmax(logits, dim=-1)
-    log_policy = F.log_softmax(logits, dim=-1)
-    entropy_per_timestep = torch.sum(-policy * log_policy, dim=-1)
-    return -torch.sum(torch.mean(entropy_per_timestep, dim=1))
-
-def compute_policy_gradient_loss(logits, actions, advantages):
-    cross_entropy = F.nll_loss(
-        F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),
-        target=torch.flatten(actions, 0, 1),
-        reduction='none')
-    cross_entropy = cross_entropy.view_as(advantages)
-    advantages.requires_grad = False
-    policy_gradient_loss_per_timestep = cross_entropy * advantages
-    return torch.sum(torch.mean(policy_gradient_loss_per_timestep, dim=1))
-
-
-def compute_forward_dynamics_loss(pred_next_emb, next_emb):
-    forward_dynamics_loss = torch.norm(pred_next_emb - next_emb, dim=2, p=2)
-    return torch.sum(torch.mean(forward_dynamics_loss, dim=1))
-
-
-def compute_inverse_dynamics_loss(pred_actions, true_actions):
-    inverse_dynamics_loss = F.nll_loss(
-        F.log_softmax(torch.flatten(pred_actions, 0, 1), dim=-1), 
-        target=torch.flatten(true_actions, 0, 1), 
-        reduction='none')
-    inverse_dynamics_loss = inverse_dynamics_loss.view_as(true_actions)
-    return torch.sum(torch.mean(inverse_dynamics_loss, dim=1))
-
-def compute_rnd_loss(pred_next_emb, next_emb):
-    forward_dynamics_loss = torch.norm(pred_next_emb - next_emb, dim=2, p=2)
-    return torch.mean(forward_dynamics_loss)
diff --git a/minihack/src/models.py b/minihack/src/models.py
deleted file mode 100644
index f4c8f70..0000000
--- a/minihack/src/models.py
+++ /dev/null
@@ -1,1092 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-
-
-# Copyright (c) Facebook, Inc. and its affiliates.
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import torch
-from torch import nn 
-from torch.nn import functional as F
-import numpy as np
-import pdb
-
-from nle import nethack
-
-NUM_CHARS = 256
-PAD_CHAR = 0
-
-def init(module, weight_init, bias_init, gain=1):
-    weight_init(module.weight.data, gain=gain)
-    bias_init(module.bias.data)
-    return module
-
-
-init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                        constant_(x, 0))
-
-init_relu_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                        constant_(x, 0), nn.init.calculate_gain('relu'))
-    
-def apply_init_(modules):
-    """
-    Initialize NN modules
-    """
-    for m in modules:
-        if isinstance(m, nn.Conv2d):
-            nn.init.xavier_uniform_(m.weight)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
-            nn.init.constant_(m.weight, 1)
-            if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-
-
-class MinigridMLPEmbeddingNet(nn.Module):
-    def __init__(self, hidden_dim=1024):
-        super(MinigridMLPEmbeddingNet, self).__init__()
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0), nn.init.calculate_gain('relu'))
-
-        self.fc = nn.Sequential(
-            nn.Linear(hidden_dim, hidden_dim),
-            nn.ReLU(),
-            nn.Linear(hidden_dim, hidden_dim),
-            nn.ReLU(),
-            nn.Linear(hidden_dim, 128),
-            nn.ReLU(),
-            nn.Linear(128, 128),
-            nn.ReLU(),
-            nn.Linear(128, 128),
-        )
-        
-    def forward(self, inputs, core_state=()):
-        x = inputs
-        T, B, *_ = x.shape
-
-        x = self.fc(x)
-
-        state_embedding = x.reshape(T, B, -1)
-
-        return state_embedding, tuple()
-
-
-class MinigridMLPTargetEmbeddingNet(nn.Module):
-    def __init__(self, hidden_dim=1024):
-        super(MinigridMLPTargetEmbeddingNet, self).__init__()
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0), nn.init.calculate_gain('relu'))
-
-        self.fc = nn.Sequential(
-            nn.Linear(hidden_dim, 128),
-            nn.ReLU(),
-            nn.Linear(128, 128),
-            nn.ReLU(),
-            nn.Linear(128, 128),
-            nn.ReLU(),
-            nn.Linear(128, 128),
-        )
-        
-    def forward(self, inputs, core_state=()):
-        x = inputs
-        T, B, *_ = x.shape
-
-        try:
-            x = self.fc(x)
-        except:
-            print(self.fc)
-            print(x.shape)
-
-        state_embedding = x.reshape(T, B, -1)
-
-        return state_embedding, tuple()
-
-
-class MinigridInverseDynamicsNet(nn.Module):
-    def __init__(self, num_actions, emb_size=128, p_dropout=0.0):
-        super(MinigridInverseDynamicsNet, self).__init__()
-        self.num_actions = num_actions 
-        
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0), nn.init.calculate_gain('relu'))
-        self.inverse_dynamics = nn.Sequential(
-            init_(nn.Linear(2 * emb_size, 256)), 
-            nn.ReLU(),
-            nn.Dropout(p=p_dropout)
-        )
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, 
-            lambda x: nn.init.constant_(x, 0))
-        self.id_out = init_(nn.Linear(256, self.num_actions))
-
-        
-    def forward(self, state_embedding, next_state_embedding):
-        inputs = torch.cat((state_embedding, next_state_embedding), dim=2)
-        action_logits = self.id_out(self.inverse_dynamics(inputs))
-        return action_logits
-    
-
-class MinigridForwardDynamicsNet(nn.Module):
-    def __init__(self, num_actions, hidden_dim=1024):
-        super(MinigridForwardDynamicsNet, self).__init__()
-        self.num_actions = num_actions 
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0), nn.init.calculate_gain('relu'))
-    
-        self.forward_dynamics = nn.Sequential(
-            init_(nn.Linear(hidden_dim + self.num_actions, hidden_dim)), 
-            nn.ReLU(), 
-        )
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, 
-            lambda x: nn.init.constant_(x, 0))
-
-        self.fd_out = init_(nn.Linear(hidden_dim, hidden_dim))
-
-    def forward(self, state_embedding, action):
-        action_one_hot = F.one_hot(action, num_classes=self.num_actions).float()
-        inputs = torch.cat((state_embedding, action_one_hot), dim=2)
-        next_state_emb = self.fd_out(self.forward_dynamics(inputs))
-        return next_state_emb
-
-
-def _step_to_range(delta, num_steps):
-    """Range of `num_steps` integers with distance `delta` centered around zero."""
-    return delta * torch.arange(-num_steps // 2, num_steps // 2)
-
-
-class Crop(nn.Module):
-    """Helper class for NetHackNet below."""
-
-    def __init__(self, height, width, height_target, width_target):
-        super(Crop, self).__init__()
-        self.width = width
-        self.height = height
-        self.width_target = width_target
-        self.height_target = height_target
-        width_grid = _step_to_range(2 / (self.width - 1), self.width_target)[
-            None, :
-        ].expand(self.height_target, -1)
-        height_grid = _step_to_range(2 / (self.height - 1), height_target)[
-            :, None
-        ].expand(-1, self.width_target)
-
-        # "clone" necessary, https://github.com/pytorch/pytorch/issues/34880
-        self.register_buffer("width_grid", width_grid.clone())
-        self.register_buffer("height_grid", height_grid.clone())
-
-    def forward(self, inputs, coordinates):
-        """Calculates centered crop around given x,y coordinates.
-        Args:
-           inputs [B x H x W]
-           coordinates [B x 2] x,y coordinates
-        Returns:
-           [B x H' x W'] inputs cropped and centered around x,y coordinates.
-        """
-        assert inputs.shape[1] == self.height
-        assert inputs.shape[2] == self.width
-
-        inputs = inputs[:, None, :, :].float()
-
-        x = coordinates[:, 0]
-        y = coordinates[:, 1]
-
-        x_shift = 2 / (self.width - 1) * (x.float() - self.width // 2)
-        y_shift = 2 / (self.height - 1) * (y.float() - self.height // 2)
-        
-        grid = torch.stack(
-            [
-                self.width_grid[None, :, :] + x_shift[:, None, None],
-                self.height_grid[None, :, :] + y_shift[:, None, None],
-            ],
-            dim=3,
-        )
-
-        # TODO: only cast to int if original tensor was int
-        return (
-            torch.round(F.grid_sample(inputs, grid, align_corners=True))
-            .squeeze(1)
-            .long()
-        )
-
-
-class Flatten(nn.Module):
-    def forward(self, input):
-        return input.view(input.size(0), -1)
-    
-class NetHackPolicyNet(nn.Module):
-    def __init__(
-        self,
-        observation_shape,
-        num_actions,
-        use_lstm,
-        sphere_norm=0,
-        hidden_dim=1024,
-        embedding_dim=64,
-        crop_dim=9,
-        num_layers=5,
-        msg_model="lt_cnn"
-    ):
-        super(NetHackPolicyNet, self).__init__()
-
-
-        self.register_buffer("reward_sum", torch.zeros(()))
-        self.register_buffer("reward_m2", torch.zeros(()))
-        self.register_buffer("reward_count", torch.zeros(()).fill_(1e-8))        
-
-        self.glyph_shape = observation_shape["glyphs"].shape
-        self.blstats_size = observation_shape["blstats"].shape[0]
-
-        self.num_actions = num_actions
-        self.use_lstm = use_lstm
-        self.sphere_norm = sphere_norm
-
-        self.H = self.glyph_shape[0]
-        self.W = self.glyph_shape[1]
-
-        self.k_dim = embedding_dim
-        self.h_dim = hidden_dim
-
-        self.crop_dim = crop_dim
-
-        self.crop = Crop(self.H, self.W, self.crop_dim, self.crop_dim)
-
-        self.embed = nn.Embedding(nethack.MAX_GLYPH, self.k_dim)
-
-        K = embedding_dim  # number of input filters
-        F = 3  # filter dimensions
-        S = 1  # stride
-        P = 1  # padding
-        M = 16  # number of intermediate filters
-        Y = 8  # number of output filters
-        L = num_layers  # number of convnet layers
-
-        in_channels = [K] + [M] * (L - 1)
-        out_channels = [M] * (L - 1) + [Y]
-
-        def interleave(xs, ys):
-            return [val for pair in zip(xs, ys) for val in pair]
-
-        conv_extract = [
-            nn.Conv2d(
-                in_channels=in_channels[i],
-                out_channels=out_channels[i],
-                kernel_size=(F, F),
-                stride=S,
-                padding=P,
-            )
-            for i in range(L)
-        ]
-
-        self.extract_representation = nn.Sequential(
-            *interleave(conv_extract, [nn.ELU()] * len(conv_extract))
-        )
-
-        # CNN crop model.
-        conv_extract_crop = [
-            nn.Conv2d(
-                in_channels=in_channels[i],
-                out_channels=out_channels[i],
-                kernel_size=(F, F),
-                stride=S,
-                padding=P,
-            )
-            for i in range(L)
-        ]
-
-        self.extract_crop_representation = nn.Sequential(
-            *interleave(conv_extract_crop, [nn.ELU()] * len(conv_extract))
-        )
-
-        out_dim = self.k_dim
-        # CNN over full glyph map
-        out_dim += self.H * self.W * Y
-
-        # CNN crop model.
-        out_dim += self.crop_dim ** 2 * Y
-
-        self.embed_blstats = nn.Sequential(
-            nn.Linear(self.blstats_size, self.k_dim),
-            nn.ReLU(),
-            nn.Linear(self.k_dim, self.k_dim),
-            nn.ReLU(),
-        )
-
-        self.msg_model = msg_model
-        if self.msg_model == 'lt_cnn':
-            self.msg_hdim = 64
-            self.msg_edim = 32
-            self.char_lt = nn.Embedding(
-                NUM_CHARS, self.msg_edim, padding_idx=PAD_CHAR
-            )            
-            self.conv1 = nn.Conv1d(
-                self.msg_edim, self.msg_hdim, kernel_size=7
-            )
-            # remaining convolutions, relus, pools, and a small FC network
-            self.conv2_6_fc = nn.Sequential(
-                nn.ReLU(),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # conv2
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=7),
-                nn.ReLU(),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # conv3
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                # conv4
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                # conv5
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                # conv6
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # fc receives -- [ B x h_dim x 5 ]
-                Flatten(),
-                nn.Linear(5 * self.msg_hdim, 2 * self.msg_hdim),
-                nn.ReLU(),
-                nn.Linear(2 * self.msg_hdim, self.msg_hdim),
-            )  # final output -- [ B x h_dim x 5 ]
-            out_dim += self.msg_hdim
-            
-        elif self.msg_model == 'lt_cnn_small':
-            self.msg_hdim = 64
-            self.msg_edim = 32
-            self.char_lt = nn.Embedding(
-                NUM_CHARS, self.msg_edim, padding_idx=PAD_CHAR
-            )            
-            self.conv1 = nn.Conv1d(
-                self.msg_edim, self.msg_hdim, kernel_size=7
-            )
-            # remaining convolutions, relus, pools, and a small FC network
-            self.conv2_6_fc = nn.Sequential(
-                nn.ReLU(),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # conv2
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=7),
-                nn.ReLU(),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # conv3
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # fc receives -- [ B x h_dim x 5 ]
-                Flatten(),
-                nn.Linear(5 * self.msg_hdim, 2 * self.msg_hdim),
-                nn.ReLU(),
-                nn.Linear(2 * self.msg_hdim, self.msg_hdim),
-            )  # final output -- [ B x h_dim x 5 ]
-            out_dim += self.msg_hdim 
-
-            
-        self.fc1 = nn.Sequential(
-            nn.Linear(out_dim, self.h_dim),
-            nn.ReLU()
-        )
-
-        self.fc2 = nn.Sequential(
-            nn.Linear(self.h_dim, self.h_dim),
-            nn.ReLU()
-        )
-        
-
-        if self.use_lstm:
-            self.core = nn.LSTM(self.h_dim, self.h_dim, num_layers=1)
-
-        self.policy = nn.Linear(self.h_dim, self.num_actions)
-        self.baseline = nn.Linear(self.h_dim, 1)
-
-
-
-    
-
-
-    @torch.no_grad()
-    def update_running_moments(self, reward_batch):
-        """Maintains a running mean of reward."""
-        new_count = len(reward_batch)
-        new_sum = torch.sum(reward_batch)
-        new_mean = new_sum / new_count
-
-        curr_mean = self.reward_sum / self.reward_count
-        new_m2 = torch.sum((reward_batch - new_mean) ** 2) + (
-            (self.reward_count * new_count)
-            / (self.reward_count + new_count)
-            * (new_mean - curr_mean) ** 2
-        )
-
-        self.reward_count += new_count
-        self.reward_sum += new_sum
-        self.reward_m2 += new_m2
-
-    @torch.no_grad()
-    def get_running_std(self):
-        """Returns standard deviation of the running mean of the reward."""
-        return torch.sqrt(self.reward_m2 / self.reward_count)        
-
-    def initial_state(self, batch_size=1):
-        if not self.use_lstm:
-            return tuple()
-        return tuple(
-            torch.zeros(self.core.num_layers, batch_size, self.core.hidden_size)
-            for _ in range(2)
-        )
-
-    def _select(self, embed, x):
-        # Work around slow backward pass of nn.Embedding, see
-        # https://github.com/pytorch/pytorch/issues/24912
-        out = embed.weight.index_select(0, x.reshape(-1))
-        return out.reshape(x.shape + (-1,))
-
-    def forward(self, env_outputs, core_state, decode=False):
-
-#        env_outputs = env_outputs['frame']
-        # -- [T x B x H x W]
-        glyphs = env_outputs["glyphs"]
-
-        # -- [T x B x F]
-        blstats = env_outputs["blstats"]
-
-        T, B, *_ = glyphs.shape
-
-        # -- [B' x H x W]
-        glyphs = torch.flatten(glyphs, 0, 1)  # Merge time and batch.
-
-        # -- [B' x F]
-        blstats = blstats.view(T * B, -1).float()
-
-        # -- [B x H x W]
-        glyphs = glyphs.long()
-        # -- [B x 2] x,y coordinates
-        coordinates = blstats[:, :2]
-        # TODO ???
-        # coordinates[:, 0].add_(-1)
-
-        # -- [B x F]
-        # FIXME: hack to use compatible blstats to before
-        # blstats = blstats[:, [0, 1, 21, 10, 11]]
-
-        blstats = blstats.view(T * B, -1).float()
-        # -- [B x K]
-        blstats_emb = self.embed_blstats(blstats)
-
-        assert blstats_emb.shape[0] == T * B
-
-        reps = [blstats_emb]
-
-        # -- [B x H' x W']
-        crop = self.crop(glyphs, coordinates)
-
-        # print("crop", crop)
-        # print("at_xy", glyphs[:, coordinates[:, 1].long(), coordinates[:, 0].long()])
-
-        # -- [B x H' x W' x K]
-        crop_emb = self._select(self.embed, crop)
-
-        # CNN crop model.
-        # -- [B x K x W' x H']
-        crop_emb = crop_emb.transpose(1, 3)  # -- TODO: slow?
-        # -- [B x W' x H' x K]
-        crop_rep = self.extract_crop_representation(crop_emb)
-
-        # -- [B x K']
-        crop_rep = crop_rep.view(T * B, -1)
-        assert crop_rep.shape[0] == T * B
-
-        reps.append(crop_rep)
-
-        # -- [B x H x W x K]
-        glyphs_emb = self._select(self.embed, glyphs)
-        # -- [B x K x W x H]
-        glyphs_emb = glyphs_emb.transpose(1, 3)  # -- TODO: slow?
-        # -- [B x W x H x K]
-        glyphs_rep = self.extract_representation(glyphs_emb)
-
-        # -- [B x K']
-        glyphs_rep = glyphs_rep.view(T * B, -1)
-        
-        assert glyphs_rep.shape[0] == T * B
-
-        # -- [B x K'']
-        reps.append(glyphs_rep)
-
-
-        # MESSAGING MODEL
-        if self.msg_model != "none":
-            # [T x B x 256] -> [T * B x 256]
-            messages = env_outputs["message"].long().view(T * B, -1)
-            if self.msg_model == "lt_cnn":
-                # [ T * B x E x 256 ]
-                char_emb = self.char_lt(messages).transpose(1, 2)
-                char_rep = self.conv2_6_fc(self.conv1(char_emb))
-            reps.append(char_rep)
-
-        
-
-        st = torch.cat(reps, dim=1)
-
-        # -- [B x K]
-        st1 = self.fc1(st)
-        st = self.fc2(st1)
-        if self.sphere_norm == 1:
-            st = F.normalize(st, p=2, dim=-1)
-
-
-        if self.use_lstm:
-            core_input = st.view(T, B, -1)
-            core_output_list = []
-            notdone = (~env_outputs["done"]).float()
-            for input, nd in zip(core_input.unbind(), notdone.unbind()):
-                # Reset core state to zero whenever an episode ended.
-                # Make `done` broadcastable with (num_layers, B, hidden_size)
-                # states:
-                nd = nd.view(1, -1, 1)
-                core_state = tuple(nd * s for s in core_state)
-                try:
-                    output, core_state = self.core(input.unsqueeze(0), core_state)
-                except:
-                    print('self.core')
-                    print(core_input)
-                    print(self.core)
-                core_output_list.append(output)
-            core_output = torch.flatten(torch.cat(core_output_list), 0, 1)
-        else:
-            core_output = st
-
-        # -- [B x A]
-        policy_logits = self.policy(core_output)
-        # -- [B x A]
-        baseline = self.baseline(core_output)
-
-        if self.training:
-            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)
-        else:
-            # Don't sample when testing.
-            action = torch.argmax(policy_logits, dim=1)
-
-        policy_logits = policy_logits.view(T, B, self.num_actions)
-        baseline = baseline.view(T, B)
-        action = action.view(T, B)
-
-        outputs = dict(policy_logits=policy_logits, baseline=baseline, action=action)
-        outputs['policy_hiddens'] = st.detach()
-        return (
-            outputs,
-            core_state,
-        )
-
-
-
-class NetHackStateEmbeddingNet(nn.Module):
-    def __init__(
-        self,
-        observation_shape,
-        use_lstm,
-        hidden_dim=1024,
-        embedding_dim=64,
-        crop_dim=9,
-        num_layers=5,
-        msg_model="lt_cnn",
-        p_dropout=0.0,
-    ):
-        super(NetHackStateEmbeddingNet, self).__init__()
-
-
-        self.register_buffer("reward_sum", torch.zeros(()))
-        self.register_buffer("reward_m2", torch.zeros(()))
-        self.register_buffer("reward_count", torch.zeros(()).fill_(1e-8))        
-
-        self.glyph_shape = observation_shape["glyphs"].shape
-        self.blstats_size = observation_shape["blstats"].shape[0]
-        self.p_dropout = p_dropout
-
-        self.use_lstm = use_lstm
-
-        self.H = self.glyph_shape[0]
-        self.W = self.glyph_shape[1]
-
-        self.k_dim = embedding_dim
-        self.h_dim = hidden_dim
-
-        self.crop_dim = crop_dim
-
-        self.crop = Crop(self.H, self.W, self.crop_dim, self.crop_dim)
-
-        self.embed = nn.Embedding(nethack.MAX_GLYPH, self.k_dim)
-
-        K = embedding_dim  # number of input filters
-        F = 3  # filter dimensions
-        S = 1  # stride
-        P = 1  # padding
-        M = 16  # number of intermediate filters
-        Y = 8  # number of output filters
-        L = num_layers  # number of convnet layers
-
-        in_channels = [K] + [M] * (L - 1)
-        out_channels = [M] * (L - 1) + [Y]
-
-        def interleave(xs, ys):
-            return [val for pair in zip(xs, ys) for val in pair]
-
-        conv_extract = [
-            nn.Conv2d(
-                in_channels=in_channels[i],
-                out_channels=out_channels[i],
-                kernel_size=(F, F),
-                stride=S,
-                padding=P,
-            )
-            for i in range(L)
-        ]
-
-        self.extract_representation = nn.Sequential(
-            *interleave(conv_extract, [nn.Sequential(nn.ELU(), nn.Dropout(self.p_dropout))] * len(conv_extract))
-        )
-
-        # CNN crop model.
-        conv_extract_crop = [
-            nn.Conv2d(
-                in_channels=in_channels[i],
-                out_channels=out_channels[i],
-                kernel_size=(F, F),
-                stride=S,
-                padding=P,
-            )
-            for i in range(L)
-        ]
-
-        self.extract_crop_representation = nn.Sequential(
-            *interleave(conv_extract_crop, [nn.Sequential(nn.ELU(), nn.Dropout(self.p_dropout))] * len(conv_extract))
-        )
-
-        self.feat_extract = self.extract_representation
-
-        out_dim = self.k_dim
-        # CNN over full glyph map
-        out_dim += self.H * self.W * Y
-
-        # CNN crop model.
-        out_dim += self.crop_dim ** 2 * Y
-
-        self.embed_blstats = nn.Sequential(
-            nn.Linear(self.blstats_size, self.k_dim),
-            nn.ReLU(),
-            nn.Dropout(self.p_dropout),
-            nn.Linear(self.k_dim, self.k_dim),
-            nn.ReLU(),
-            nn.Dropout(self.p_dropout),
-        )
-
-        self.msg_model = msg_model        
-        if msg_model == 'lt_cnn':
-            self.msg_hdim = 64
-            self.msg_edim = 32
-            self.char_lt = nn.Embedding(
-                NUM_CHARS, self.msg_edim, padding_idx=PAD_CHAR
-            )            
-            self.conv1 = nn.Conv1d(
-                self.msg_edim, self.msg_hdim, kernel_size=7
-            )
-            # remaining convolutions, relus, pools, and a small FC network
-            self.conv2_6_fc = nn.Sequential(
-                nn.ReLU(),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # conv2
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=7),
-                nn.ReLU(),
-                nn.Dropout2d(p=self.p_dropout),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # conv3
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                nn.Dropout2d(p=self.p_dropout),
-                # conv4
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                nn.Dropout2d(p=self.p_dropout),
-                # conv5
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                nn.Dropout2d(p=self.p_dropout),
-                # conv6
-                nn.Conv1d(self.msg_hdim, self.msg_hdim, kernel_size=3),
-                nn.ReLU(),
-                nn.Dropout2d(p=self.p_dropout),
-                nn.MaxPool1d(kernel_size=3, stride=3),
-                # fc receives -- [ B x h_dim x 5 ]
-                Flatten(),
-                nn.Linear(5 * self.msg_hdim, 2 * self.msg_hdim),
-                nn.ReLU(),
-                nn.Dropout(p=self.p_dropout),
-                nn.Linear(2 * self.msg_hdim, self.msg_hdim),
-            )  # final output -- [ B x h_dim x 5 ]
-            out_dim += self.msg_hdim
-            
-
-        
-
-        self.fc1 = nn.Sequential(
-            nn.Linear(out_dim, self.h_dim),
-            nn.ReLU(),
-            nn.Dropout(p=self.p_dropout),
-        )
-
-        self.fc2 = nn.Sequential(
-            nn.Linear(self.h_dim, self.h_dim),
-            nn.ReLU(),
-        )
-
-        if self.use_lstm:
-            self.core = nn.LSTM(self.h_dim, self.h_dim, num_layers=1)
-
-
-
-    @torch.no_grad()
-    def update_running_moments(self, reward_batch):
-        """Maintains a running mean of reward."""
-        new_count = len(reward_batch)
-        new_sum = torch.sum(reward_batch)
-        new_mean = new_sum / new_count
-
-        curr_mean = self.reward_sum / self.reward_count
-        new_m2 = torch.sum((reward_batch - new_mean) ** 2) + (
-            (self.reward_count * new_count)
-            / (self.reward_count + new_count)
-            * (new_mean - curr_mean) ** 2
-        )
-
-        self.reward_count += new_count
-        self.reward_sum += new_sum
-        self.reward_m2 += new_m2
-
-    @torch.no_grad()
-    def get_running_std(self):
-        """Returns standard deviation of the running mean of the reward."""
-        return torch.sqrt(self.reward_m2 / self.reward_count)        
-
-    def initial_state(self, batch_size=1):
-        if not self.use_lstm:
-            return tuple()
-        return tuple(
-            torch.zeros(self.core.num_layers, batch_size, self.core.hidden_size)
-            for _ in range(2)
-        )
-
-    def _select(self, embed, x):
-        # Work around slow backward pass of nn.Embedding, see
-        # https://github.com/pytorch/pytorch/issues/24912
-        out = embed.weight.index_select(0, x.reshape(-1))
-        return out.reshape(x.shape + (-1,))
-
-    def forward(self, env_outputs, core_state):
-        # -- [T x B x H x W]
-        glyphs = env_outputs["glyphs"]
-
-        # -- [T x B x F]
-        blstats = env_outputs["blstats"]
-            
-        T, B, *_ = glyphs.shape
-
-        # -- [B' x H x W]
-        glyphs = torch.flatten(glyphs, 0, 1)  # Merge time and batch.
-
-        # -- [B' x F]
-        blstats = blstats.view(T * B, -1).float()
-
-        # -- [B x H x W]
-        glyphs = glyphs.long()
-        # -- [B x 2] x,y coordinates
-        coordinates = blstats[:, :2]
-        # TODO ???
-        # coordinates[:, 0].add_(-1)
-
-        # -- [B x F]
-        # FIXME: hack to use compatible blstats to before
-        # blstats = blstats[:, [0, 1, 21, 10, 11]]
-
-        blstats = blstats.view(T * B, -1).float()
-        # -- [B x K]
-        blstats_emb = self.embed_blstats(blstats)
-
-        assert blstats_emb.shape[0] == T * B
-
-        reps = [blstats_emb]
-
-        # -- [B x H' x W']
-        crop = self.crop(glyphs, coordinates)
-
-        # -- [B x H' x W' x K]
-        crop_emb = self._select(self.embed, crop)
-
-        # CNN crop model.
-        # -- [B x K x W' x H']
-        crop_emb = crop_emb.transpose(1, 3)  # -- TODO: slow?
-        # -- [B x W' x H' x K]
-        crop_rep = self.extract_crop_representation(crop_emb)
-
-        # -- [B x K']
-        crop_rep = crop_rep.view(T * B, -1)
-        assert crop_rep.shape[0] == T * B
-
-        reps.append(crop_rep)
-
-        # -- [B x H x W x K]
-        glyphs_emb = self._select(self.embed, glyphs)
-        # -- [B x K x W x H]
-        glyphs_emb = glyphs_emb.transpose(1, 3)  # -- TODO: slow?
-        # -- [B x W x H x K]
-        glyphs_rep = self.extract_representation(glyphs_emb)
-
-        # -- [B x K']
-        glyphs_rep = glyphs_rep.view(T * B, -1)
-
-        assert glyphs_rep.shape[0] == T * B
-
-        # -- [B x K'']
-        reps.append(glyphs_rep)
-
-        # MESSAGING MODEL
-        if self.msg_model != "none":
-            # [T x B x 256] -> [T * B x 256]
-            messages = env_outputs["message"].long()                
-            messages = messages.view(T * B, -1)
-            if self.msg_model == "lt_cnn":
-                # [ T * B x E x 256 ]
-                char_emb = self.char_lt(messages).transpose(1, 2)
-                char_rep = self.conv2_6_fc(self.conv1(char_emb))
-            reps.append(char_rep)
-        
-
-        st = torch.cat(reps, dim=1)
-
-        # -- [B x K]
-        st1 = self.fc1(st)
-        st = self.fc2(st1)
-        
-
-        if self.use_lstm:
-            core_input = st.view(T, B, -1)
-            core_output_list = []
-            notdone = (~env_outputs["done"]).float()
-            for input, nd in zip(core_input.unbind(), notdone.unbind()):
-                # Reset core state to zero whenever an episode ended.
-                # Make `done` broadcastable with (num_layers, B, hidden_size)
-                # states:
-                nd = nd.view(1, -1, 1)
-                core_state = tuple(nd * s for s in core_state)
-                try:
-                    output, core_state = self.core(input.unsqueeze(0), core_state)
-                except:
-                    print('self.core')
-                    print(core_input)
-                    print(self.core)
-                core_output_list.append(output)
-            core_output = torch.flatten(torch.cat(core_output_list), 0, 1)
-        else:
-            core_output = st
-
-        core_output = core_output.view(T, B, -1)
-
-        return (
-            core_output,
-            core_state,
-        )
-
-
-
-
-class MarioDoomPolicyNet(nn.Module):
-    def __init__(self, observation_shape, num_actions, use_lstm=True, hidden_size=288):
-        super(MarioDoomPolicyNet, self).__init__()
-        self.observation_shape = observation_shape
-        self.num_actions = num_actions
-        self.hidden_size = hidden_size
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                                constant_(x, 0), nn.init.calculate_gain('relu'))
-
-        self.feat_extract = nn.Sequential(
-            init_(nn.Conv2d(in_channels=self.observation_shape[0], out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-            init_(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-            init_(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-            init_(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-        )
-        self.use_lstm = use_lstm
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                                constant_(x, 0))
-
-        if self.use_lstm:
-            self.core = nn.LSTM(self.hidden_size, 256, 2)
-        else:
-            self.core = nn.Linear(self.hidden_size, 256)
-
-        self.policy = init_(nn.Linear(256, self.num_actions))
-        self.baseline = init_(nn.Linear(256, 1))
-
-
-    def initial_state(self, batch_size):
-        if self.use_lstm:
-            return tuple(torch.zeros(self.core.num_layers, batch_size, 
-                                     self.core.hidden_size) for _ in range(2))
-        else:
-            return ()
-
-    def forward(self, inputs, core_state=()):
-        # -- [unroll_length x batch_size x height x width x channels]
-        x = inputs['frame']
-        T, B, C, W, H = x.shape
-        x = x.reshape(T, B, W, H, C)
-
-        # -- [unroll_length*batch_size x height x width x channels]
-        x = torch.flatten(x, 0, 1)  # Merge time and batch.
-        x = x.float() / 255.0
-        
-        # -- [unroll_length*batch_size x channels x width x height]
-        x = x.transpose(1, 3)
-        x = self.feat_extract(x)
-
-
-        core_input = x.view(T * B, -1)
-
-
-        if self.use_lstm:
-            core_input = core_input.view(T, B, -1)
-            core_output_list = []
-#            notdone = (~inputs['done'].type(torch.ByteTensor)).float()
-            notdone = (~inputs['done']).float()
-            if core_input.is_cuda:
-                notdone = notdone.cuda()
-            t = 0
-            for input, nd in zip(core_input.unbind(), notdone.unbind()):
-                nd = nd.view(1, -1, 1)
-                core_state = tuple(nd * s for s in core_state)
-                output, core_state = self.core(input.unsqueeze(0), core_state)
-                t += 1
-                core_output_list.append(output)
-            core_output = torch.flatten(torch.cat(core_output_list), 0, 1)
-        else:
-            core_output = self.core(core_input)
-
-        policy_logits = self.policy(core_output)
-        baseline = self.baseline(core_output)
-
-        if self.training:
-            action = torch.multinomial(
-                F.softmax(policy_logits, dim=1), num_samples=1)
-        else:
-            action = torch.argmax(policy_logits, dim=1)
-
-        policy_logits = policy_logits.view(T, B, self.num_actions)
-        baseline = baseline.view(T, B)
-        action = action.view(T, B)
-
-        return dict(policy_logits=policy_logits, baseline=baseline, 
-                    action=action), core_state
-
-
-class MarioDoomStateEmbeddingNet(nn.Module):
-    def __init__(self, observation_shape):
-        super(MarioDoomStateEmbeddingNet, self).__init__()
-        self.observation_shape = observation_shape
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0), nn.init.calculate_gain('relu'))
-
-        self.feat_extract = nn.Sequential(
-            init_(nn.Conv2d(in_channels=self.observation_shape[0], out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-            init_(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-            init_(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-            init_(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2, padding=1)),
-            nn.ELU(),
-        )
-
-    def initial_state(self, batch_size):
-        return ()
-        
-    
-    def forward(self, inputs, x=None, decode=False):
-        # -- [unroll_length x batch_size x height x width x channels]
-        x = inputs['frame']
-        T, B, C, W, H = x.shape
-        x = x.reshape(T, B, W, H, C)
-
-        # -- [unroll_length*batch_size x height x width x channels]
-        x = torch.flatten(x, 0, 1)  # Merge time and batch.
-        x = x.float() / 255.0
-
-        # -- [unroll_length*batch_size x channels x width x height]
-        x = x.transpose(1, 3)
-        x = self.feat_extract(x)
-
-        state_embedding = x.view(T, B, -1)
-        
-        return state_embedding, None
-
-
-class MarioDoomForwardDynamicsNet(nn.Module):
-    def __init__(self, num_actions, hidden_size=288):
-        super(MarioDoomForwardDynamicsNet, self).__init__()
-        self.num_actions = num_actions
-        self.hidden_size = hidden_size
-            
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0), nn.init.calculate_gain('relu'))
-    
-        self.forward_dynamics = nn.Sequential(
-            init_(nn.Linear(self.hidden_size + self.num_actions, 256)), 
-            nn.ReLU(), 
-        )
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0))
-
-        self.fd_out = init_(nn.Linear(256, self.hidden_size))
-
-    def forward(self, state_embedding, action):
-        action_one_hot = F.one_hot(action, num_classes=self.num_actions).float()
-        inputs = torch.cat((state_embedding, action_one_hot), dim=2)
-        next_state_emb = self.fd_out(self.forward_dynamics(inputs))
-        return next_state_emb
-
-
-class MarioDoomInverseDynamicsNet(nn.Module):
-    def __init__(self, num_actions, hidden_size=288):
-        super(MarioDoomInverseDynamicsNet, self).__init__()
-        self.num_actions = num_actions
-        self.hidden_size = hidden_size
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0), nn.init.calculate_gain('relu'))
-        self.inverse_dynamics = nn.Sequential(
-            init_(nn.Linear(2 * self.hidden_size, 256)), 
-            nn.ReLU(), 
-        )
-
-        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
-                            constant_(x, 0))
-
-        self.id_out = init_(nn.Linear(256, self.num_actions))
-
-        
-    def forward(self, state_embedding, next_state_embedding):
-        inputs = torch.cat((state_embedding, next_state_embedding), dim=2)
-        action_logits = self.id_out(self.inverse_dynamics(inputs))
-        return action_logits
-    
diff --git a/minihack/src/utils.py b/minihack/src/utils.py
deleted file mode 100644
index e0d52c2..0000000
--- a/minihack/src/utils.py
+++ /dev/null
@@ -1,519 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-
-# Copyright (c) Facebook, Inc. and its affiliates.
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-from __future__ import division
-import torch.nn as nn
-import torch 
-import typing
-import gym 
-import threading
-from torch import multiprocessing as mp
-import logging
-import traceback
-import os 
-import numpy as np
-import copy
-import nle
-import minihack
-import pdb
-import time
-import contextlib
-import termios
-import tty
-import gc
-
-import nle  # noqa: F401
-from nle import nethack
-
-
-
-from src.core import prof
-from src.env_utils import FrameStack, Environment
-from src import atari_wrappers as atari_wrappers
-
-#from gym_minigrid import wrappers as wrappers
-
-
-@contextlib.contextmanager
-def no_echo():
-    tt = termios.tcgetattr(0)
-    try:
-        tty.setraw(0)
-        yield
-    finally:
-        termios.tcsetattr(0, termios.TCSAFLUSH, tt)
-
-
-
-def repeat_batch(batch, n_repeats=10):
-    batch_rep = {}
-    batch_rep['glyphs'] = batch['glyphs'].repeat(n_repeats, 1, 1, 1)
-    batch_rep['blstats'] = batch['blstats'].repeat(n_repeats, 1, 1)
-    batch_rep['message'] = batch['message'].repeat(n_repeats, 1, 1)
-    return batch_rep
-    
-
-
-# This augmentation is based on random walk of agents
-def augmentation(frames):
-    # agent_loc = agent_loc(frames)
-    return frames
-
-# Entropy loss on categorical distribution
-def catentropy(logits):
-    a = logits - torch.max(logits, dim=-1, keepdim=True)[0]
-    e = torch.exp(a)
-    z = torch.sum(e, dim=-1, keepdim=True)
-    p = e / z
-    entropy = torch.sum(p * (torch.log(z) - a), dim=-1)
-    return torch.mean(entropy)
-
-# Here is computing how many objects
-def num_objects(frames):
-    T, B, H, W, *_ = frames.shape
-    num_objects = frames[:, :, :, :, 0]
-    num_objects = (num_objects == 4).long() + (num_objects == 5).long() + \
-        (num_objects == 6).long() + (num_objects == 7).long() + (num_objects == 8).long()
-    return num_objects
-
-# EMA of the 2 networks
-def soft_update_params(net, target_net, tau):
-    for param, target_param in zip(net.parameters(), target_net.parameters()):
-        target_param.data.copy_(
-            tau * param.data + (1 - tau) * target_param.data
-        )
-
-
-shandle = logging.StreamHandler()
-shandle.setFormatter(
-    logging.Formatter(
-        '[%(levelname)s:%(process)d %(module)s:%(lineno)d %(asctime)s] '
-        '%(message)s'))
-log = logging.getLogger('torchbeast')
-log.propagate = False
-log.addHandler(shandle)
-log.setLevel(logging.INFO)
-
-Buffers = typing.Dict[str, typing.List[torch.Tensor]]
-
-
-def create_env(flags):
-    if 'MiniHack' in flags.env:
-        seed = int.from_bytes(os.urandom(4), byteorder="little")
-        kwargs = {}
-        kwargs["observation_keys"]=("glyphs", "blstats", "chars", "message")
-        kwargs["savedir"] = None            
-        env = gym.make(flags.env, **kwargs)
-        env.seed(seed)
-        return env
-    
-    elif 'Mario' in flags.env:
-        env = atari_wrappers.wrap_pytorch(
-            atari_wrappers.wrap_deepmind(
-                atari_wrappers.make_atari(flags.env, noop=True),
-                clip_rewards=False,
-                frame_stack=True,
-                scale=False,
-                fire=True)) 
-        env = JoypadSpace(env, COMPLETE_MOVEMENT)
-        return env
-    else:
-        env = atari_wrappers.wrap_pytorch(
-            atari_wrappers.wrap_deepmind(
-                atari_wrappers.make_atari(flags.env, noop=False),
-                clip_rewards=False,
-                frame_stack=True,
-                scale=False,
-                fire=False)) 
-        return env
-
-
-def get_batch(free_queue: mp.Queue,
-              full_queue: mp.Queue,
-              buffers: Buffers,
-              initial_agent_state_buffers,
-              flags,
-              timings,
-              lock=threading.Lock()):
-    with lock:
-        timings.time('lock')
-        indices = [full_queue.get() for _ in range(flags.batch_size)]
-        timings.time('dequeue')
-    batch = {
-        key: torch.stack([buffers[key][m] for m in indices], dim=1)
-        for key in buffers
-    }
-    initial_agent_state = (
-        torch.cat(ts, dim=1)
-        for ts in zip(*[initial_agent_state_buffers[m] for m in indices])
-    )
-    timings.time('batch')
-    for m in indices:
-        free_queue.put(m)
-    timings.time('enqueue')
-    batch = {
-        k: t.to(device=flags.device, non_blocking=True)
-        for k, t in batch.items()
-    }
-    initial_agent_state = tuple(t.to(device=flags.device, non_blocking=True)
-                                for t in initial_agent_state)
-    timings.time('device')
-    return batch, initial_agent_state
-
-def create_heatmap_buffers(obs_shape):
-    specs = []
-    for r in range(obs_shape[0]):
-        for c in range(obs_shape[1]):
-            specs.append(tuple([r, c]))
-    buffers: Buffers = {key: torch.zeros(1).share_memory_() for key in specs}
-    return buffers
-
-def create_buffers(obs_space, num_actions, flags) -> Buffers:
-    T = flags.unroll_length
-
-
-    if type(obs_space) is gym.spaces.dict.Dict:
-        size = (flags.unroll_length + 1,)
-        # Get specimens to infer shapes and dtypes.
-        samples = {k: torch.from_numpy(v) for k, v in obs_space.sample().items()}
-        specs = {
-            key: dict(size=size + sample.shape, dtype=sample.dtype)
-            for key, sample in samples.items()
-        }
-        specs.update(
-            policy_hiddens=dict(size=(T + 1, flags.hidden_dim), dtype=torch.float32),
-            reward=dict(size=size, dtype=torch.float32),
-            bonus_reward=dict(size=size, dtype=torch.float32),
-            bonus_reward2=dict(size=size, dtype=torch.float32),
-            done=dict(size=size, dtype=torch.bool),
-            episode_return=dict(size=size, dtype=torch.float32),
-            episode_step=dict(size=size, dtype=torch.int32),
-            policy_logits=dict(size=size + (num_actions,), dtype=torch.float32),
-            episode_state_count=dict(size=(T + 1, ), dtype=torch.float32),
-            global_state_count=dict(size=(T + 1, ), dtype=torch.float32),
-            baseline=dict(size=size, dtype=torch.float32),
-            last_action=dict(size=size, dtype=torch.int64),
-            action=dict(size=size, dtype=torch.int64),
-            state_visits=dict(size=size, dtype=torch.int32),
-        )
-        
-    else:
-        obs_shape = obs_space.shape
-        specs = dict(
-            frame=dict(size=(T + 1, *obs_shape), dtype=torch.uint8),
-            reward=dict(size=(T + 1,), dtype=torch.float32),
-            bonus_reward=dict(size=(T + 1,), dtype=torch.float32),
-            bonus_reward2=dict(size=(T + 1,), dtype=torch.float32),
-            done=dict(size=(T + 1,), dtype=torch.bool),
-            episode_return=dict(size=(T + 1,), dtype=torch.float32),
-            episode_step=dict(size=(T + 1,), dtype=torch.int32),
-            last_action=dict(size=(T + 1,), dtype=torch.int64),
-            policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),
-            baseline=dict(size=(T + 1,), dtype=torch.float32),
-            action=dict(size=(T + 1,), dtype=torch.int64),
-            episode_win=dict(size=(T + 1,), dtype=torch.int32),
-            carried_obj=dict(size=(T + 1,), dtype=torch.int32),
-            carried_col=dict(size=(T + 1,), dtype=torch.int32),
-            partial_obs=dict(size=(T + 1, 7, 7, 3), dtype=torch.uint8),
-            episode_state_count=dict(size=(T + 1, ), dtype=torch.float32),
-            global_state_count=dict(size=(T + 1, ), dtype=torch.float32),
-            partial_state_count=dict(size=(T + 1, ), dtype=torch.float32),
-            encoded_state_count=dict(size=(T + 1, ), dtype=torch.float32),
-        )
-        
-    buffers: Buffers = {key: [] for key in specs}
-    for _ in range(flags.num_buffers):
-        for key in buffers:
-            buffers[key].append(torch.empty(**specs[key]).share_memory_())
-    return buffers
-
-
-
-
-def extract_state_key(env_output, flags):
-    if flags.episodic_bonus_type != 'none':
-        bonus_type = flags.episodic_bonus_type
-    else:
-        bonus_type = flags.global_bonus_type
-
-    if bonus_type == 'counts-obs':
-        # full observation: glyph image + stats + message
-        state_key = tuple(env_output['glyphs'].view(-1).tolist() \
-                          + env_output['blstats'].view(-1).tolist() \
-                          + env_output['message'].view(-1).tolist())
-    elif bonus_type == 'counts-msg':
-        # message only
-        state_key = tuple(env_output['message'].view(-1).tolist())
-    elif bonus_type == 'counts-glyphs':
-        # glyph image only
-        state_key = tuple(env_output['glyphs'].view(-1).tolist())
-    elif bonus_type == 'counts-pos':
-        # (x, y) position extracted from the stats vector
-        state_key = tuple(env_output['blstats'].view(-1).tolist()[:2])
-    elif bonus_type == 'counts-img':
-        # pixel image (for Vizdoom)
-        state_key = tuple(env_output['frame'].contiguous().view(-1).tolist())
-    else:
-        state_key = ()
-        
-    return state_key
-    
-    
-
-
-
-
-def act(i: int,
-        free_queue: mp.Queue,
-        full_queue: mp.Queue,
-        model: torch.nn.Module, 
-        encoder: torch.nn.Module,
-        buffers: Buffers, 
-        episode_state_count_dict: dict,
-        global_state_count_dict: dict,
-        initial_agent_state_buffers, 
-        flags):
-    try:
-        log.info('Actor %i started.', i)
-        timings = prof.Timings()  
-
-        gym_env = create_env(flags)
-        seed = i ^ int.from_bytes(os.urandom(4), byteorder='little')
-        gym_env.seed(seed)
-        
-        if flags.num_input_frames > 1:
-            gym_env = FrameStack(gym_env, flags.num_input_frames)  
-
-        env = Environment(gym_env, fix_seed=flags.fix_seed, env_seed=flags.env_seed)
-
-        env_output = env.initial()
-        agent_state = model.initial_state(batch_size=1)
-
-        agent_output, unused_state = model(env_output, agent_state)
-        prev_env_output = None
-
-        rank1_update = True 
-
-        if flags.episodic_bonus_type in ['elliptical-policy', 'elliptical-rand', 'elliptical-icm', 'elliptical-icm-lifelong']:
-            if rank1_update:
-                cov_inverse = torch.eye(flags.hidden_dim) * (1.0 / flags.ridge)
-            else:
-                cov = torch.eye(flags.hidden_dim) * flags.ridge
-            outer_product_buffer = torch.empty(flags.hidden_dim, flags.hidden_dim)
-    
-        step = 0
-
-
-        while True:
-            index = free_queue.get()
-            if index is None:
-                break
-
-            # Write old rollout end.
-            for key in env_output:
-                buffers[key][index][0, ...] = env_output[key]
-            for key in agent_output:
-                buffers[key][index][0, ...] = agent_output[key]
-            for i, tensor in enumerate(agent_state):
-                initial_agent_state_buffers[index][i][...] = tensor                
-
-
-
-            state_key = extract_state_key(env_output, flags)
-
-                
-            '''
-            if flags.episodic_bonus_type == 'counts-obs':
-                # full observation: glyph image + stats + message
-                state_key = tuple(env_output['glyphs'].view(-1).tolist() \
-                                          + env_output['blstats'].view(-1).tolist() \
-                                          + env_output['message'].view(-1).tolist())
-            elif flags.episodic_bonus_type == 'counts-msg':
-                # message only
-                state_key = tuple(env_output['message'].view(-1).tolist())
-            elif flags.episodic_bonus_type == 'counts-glyphs':
-                # glyph image only
-                state_key = tuple(env_output['glyphs'].view(-1).tolist())
-            elif flags.episodic_bonus_type == 'counts-pos':
-                # (x, y) position extracted from the stats vector
-                state_key = tuple(env_output['blstats'].view(-1).tolist()[:2])
-            elif flags.episodic_bonus_type == 'counts-img':
-                # pixel image (for Vizdoom)
-                state_key = tuple(env_output['frame'].contiguous().view(-1).tolist())
-            else:
-                state_key = ()
-
-            print(state_key)
-            '''
-
-
-            if flags.episodic_bonus_type in ['counts-obs', 'counts-msg', 'counts-glyphs', 'counts-pos', 'counts-img']:
-                if state_key in episode_state_count_dict:
-                    episode_state_count_dict[state_key] += 1
-                else:
-                    episode_state_count_dict.update({state_key: 1})
-                buffers['episode_state_count'][index][0, ...] = \
-                    torch.tensor(1 / np.sqrt(episode_state_count_dict.get(state_key)))
-
-
-            if flags.global_bonus_type in ['counts-obs', 'counts-msg', 'counts-glyphs', 'counts-pos', 'counts-img']:
-                if state_key in global_state_count_dict:
-                    global_state_count_dict[state_key] += 1
-                else:
-                    global_state_count_dict.update({state_key: 1})
-                buffers['global_state_count'][index][0, ...] = \
-                    torch.tensor(1 / np.sqrt(global_state_count_dict.get(state_key)))
-                
-            
-            # Reset the episode state counts or (inverse) covariance matrix when the episode is over
-            if env_output['done'][0][0]:
-                step = 0
-                for state_key in episode_state_count_dict:
-                    episode_state_count_dict = dict()
-                if flags.episodic_bonus_type in ['elliptical-policy', 'elliptical-rand', 'elliptical-icm']:
-                    if rank1_update:
-                        cov_inverse = torch.eye(flags.hidden_dim) * (1.0 / flags.ridge)
-                    else:
-                        cov_inverse = torch.eye(flags.hidden_dim) * flags.ridge
-                    
-                    
-                
-            # Do new rollout
-            for t in range(flags.unroll_length):
-                timings.reset()
-
-                with torch.no_grad():
-                    agent_output, agent_state = model(env_output, agent_state)
-                    if flags.episodic_bonus_type in ['elliptical-rand', 'elliptical-icm', 'elliptical-icm-diag', 'elliptical-icm-lifelong']:
-                        encoder_output, encoder_state = encoder(env_output, tuple())
-                
-
-                timings.time('model')
-
-                env_output = env.step(agent_output['action'])
-
-                timings.time('step')
-
-                for key in env_output:
-                    buffers[key][index][t + 1, ...] = env_output[key]
-    
-                for key in agent_output:
-                    buffers[key][index][t + 1, ...] = agent_output[key]
-
-
-                if flags.episodic_bonus_type == 'elliptical-policy':
-                    # run through policy net to get embeddings
-                    h = agent_output['policy_hiddens'].squeeze().detach()
-                    u = torch.mv(cov_inverse, h)
-                    b = torch.dot(h, u).item()
-
-                    torch.outer(u, u, out=outer_product_buffer)
-                    torch.add(cov_inverse, outer_product_buffer, alpha=-(1./(1. + b)), out=cov_inverse)                    
-
-                    if step == 0:
-                        b = 0
-                        
-                    buffers['bonus_reward'][index][t + 1, ...] = b
-                    
-
-                elif flags.episodic_bonus_type in ['elliptical-rand', 'elliptical-icm', 'elliptical-icm-lifelong']:
-                    # run through encoder to get embeddings
-                    h = encoder_output.squeeze().detach()
-
-                    if rank1_update:
-                        u = torch.mv(cov_inverse, h)
-                        b = torch.dot(h, u).item()
-
-                        torch.outer(u, u, out=outer_product_buffer)
-                        torch.add(cov_inverse, outer_product_buffer, alpha=-(1./(1. + b)), out=cov_inverse)
-                    else:
-                        cov = cov + torch.outer(h, h)
-                        cov_inverse = torch.inverse(cov)
-                        u = torch.mv(cov_inverse, h)
-                        b = torch.dot(h, u).item()
-
-                    if step == 0:
-                        b = 0
-                        
-                    buffers['bonus_reward'][index][t + 1, ...] = b
-
-                elif flags.episodic_bonus_type == 'none':
-                    buffers['bonus_reward'][index][t + 1, ...] = 0
-                else:
-                    assert 'counts' in flags.episodic_bonus_type
-                    
-                step += 1                
-
-                
-                '''
-                if flags.episodic_bonus_type == 'counts-obs':
-                    state_key = tuple(env_output['glyphs'].view(-1).tolist() \
-                                              + env_output['blstats'].view(-1).tolist() \
-                                              + env_output['message'].view(-1).tolist())
-                elif flags.episodic_bonus_type == 'counts-msg':
-                    state_key = tuple(env_output['message'].view(-1).tolist())
-                elif flags.episodic_bonus_type == 'counts-glyphs':
-                    state_key = tuple(env_output['glyphs'].view(-1).tolist())
-                elif flags.episodic_bonus_type == 'counts-pos':
-                    state_key = tuple(env_output['blstats'].view(-1).tolist()[:2])
-                elif flags.episodic_bonus_type == 'counts-img':
-                    state_key = tuple(env_output['frame'].contiguous().view(-1).tolist())
-                else:
-                    state_key = ()
-                '''
-
-                state_key = extract_state_key(env_output, flags)
-
-#                print(state_key)
-                
-                
-                    
-                if flags.episodic_bonus_type in ['counts-obs', 'counts-msg', 'counts-glyphs', 'counts-pos', 'counts-img']:
-                    if state_key in episode_state_count_dict:
-                        episode_state_count_dict[state_key] += 1
-                    else:
-                        episode_state_count_dict.update({state_key: 1})
-                    buffers['episode_state_count'][index][t + 1, ...] = \
-                        torch.tensor(1 / np.sqrt(episode_state_count_dict.get(state_key)))
-
-
-                if flags.global_bonus_type in ['counts-obs', 'counts-msg', 'counts-glyphs', 'counts-pos', 'counts-img']:
-                    if state_key in global_state_count_dict:
-                        global_state_count_dict[state_key] += 1
-                    else:
-                        global_state_count_dict.update({state_key: 1})
-                    buffers['global_state_count'][index][t + 1, ...] = \
-                        torch.tensor(1 / np.sqrt(global_state_count_dict.get(state_key)))
-                    
-
-                timings.time('bonus update')
-                # Reset the episode state counts/covariance when the episode is over
-                if env_output['done'][0][0]:
-                    step = 0
-                    episode_state_count_dict = dict()
-                    if flags.episodic_bonus_type in ['elliptical-policy', 'elliptical-rand', 'elliptical-icm']:
-                        if rank1_update:
-                            cov_inverse = torch.eye(flags.hidden_dim) * (1.0 / flags.ridge)
-                        else:
-                            cov = torch.eye(flags.hidden_dim) * flags.ridge
-                            
-
-                timings.time('write')
-
-                
-            full_queue.put(index)
-
-        if i == 0:
-            log.info('Actor %i: %s', i, timings.summary())
-
-    except KeyboardInterrupt:
-        pass  
-    except Exception as e:
-        logging.error('Exception in worker process %i', i)
-        traceback.print_exc()
-        print()
-        raise e
diff --git a/minihack/sweep_slurm_icml_2023.py b/minihack/sweep_slurm_icml_2023.py
deleted file mode 100755
index b427131..0000000
--- a/minihack/sweep_slurm_icml_2023.py
+++ /dev/null
@@ -1,328 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-
-#!/usr/bin/env python3
-import numpy as np
-import numpy.random as npr
-
-import time
-import os
-import sys
-import argparse
-import pdb
-import itertools
-from subprocess import Popen, DEVNULL
-
-        
-# creates commands and job file names for a grid search over lists of hyperparameters
-class Overrides(object):
-    def __init__(self):
-        self.args = []
-
-    def add(self, arg_name, arg_values):
-        self.args.append([{'name': arg_name, 'value': v} for v in arg_values])
-                
-    def parse(self, basecmd, cmd_format='argparse'):
-        cmd, job = [], []
-        for combos in itertools.product(*self.args):
-            c = basecmd
-            j = 'job'
-            d = dict()
-            for arg in combos:
-                c += f" --{arg['name']} {str(arg['value'])}"
-                if arg['name'] != 'savedir':
-                    j += f"_{arg['name']}={str(arg['value'])}"
-                d[arg['name']] = arg['value']
-            cmd.append(c)
-            job.append(j)                
-        return cmd, job
-
-
-# copies the code before the sweep is run 
-def make_code_snapshot(savedir):
-    snap_dir = f'{savedir}/code/'
-    dirs_to_copy = ['.', 'src', 'src/algos', 'src/core']
-    
-    def copy_dir(dir, pat):
-        dst_dir = f'{snap_dir}/{dir}/'
-        os.system(f'mkdir -p {dst_dir}')
-        os.system(f'cp {dir}/{pat} {dst_dir}/')
-
-    for dir in dirs_to_copy:
-        copy_dir(dir, '*.py')
-        
-        
-    
-    
-# Note: may need to adapt this based on compute infrastructure
-def write_slurm_script(name, cmd, partition, device=0, ncpu=1):
-    scriptfile = f'slurm/scripts/run.{name}.sh'
-    slurmfile = f'slurm/scripts/run.{name}.slrm'
-    os.system('mkdir -p slurm/scripts/')
-    with open(slurmfile, 'w') as s:
-        s.write("#!/bin/sh\n")
-        s.write(f"#SBATCH --job-name={name}\n")
-        s.write(f"#SBATCH --output=slurm/stdout/{name}.%j\n")
-        s.write(f"#SBATCH --error=slurm/stderr/{name}.%j\n")
-        s.write(f"#SBATCH --partition={partition}\n")
-        s.write("#SBATCH --mem=200000\n")
-        s.write("#SBATCH --time=4320\n")
-        s.write("#SBATCH --nodes=1\n")
-        s.write(f"#SBATCH --cpus-per-task={ncpu}\n")
-        s.write("#SBATCH --ntasks-per-node=1\n")
-        s.write("#SBATCH --requeue\n")
-        s.write("#SBATCH --gres=gpu:1\n")
-        s.write(f"srun sh {scriptfile}\n")
-
-    with open(scriptfile, 'w') as s:
-        s.write("#!/bin/sh\n")
-        s.write("nvidia-smi\n")
-        s.write(f"cd {os.getcwd()}\n")
-        s.write(f"{cmd}\n")
-        s.write("nvidia-smi\n")
-        
-
-
-
-
-SKILL_TASKS = [
-         'MiniHack-Levitate-Potion-Restricted-v0',
-         'MiniHack-Levitate-Boots-Restricted-v0',
-         'MiniHack-Freeze-Horn-Restricted-v0', 
-         'MiniHack-Freeze-Wand-Restricted-v0', 
-         'MiniHack-Freeze-Random-Restricted-v0',
-         'MiniHack-LavaCross-Restricted-v0', 
-         'MiniHack-WoD-Hard-Restricted-v0'
-         ] 
-
-NAV_TASKS = [
-         'MiniHack-MultiRoom-N4-Locked-v0',
-         'MiniHack-MultiRoom-N6-Lava-v0',
-         'MiniHack-MultiRoom-N6-Lava-OpenDoor-v0',
-         'MiniHack-MultiRoom-N6-LavaMonsters-v0', 
-         'MiniHack-MultiRoom-N10-OpenDoor-v0',  
-         'MiniHack-MultiRoom-N10-Lava-OpenDoor-v0', 
-         'MiniHack-LavaCrossingS19N13-v0',
-         'MiniHack-LavaCrossingS19N17-v0',             
-         'MiniHack-Labyrinth-Big-v0',
-         ]
-
-ALL_TASKS = SKILL_TASKS + NAV_TASKS
-
-        
-
-def main():
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--task', default='train-elliptical')
-    parser.add_argument('--queue', default='')
-    parser.add_argument('--dry', action='store_true')
-    parser.add_argument('--indx', type=int, default=-1)
-    args = parser.parse_args()
-
-    overrides = Overrides()
-    
-
-    # full E3B on MiniHack
-    if args.task == 'train-elliptical':
-        SAVEDIR = './results/elliptical/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['e3b'])
-        overrides.add('episodic_bonus_type', ['elliptical-icm'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('ridge', [0.1])
-        overrides.add('reward_norm', ['int'])
-        overrides.add('intrinsic_reward_coef', [1.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-
-    # MultiRoom with episodic bonus and positional encodings as described in Section 3.1
-    elif args.task == 'train-counts-episodic-multiroom':
-        SAVEDIR = './results/counts_episodic_multiroom/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['counts-pos'])
-        overrides.add('global_bonus_type', ['none'])
-        overrides.add('env', ['MiniHack-MultiRoom-N6-Lava-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 10.0])
-        overrides.add('num_contexts', [1, 3, 5, 10, -1])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-    # MultiRoom with global bonus and positional encodings as described in Section 3.1
-    elif args.task == 'train-counts-global-multiroom':
-        SAVEDIR = './results/counts_global_multiroom/'
-#        SAVEDIR = './results/tmp/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['none'])
-        overrides.add('global_bonus_type', ['counts-pos'])
-        overrides.add('env', ['MiniHack-MultiRoom-N6-Lava-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 10.0])
-        overrides.add('num_contexts', [1, 3, 5, 10, -1])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # Corridors with episodic bonus and positional encodings as described in Section 3.2
-    elif args.task == 'train-counts-episodic-corridors':
-        SAVEDIR = './results/counts_episodic_corridors/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['counts-pos'])
-        overrides.add('global_bonus_type', ['none'])
-        overrides.add('env', ['MiniHack-Corridor-R5-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 10.0])
-        overrides.add('num_contexts', [1])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-
-    # Corridors with global bonus and positional encodings as described in Section 3.2
-    elif args.task == 'train-counts-global-corridors':
-        SAVEDIR = './results/counts_global_corridors/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['none'])
-        overrides.add('global_bonus_type', ['counts-pos'])
-        overrides.add('env', ['MiniHack-Corridor-R5-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 10.0])
-        overrides.add('num_contexts', [1])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # KeyRoom and MultiRoom with episodic bonus and message encodings as described in Section 3.2
-    elif args.task == 'train-counts-episodic-msg':
-        SAVEDIR = './results/counts_episodic_msg/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['counts-msg'])
-        overrides.add('global_bonus_type', ['none'])
-        overrides.add('env', ['MiniHack-KeyRoom-S10-v0', 'MiniHack-MultiRoom-N6-Lava-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 10.0])
-        overrides.add('num_contexts', [-1])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-
-    # KeyRoom and MultiRoom with global bonus and message encodings as described in Section 3.2
-    elif args.task == 'train-counts-global-msg':
-        SAVEDIR = './results/counts_global_msg/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['none'])
-        overrides.add('global_bonus_type', ['counts-msg'])
-        overrides.add('env', ['MiniHack-KeyRoom-S10-v0', 'MiniHack-MultiRoom-N6-Lava-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('num_contexts', [-1])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 10.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # Combined bonus with positional encodings (Section 3.4)
-    elif args.task == 'train-counts-combined-pos':
-        SAVEDIR = './results/counts_combined_pos/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['counts-pos'])
-        overrides.add('global_bonus_type', ['counts-pos'])
-        overrides.add('env', ['MiniHack-Corridor-R5-v0', 'MiniHack-MultiRoom-N6-Lava-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('num_contexts', [1, -1])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 10.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # Combined bonus with message encodings (Section 3.4)
-    elif args.task == 'train-counts-combined-msg':
-        SAVEDIR = './results/counts_combined_msg/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['count'])
-        overrides.add('episodic_bonus_type', ['counts-msg'])
-        overrides.add('global_bonus_type', ['counts-msg'])
-        overrides.add('env', ['MiniHack-KeyRoom-S10-v0', 'MiniHack-MultiRoom-N6-Lava-v0'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('num_contexts', [-1])
-        overrides.add('intrinsic_reward_coef', [0.1, 1.0, 2.0, 10.0]) # we include 2.0 for MultiRoom, this one is a bit more sensitive
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # Combined bonus for E3Bx{NovelD, RND} (Section 4.2)
-    # args.scale_fac=0 corresponds to E3BxRND
-    elif args.task == 'train-e3b-noveld':
-        SAVEDIR = './results/e3b_noveld/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['e3b-noveld'])
-        overrides.add('episodic_bonus_type', ['elliptical-icm'])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('scale_fac', [0.0, 0.1])
-        overrides.add('num_contexts', [-1])
-        overrides.add('intrinsic_reward_coef', [1.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-        
-        
-        
-            
-        
-        
-
-    os.system('mkdir -p slurm/stdout slurm/stderr')
-#    cmds = [cmd + f' --device {args.device}' for cmd in cmds]
-    n_jobs = len(cmds)
-    print(f'submitting {n_jobs} jobs')
-    import pdb; pdb.set_trace()
-    for i in range(n_jobs):
-        if args.dry:
-            print(cmds[i])
-            if i == len(cmds) + args.indx:
-                os.system(cmds[i])
-        else:
-            print(cmds[i])
-            write_slurm_script(jobs[i], cmds[i], partition=args.queue, ncpu=args.ncpu)
-            os.system(f'sbatch slurm/scripts/run.{jobs[i]}.slrm')
-        
-
-if __name__ == '__main__':
-    main()
diff --git a/minihack/sweep_slurm_neurips_2022.py b/minihack/sweep_slurm_neurips_2022.py
deleted file mode 100755
index ceb744a..0000000
--- a/minihack/sweep_slurm_neurips_2022.py
+++ /dev/null
@@ -1,397 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved
-
-
-#!/usr/bin/env python3
-import numpy as np
-import numpy.random as npr
-
-import time
-import os
-import sys
-import argparse
-import pdb
-import itertools
-from subprocess import Popen, DEVNULL
-
-        
-# creates commands and job file names for a grid search over lists of hyperparameters
-class Overrides(object):
-    def __init__(self):
-        self.args = []
-
-    def add(self, arg_name, arg_values):
-        self.args.append([{'name': arg_name, 'value': v} for v in arg_values])
-                
-    def parse(self, basecmd, cmd_format='argparse'):
-        cmd, job = [], []
-        for combos in itertools.product(*self.args):
-            c = basecmd
-            j = 'job'
-            d = dict()
-            for arg in combos:
-                c += f" --{arg['name']} {str(arg['value'])}"
-                if arg['name'] != 'savedir':
-                    j += f"_{arg['name']}={str(arg['value'])}"
-                d[arg['name']] = arg['value']
-            cmd.append(c)
-            job.append(j)                
-        return cmd, job
-
-
-# copies the code before the sweep is run 
-def make_code_snapshot(savedir):
-    snap_dir = f'{savedir}/code/'
-    dirs_to_copy = ['.', 'src', 'src/algos', 'src/core']
-    
-    def copy_dir(dir, pat):
-        dst_dir = f'{snap_dir}/{dir}/'
-        os.system(f'mkdir -p {dst_dir}')
-        os.system(f'cp {dir}/{pat} {dst_dir}/')
-
-    for dir in dirs_to_copy:
-        copy_dir(dir, '*.py')
-        
-        
-    
-    
-# Note: may need to adapt this based on compute infrastructure
-def write_slurm_script(name, cmd, partition, device=0, ncpu=1):
-    scriptfile = f'slurm/scripts/run.{name}.sh'
-    slurmfile = f'slurm/scripts/run.{name}.slrm'
-    os.system('mkdir -p slurm/scripts/')
-    with open(slurmfile, 'w') as s:
-        s.write("#!/bin/sh\n")
-        s.write(f"#SBATCH --job-name={name}\n")
-        s.write(f"#SBATCH --output=slurm/stdout/{name}.%j\n")
-        s.write(f"#SBATCH --error=slurm/stderr/{name}.%j\n")
-        s.write(f"#SBATCH --partition={partition}\n")
-        s.write("#SBATCH --mem=200000\n")
-        s.write("#SBATCH --time=4320\n")
-        s.write("#SBATCH --nodes=1\n")
-        s.write(f"#SBATCH --cpus-per-task={ncpu}\n")
-        s.write("#SBATCH --ntasks-per-node=1\n")
-        s.write("#SBATCH --requeue\n")
-        s.write("#SBATCH --gres=gpu:1\n")
-        s.write(f"srun sh {scriptfile}\n")
-
-    with open(scriptfile, 'w') as s:
-        s.write("#!/bin/sh\n")
-        s.write("nvidia-smi\n")
-        s.write(f"cd {os.getcwd()}\n")
-        s.write(f"{cmd}\n")
-        s.write("nvidia-smi\n")
-        
-
-
-
-
-SKILL_TASKS = [
-         'MiniHack-Levitate-Potion-Restricted-v0',
-         'MiniHack-Levitate-Boots-Restricted-v0',
-         'MiniHack-Freeze-Horn-Restricted-v0', 
-         'MiniHack-Freeze-Wand-Restricted-v0', 
-         'MiniHack-Freeze-Random-Restricted-v0',
-         'MiniHack-LavaCross-Restricted-v0', 
-         'MiniHack-WoD-Hard-Restricted-v0'
-         ] 
-
-NAV_TASKS = [
-         'MiniHack-MultiRoom-N4-Locked-v0',
-         'MiniHack-MultiRoom-N6-Lava-v0',
-         'MiniHack-MultiRoom-N6-Lava-OpenDoor-v0',
-         'MiniHack-MultiRoom-N6-LavaMonsters-v0', 
-         'MiniHack-MultiRoom-N10-OpenDoor-v0',  
-         'MiniHack-MultiRoom-N10-Lava-OpenDoor-v0', 
-         'MiniHack-LavaCrossingS19N13-v0',
-         'MiniHack-LavaCrossingS19N17-v0',             
-         'MiniHack-Labyrinth-Big-v0',
-         ]
-
-ALL_TASKS = SKILL_TASKS + NAV_TASKS
-
-        
-
-def main():
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--task', default='train-elliptical')
-    parser.add_argument('--queue', default='')
-    parser.add_argument('--dry', action='store_true')
-    parser.add_argument('--indx', type=int, default=-1)
-    args = parser.parse_args()
-
-    overrides = Overrides()
-    
-
-    # full E3B on MiniHack
-    if args.task == 'train-elliptical':
-        SAVEDIR = './results/elliptical/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['e3b'])
-        overrides.add('episodic_bonus_type', ['elliptical-icm'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('ridge', [0.1])
-        overrides.add('reward_norm', ['int'])
-        overrides.add('intrinsic_reward_coef', [1.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-        
-    # ablation: E3B (non-episodic), i.e. with lifelong novelty bonus
-    elif args.task == 'train-elliptical-lifelong':
-        SAVEDIR = './results/elliptical_lifelong/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['e3b'])
-        overrides.add('episodic_bonus_type', ['elliptical-icm-lifelong'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('ridge', [0.1])
-        overrides.add('reward_norm', ['int'])
-        overrides.add('intrinsic_reward_coef', [1.0]) 
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-        
-
-    # ablation: E3B with fixed random encoder 
-    elif args.task == 'train-elliptical-rand-encoder':
-        SAVEDIR = './results/elliptical_rand_encoder/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['e3b'])
-        overrides.add('predictor_learning_rate', [0.0])
-        overrides.add('episodic_bonus_type', ['elliptical-icm'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('ridge', [0.1])
-        overrides.add('reward_norm', ['int'])
-        overrides.add('intrinsic_reward_coef', [1.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # ablation: E3B with policy network encoder
-    elif args.task == 'train-elliptical-policy-encoder':
-        SAVEDIR = './results/elliptical_policy_encoder/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['e3b'])
-        overrides.add('predictor_learning_rate', [0.0])
-        overrides.add('episodic_bonus_type', ['elliptical-policy'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('ridge', [0.1])
-        overrides.add('reward_norm', ['int'])
-        overrides.add('intrinsic_reward_coef', [1.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-
-
-    # E3B on Vizdoom
-    elif args.task == 'train-elliptical-vizdoom':
-        SAVEDIR = './results/vizdoom/elliptical_final/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['e3b'])
-        overrides.add('episodic_bonus_type', ['elliptical-icm'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ['VizdoomMyWayHomeDense-v0', 'VizdoomMyWayHomeSparse-v0', 'VizdoomMyWayHomeVerySparse-v0'])
-        overrides.add('ridge', [0.1])
-        overrides.add('reward_norm', ['none'])
-        overrides.add('hidden_dim', [288])
-        overrides.add('intrinsic_reward_coef', [3e-5, 1e-5, 3e-6, 1e-6, 3e-7, 0.0])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-
-        
-
-                
-
-    # NovelD with 4 variants for the episodic bonus:
-    # -standard (episodic_bonus_type = counts-obs)
-    # -symbolic image (episodic_bonus_type = counts-glyphs)
-    # -(x, y) coordinates (episodic_bonus_type = counts-pos)
-    # -message (episodic_bonus_type = counts-msg)
-    elif args.task == 'train-noveld':
-        SAVEDIR = './results/noveld/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['bebold'])
-        overrides.add('episodic_bonus_type', ['counts-glyphs', 'counts-pos', 'counts-msg', 'counts-obs'])
-        overrides.add('count_reward_type', ['ind'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('reward_norm', ['int'])
-        overrides.add('intrinsic_reward_coef', [1.0]) 
-        overrides.add('scale_fac', [0.1]) # tried 0.5, 0.1 in early experiments, 0.1 worked best
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # NovelD on Vizdoom 
-    elif args.task == 'train-noveld-vizdoom':
-        SAVEDIR = './results/vizdoom/noveld/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0005])
-        overrides.add('model', ['bebold'])
-        overrides.add('episodic_bonus_type', ['counts-img'])
-        overrides.add('count_reward_type', ['ind'])
-        overrides.add('total_frames', [int(1e7)])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('hidden_dim', [288])
-        overrides.add('entropy_cost', [0.005, 0.0005])
-        overrides.add('env', ['VizdoomMyWayHomeDense-v0', 'VizdoomMyWayHomeSparse-v0', 'VizdoomMyWayHomeVerySparse-v0'])
-        overrides.add('intrinsic_reward_coef', [1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-
-        
-    # train RIDE on MiniHack
-    elif args.task == 'train-ride':
-        SAVEDIR = './results/ride/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['ride'])
-        overrides.add('episodic_bonus_type', ['counts-obs'])
-        overrides.add('count_reward_type', ['ind'])
-        overrides.add('forward_loss_coef', [1.0]) # from MiniHack paper
-        overrides.add('inverse_loss_coef', [0.1]) # MiniHack paper uses 0.1, also try 1.0
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('intrinsic_reward_coef', [0.001, 0.01, 0.1, 1.0]) # MiniHack paper uses 0.1, sweep around
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # train RIDE on Vizdoom
-    elif args.task == 'train-ride-vizdoom':
-        SAVEDIR = './results/vizdoom/ride/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0005])
-        overrides.add('model', ['ride'])
-        overrides.add('episodic_bonus_type', ['counts-img'])
-        overrides.add('count_reward_type', ['ind'])
-        overrides.add('forward_loss_coef', [0.5]) # from author
-        overrides.add('inverse_loss_coef', [0.8]) # from author
-        overrides.add('pg_loss_coef', [0.1]) # from author
-        overrides.add('total_frames', [int(1e7)])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ['VizdoomMyWayHomeDense-v0', 'VizdoomMyWayHomeSparse-v0', 'VizdoomMyWayHomeVerySparse-v0'])
-        overrides.add('intrinsic_reward_coef', [1e-2, 3e-3, 1e-3, 3e-4, 1e-4])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # train RND on MiniHack
-    elif args.task == 'train-rnd':
-        SAVEDIR = './results/rnd/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['rnd'])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('intrinsic_reward_coef', [0.001]) 
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-    # train ICM on MiniHack
-    elif args.task == 'train-icm':
-        SAVEDIR = './results/icm/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['curiosity'])
-        overrides.add('episodic_bonus_type', ['counts-obs'])
-        overrides.add('forward_loss_coef', [1.0]) # from MiniHack paper
-        overrides.add('inverse_loss_coef', [0.1]) # MiniHack paper uses 0.1
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('intrinsic_reward_coef', [0.1]) 
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-
-    # train ICM on Vizdoom
-    elif args.task == 'train-icm-vizdoom':
-        SAVEDIR = './results/vizdoom/icm/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0005])
-        overrides.add('model', ['curiosity'])
-        overrides.add('episodic_bonus_type', ['counts-img'])
-        overrides.add('entropy_cost', [0.005]) # from author
-        overrides.add('forward_loss_coef', [0.2]) # from author
-        overrides.add('inverse_loss_coef', [0.8]) # from author
-        overrides.add('total_frames', [int(1e7)])
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('env', ['VizdoomMyWayHomeDense-v0', 'VizdoomMyWayHomeSparse-v0', 'VizdoomMyWayHomeVerySparse-v0'])
-        overrides.add('intrinsic_reward_coef', [0.001, 0.003, 0.01, 0.03]) 
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-
-        
-        
-        
-    # train IMPALA on MiniHack
-    elif args.task == 'train-impala':
-        SAVEDIR = './results/impala/'
-        make_code_snapshot(SAVEDIR)
-        overrides.add('learning_rate', [0.0001])
-        overrides.add('model', ['vanilla'])
-        overrides.add('env', ALL_TASKS)
-        overrides.add('savedir', [SAVEDIR])
-        overrides.add('seed', [1, 2, 3, 4, 5])
-        cmds, jobs = overrides.parse('OMP_NUM_THREADS=1 python main.py ', cmd_format='argparse')
-        args.ncpu = 40
-        print(cmds)
-        
-        
-        
-        
-        
-        
-
-    os.system('mkdir -p slurm/stdout slurm/stderr')
-#    cmds = [cmd + f' --device {args.device}' for cmd in cmds]
-    n_jobs = len(cmds)
-    print(f'submitting {n_jobs} jobs')
-    for i in range(n_jobs):
-        if args.dry:
-            print(cmds[i])
-            if i == len(cmds) + args.indx:
-                os.system(cmds[i])
-        else:
-            print(cmds[i])
-            write_slurm_script(jobs[i], cmds[i], partition=args.queue, ncpu=args.ncpu)
-            os.system(f'sbatch slurm/scripts/run.{jobs[i]}.slrm')
-        
-
-if __name__ == '__main__':
-    main()
